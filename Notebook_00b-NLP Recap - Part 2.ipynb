{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Recap - Part 2\n",
    "## Deep Learning and Generative AI\n",
    "### [Dr. Elias Jacob de Menezes Neto](https://docente.ufrn.br/elias.jacob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "## Keypoints\n",
    "- Common Natural Language Processing (NLP) pipelines consists of three main steps: text processing (normalization, tokenization, numericalization), feature extraction, and model training.\n",
    "\n",
    "- Hyperparameter optimization techniques include Grid Search, Random Search, and Bayesian Optimization, each with its own advantages and use cases.\n",
    "\n",
    "- The \"60 iterations rule\" in Random Search states that 60 iterations can find the best 5% set of parameters 95% of the time, regardless of the grid size.\n",
    "\n",
    "- Bayesian Optimization uses a surrogate model, objective function, and acquisition function to efficiently navigate the hyperparameter space.\n",
    "\n",
    "- Ensemble methods like StackingClassifier can improve model performance by combining multiple base classifiers with a meta-classifier.\n",
    "\n",
    "- Chronological data splitting is crucial for legal ruling analysis to prevent temporal leakage and maintain the integrity of legal precedents.\n",
    "\n",
    "## Takeaways\n",
    "- Proper data preprocessing and feature extraction are essential for effective NLP tasks, especially in specialized domains like legal text analysis.\n",
    "\n",
    "- Choosing the right hyperparameter optimization technique depends on the problem complexity, computational resources, and desired balance between exploration and exploitation.\n",
    "\n",
    "- Ensemble methods can significantly enhance model performance by leveraging the strengths of multiple classifiers, but may increase computational cost and reduce interpretability.\n",
    "\n",
    "- When working with time-sensitive data like legal rulings, it's crucial to consider the temporal nature of the data in both preprocessing and model evaluation stages.\n",
    "\n",
    "- Evaluation metrics should be chosen carefully based on the specific problem and domain; in this case, Matthews Correlation Coefficient (MCC) was emphasized for its effectiveness in imbalanced classification tasks.\n",
    "\n",
    "- Balancing model performance with interpretability and computational efficiency is a key consideration in practical machine learning applications, especially in sensitive domains like legal prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the BrCAD-5 Dataset\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The dataset we're working with is a sample from the [BrCAD-5](https://www.kaggle.com/datasets/eliasjacob/brcad5), a extensive collection of legal rulings from Brazilian Federal Small Claims Courts (FSCC). This dataset was specifically curated for academic purposes, with the primary goal of developing AI models capable of predicting appeal outcomes within the jurisdiction of the 5th Regional Federal Court (TRF5).\n",
    "\n",
    "## Key Features of the Dataset\n",
    "\n",
    "- **Sample Size**: Our sample contains over 40,000 legal rulings, providing a robust foundation for analysis.\n",
    "- **Jurisdiction**: All cases are from the 5th Regional Federal Court (TRF5) jurisdiction in Brazil.\n",
    "- **Case Types**: The dataset includes rulings from both federal courts and appellate panels.\n",
    "\n",
    "## Dataset Structure\n",
    "\n",
    "The dataset is composed of three main columns:\n",
    "\n",
    "1. **`case_number`**:\n",
    "    - A unique identifier for each legal ruling\n",
    "    - Ensures each case can be distinctly referenced and tracked\n",
    "\n",
    "2. **`ruling_type`**:\n",
    "- Indicates the type of legal ruling\n",
    "    - Two categories:\n",
    "    - ACÓRDÃO (Judgment): Typically refers to decisions made by appellate panels\n",
    "    - SENTENÇA (Sentence): Usually refers to decisions made by a single judge in a lower court\n",
    "\n",
    "3. **`outcome`**:\n",
    "    - Represents the result of the legal ruling\n",
    "    - Multiple possible outcomes:\n",
    "    - PROVIMENTO: Appeal granted\n",
    "    - PROVIMENTO PARCIAL: Appeal partially granted\n",
    "    - NÃO PROVIMENTO: Appeal denied\n",
    "    - IMPROCEDENTE: Claim dismissed\n",
    "    - PROCEDENTE: Claim upheld\n",
    "    - PARCIALMENTE PROCEDENTE: Claim partially upheld\n",
    "    - EXTINTO SEM MÉRITO: Case dismissed without judgment on merits\n",
    "    - HOMOLOGADA TRANSAÇÃO: Settlement agreement approved\n",
    "\n",
    "## Significance of the Dataset\n",
    "\n",
    "1. **AI in Legal Prediction**: This dataset serves as a valuable resource for developing machine learning models that can predict legal outcomes, potentially revolutionizing legal research and case preparation.\n",
    "\n",
    "2. **Understanding Brazilian Legal System**: It offers insights into the decision-making patterns within Brazilian Federal Small Claims Courts, which could be useful for comparative legal studies.\n",
    "\n",
    "3. **Natural Language Processing (NLP) Applications**: While not explicitly mentioned, such datasets often include text data that can be used for NLP tasks like legal document classification or summarization.\n",
    "\n",
    "## Considerations for Analysis\n",
    "\n",
    "- **Balanced Representation**: It's important to check if all outcome categories are adequately represented in the sample to ensure unbiased analysis.\n",
    "- **Temporal Aspects**: Consider whether the rulings span a specific time period, as legal trends may change over time.\n",
    "- **Contextual Factors**: While not provided in this dataset, factors like the judge's identity, case subject matter, or regional variations could be influential in outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our data.\n",
    "import pandas as pd\n",
    "\n",
    "df_texts = pd.read_parquet(\"data/brcad5/texts_sample.parquet.gz\")\n",
    "df_meta = pd.read_parquet(\"data/brcad5/metadata_sample.parquet.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_number</th>\n",
       "      <th>text</th>\n",
       "      <th>outcome</th>\n",
       "      <th>ruling_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0515165-56.2018.4.05.8202</td>\n",
       "      <td>SENTENÇA I - RELATÓRIO Dispensada a feitura do...</td>\n",
       "      <td>IMPROCEDENTE</td>\n",
       "      <td>SENTENÇA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0506287-42.2018.4.05.8300</td>\n",
       "      <td>SENTENÇA I - RELATÓRIO Dispensado, nos termos ...</td>\n",
       "      <td>PARCIALMENTE PROCEDENTE</td>\n",
       "      <td>SENTENÇA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0513000-08.2019.4.05.8103</td>\n",
       "      <td>RECURSO INOMINADO CONTRA SENTENÇA DE EXTINÇÃO ...</td>\n",
       "      <td>NÃO PROVIMENTO</td>\n",
       "      <td>ACÓRDÃO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0503661-32.2018.4.05.8015</td>\n",
       "      <td>PROCESSO No 0503661-32.2018.4.05.8015 RECORREN...</td>\n",
       "      <td>NÃO PROVIMENTO</td>\n",
       "      <td>ACÓRDÃO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0516813-86.2018.4.05.8100</td>\n",
       "      <td>AMPARO SOCIAL (LOAS). REQUISITOS NÃO PREENCHID...</td>\n",
       "      <td>NÃO PROVIMENTO</td>\n",
       "      <td>ACÓRDÃO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  case_number  \\\n",
       "2   0515165-56.2018.4.05.8202   \n",
       "11  0506287-42.2018.4.05.8300   \n",
       "15  0513000-08.2019.4.05.8103   \n",
       "18  0503661-32.2018.4.05.8015   \n",
       "19  0516813-86.2018.4.05.8100   \n",
       "\n",
       "                                                 text  \\\n",
       "2   SENTENÇA I - RELATÓRIO Dispensada a feitura do...   \n",
       "11  SENTENÇA I - RELATÓRIO Dispensado, nos termos ...   \n",
       "15  RECURSO INOMINADO CONTRA SENTENÇA DE EXTINÇÃO ...   \n",
       "18  PROCESSO No 0503661-32.2018.4.05.8015 RECORREN...   \n",
       "19  AMPARO SOCIAL (LOAS). REQUISITOS NÃO PREENCHID...   \n",
       "\n",
       "                    outcome ruling_type  \n",
       "2              IMPROCEDENTE    SENTENÇA  \n",
       "11  PARCIALMENTE PROCEDENTE    SENTENÇA  \n",
       "15           NÃO PROVIMENTO     ACÓRDÃO  \n",
       "18           NÃO PROVIMENTO     ACÓRDÃO  \n",
       "19           NÃO PROVIMENTO     ACÓRDÃO  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ruling_type  outcome                \n",
       "ACÓRDÃO      NÃO PROVIMENTO             15581\n",
       "             PROVIMENTO                  3004\n",
       "             PROVIMENTO PARCIAL          1415\n",
       "SENTENÇA     IMPROCEDENTE               11550\n",
       "             PROCEDENTE                  5253\n",
       "             PARCIALMENTE PROCEDENTE     2403\n",
       "             EXTINTO SEM MÉRITO           791\n",
       "             HOMOLOGADA TRANSAÇÃO           3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_texts.groupby(\"ruling_type\").outcome.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_number</th>\n",
       "      <th>filing_date</th>\n",
       "      <th>defendant_normalized</th>\n",
       "      <th>date_first_instance_ruling</th>\n",
       "      <th>date_appeal_panel_ruling</th>\n",
       "      <th>case_topic_code</th>\n",
       "      <th>case_topic_1st_level</th>\n",
       "      <th>case_topic_2nd_level</th>\n",
       "      <th>case_topic_3rd_level</th>\n",
       "      <th>court_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0510491-75.2017.4.05.8103</td>\n",
       "      <td>2017-10-11 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2018-01-16 11:16:19</td>\n",
       "      <td>2018-03-26 17:07:16</td>\n",
       "      <td>6095</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Aposentadoria por Invalidez</td>\n",
       "      <td>19-CE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0509917-52.2017.4.05.8103</td>\n",
       "      <td>2017-09-26 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2018-01-15 18:07:55</td>\n",
       "      <td>2018-03-26 17:07:16</td>\n",
       "      <td>6101</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Auxílio-Doença Previdenciário</td>\n",
       "      <td>31-CE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0501051-55.2017.4.05.8103</td>\n",
       "      <td>2017-02-03 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2017-11-21 11:44:40</td>\n",
       "      <td>2018-03-26 17:07:16</td>\n",
       "      <td>6103</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Salário-Maternidade (Art. 71/73)</td>\n",
       "      <td>31-CE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0511681-76.2017.4.05.8102</td>\n",
       "      <td>2017-09-27 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2017-12-29 02:18:24</td>\n",
       "      <td>2018-03-26 17:07:16</td>\n",
       "      <td>6104</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Pensão por Morte (Art. 74/9)</td>\n",
       "      <td>30-CE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0500777-79.2017.4.05.8107</td>\n",
       "      <td>2017-02-23 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2017-11-27 17:58:46</td>\n",
       "      <td>2018-03-26 17:07:16</td>\n",
       "      <td>6103</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Salário-Maternidade (Art. 71/73)</td>\n",
       "      <td>25-CE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  case_number          filing_date defendant_normalized  \\\n",
       "11  0510491-75.2017.4.05.8103  2017-10-11 00:00:00                 INSS   \n",
       "16  0509917-52.2017.4.05.8103  2017-09-26 00:00:00                 INSS   \n",
       "19  0501051-55.2017.4.05.8103  2017-02-03 00:00:00                 INSS   \n",
       "23  0511681-76.2017.4.05.8102  2017-09-27 00:00:00                 INSS   \n",
       "33  0500777-79.2017.4.05.8107  2017-02-23 00:00:00                 INSS   \n",
       "\n",
       "   date_first_instance_ruling date_appeal_panel_ruling  case_topic_code  \\\n",
       "11        2018-01-16 11:16:19      2018-03-26 17:07:16             6095   \n",
       "16        2018-01-15 18:07:55      2018-03-26 17:07:16             6101   \n",
       "19        2017-11-21 11:44:40      2018-03-26 17:07:16             6103   \n",
       "23        2017-12-29 02:18:24      2018-03-26 17:07:16             6104   \n",
       "33        2017-11-27 17:58:46      2018-03-26 17:07:16             6103   \n",
       "\n",
       "      case_topic_1st_level   case_topic_2nd_level  \\\n",
       "11  Direito Previdenciário  Benefícios em Espécie   \n",
       "16  Direito Previdenciário  Benefícios em Espécie   \n",
       "19  Direito Previdenciário  Benefícios em Espécie   \n",
       "23  Direito Previdenciário  Benefícios em Espécie   \n",
       "33  Direito Previdenciário  Benefícios em Espécie   \n",
       "\n",
       "                case_topic_3rd_level court_id  \n",
       "11       Aposentadoria por Invalidez    19-CE  \n",
       "16     Auxílio-Doença Previdenciário    31-CE  \n",
       "19  Salário-Maternidade (Art. 71/73)    31-CE  \n",
       "23      Pensão por Morte (Art. 74/9)    30-CE  \n",
       "33  Salário-Maternidade (Art. 71/73)    25-CE  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['case_number', 'filing_date', 'defendant_normalized',\n",
       "       'date_first_instance_ruling', 'date_appeal_panel_ruling',\n",
       "       'case_topic_code', 'case_topic_1st_level', 'case_topic_2nd_level',\n",
       "       'case_topic_3rd_level', 'court_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['case_number', 'text', 'outcome', 'ruling_type'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_texts.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_texts.case_number.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Next Steps\n",
    "\n",
    "As we proceed with our analysis, we'll need to:\n",
    "1. Load the data into our working environment\n",
    "2. Perform initial exploratory data analysis to understand the distribution of ruling types and outcomes\n",
    "3. Consider any necessary data preprocessing steps, such as handling missing values or encoding categorical variables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) Pipeline\n",
    "\n",
    "Natural Language Processing (NLP) is a field at the intersection of computer science, artificial intelligence, and linguistics. It focuses on enabling computers to understand, interpret, and generate human language. The NLP pipeline is a crucial concept that outlines the structured series of steps required to extract valuable information from text data and prepare it for machine learning algorithms.\n",
    "\n",
    "## 1. Text Processing\n",
    "\n",
    "Text processing is the foundation of any NLP pipeline, transforming raw text into a format suitable for machine learning algorithms. This stage involves three key steps:\n",
    "\n",
    "### 1.1 Normalization\n",
    "\n",
    "Normalization standardizes text to ensure consistency across the dataset. This process can include:\n",
    "\n",
    "- **Lowercasing**: Converting all text to lowercase to eliminate case sensitivity.\n",
    "- **Punctuation removal**: Eliminating punctuation marks that may not contribute to meaning.\n",
    "- **Number removal**: Removing numerical digits, unless they're crucial for the task.\n",
    "- **Stop word removal**: Eliminating common words (e.g., \"the,\" \"is,\" \"at\") that often don't carry significant meaning.\n",
    "- **Stemming**: Reducing words to their root form (e.g., \"running\" to \"run\").\n",
    "- **Lemmatization**: Similar to stemming, but considers the context to produce a proper base form of the word.\n",
    "\n",
    "> **Note**: Deep learning techniques often bypass normalization. This is because these models can learn to handle variations in text representation during training, potentially preserving useful information that might be lost in normalization.\n",
    "\n",
    "### 1.2 Tokenization\n",
    "\n",
    "Tokenization breaks down text into smaller units called tokens. These can be:\n",
    "\n",
    "- **Words**: The most common form of tokenization.\n",
    "- **Characters**: Useful for tasks that require character-level analysis.\n",
    "- **Subwords**: Fragments of words that capture meaningful units (e.g., prefixes, suffixes).\n",
    "\n",
    "Tokenization is crucial as it defines the basic units that will be processed in subsequent steps.\n",
    "\n",
    "### 1.3 Numericalization\n",
    "\n",
    "Numericalization converts tokens into numerical representations that machines can process. This step bridges the gap between human-readable text and machine-processable data.\n",
    "\n",
    "## 2. Feature Extraction\n",
    "\n",
    "Feature extraction transforms the numerical representations of tokens into features that capture the essence of the text. This step is critical for making the text data interpretable for machine learning models. Common techniques include:\n",
    "\n",
    "- **Bag of Words (BoW)**: Represents text as a vector of word frequencies.\n",
    "- **Term Frequency-Inverse Document Frequency (TF-IDF)**: A statistical measure that evaluates the importance of a word in a document relative to a collection of documents.\n",
    "- **Word Embeddings**: Dense vector representations of words that capture semantic relationships (e.g., Word2Vec, GloVe).\n",
    "- **Sentence Embeddings**: Vector representations of entire sentences or paragraphs (e.g., Universal Sentence Encoder).\n",
    "\n",
    "These techniques aim to capture different aspects of language, from simple word occurrence to complex semantic relationships.\n",
    "\n",
    "## 3. Model Training\n",
    "\n",
    "Model training is where the extracted features are used to teach a machine learning model to perform specific NLP tasks. This stage involves:\n",
    "\n",
    "1. **Model Selection**: Choosing an appropriate algorithm based on the task (e.g., classification, named entity recognition, sentiment analysis).\n",
    "2. **Hyperparameter Tuning**: Adjusting the model's parameters to optimize performance.\n",
    "3. **Training Process**: Feeding the extracted features into the model and adjusting its internal parameters based on the observed errors.\n",
    "\n",
    "The choice of model can range from traditional machine learning algorithms (e.g., Naive Bayes, Support Vector Machines) to advanced deep learning architectures (e.g., Recurrent Neural Networks, Transformers).\n",
    "\n",
    "## 4. Model Evaluation\n",
    "\n",
    "Evaluation is crucial to assess the model's performance and ensure it generalizes well to unseen data. This stage typically involves:\n",
    "\n",
    "1. **Splitting the Data**: Dividing the dataset into training, validation, and test sets.\n",
    "2. **Metric Selection**: Choosing appropriate evaluation metrics based on the task (e.g., accuracy, precision, recall, F1-score).\n",
    "3. **Performance Assessment**: Applying the trained model to the test set and calculating the chosen metrics.\n",
    "4. **Error Analysis**: Examining misclassified examples to understand the model's weaknesses.\n",
    "\n",
    "It's important to use a separate validation set during training to tune hyperparameters and prevent overfitting. The test set should only be used for final evaluation to get an unbiased estimate of the model's performance.\n",
    "\n",
    "\n",
    "> **Important**: The NLP pipeline is an iterative process. Based on evaluation results, you may need to revisit earlier stages, adjusting text processing techniques, extracting different features, or selecting alternative models. This cyclical approach helps refine the pipeline and improve overall performance on the given NLP task.\n",
    "> Understanding each component of the NLP pipeline is crucial for effectively tackling various language-related problems in artificial intelligence and machine learning applications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement 1 - Classifying \"Acórdãos\"\n",
    "\n",
    "When a legal case is adjudicated by a judge, the involved parties have the option to appeal the decision to a higher court. This appellate court then reviews the case and issues a document called an **\"Acórdão\"**. The \"Acórdão\" details the court's decision and the reasoning behind it, playing a crucial role in the judicial process.\n",
    "\n",
    "### Purpose and Importance of \"Acórdãos\"\n",
    "\n",
    "The \"Acórdão\" can either:\n",
    "- **Uphold** the original decision made by the lower court.\n",
    "- **Overturn** the decision, leading to a different outcome.\n",
    "\n",
    "These documents are invaluable for understanding the application of laws in various contexts. They offer insights into:\n",
    "- **Judicial reasoning**: How judges interpret and apply the law.\n",
    "- **Legal precedents**: Past decisions that influence future cases.\n",
    "\n",
    "### Challenges in Analyzing \"Acórdãos\"\n",
    "\n",
    "Despite their importance, \"Acórdãos\" are written in natural language, which poses challenges for systematic analysis:\n",
    "1. **Complexity of Legal Language**: Legal terminology and reasoning can be elaborate and difficult to parse for those without legal training.\n",
    "2. **Volume of Data**: The sheer number of \"Acórdãos\" can be overwhelming, making manual analysis impractical.\n",
    "\n",
    "To address these challenges, automated classification techniques can be employed to categorize and extract meaningful insights from these documents. By training machine learning models on labeled \"Acórdãos,\" we can develop systems that classify the outcome of an appeal based on the content of the document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement 2 - Predicting the Time to Issue an \"Acórdão\"\n",
    "\n",
    "Once a case has been tried by a judge, the appellate court issues the \"Acórdão\" after a certain period. The duration between the initial trial and the issuance of the \"Acórdão\" can vary significantly, ranging from a few days to several months.\n",
    "\n",
    "### Effects of the Time Frame\n",
    "\n",
    "The time it takes to issue an \"Acórdão\" has significant consequences for the parties involved:\n",
    "- **Closure**: If the \"Acórdão\" supports the original decision, parties can find closure and move forward.\n",
    "- **Uncertainty**: If the \"Acórdão\" overrules the initial decision, the parties may face prolonged uncertainty, potentially lasting months or years.\n",
    "\n",
    "### Calculating the Time Frame\n",
    "\n",
    "In the dataset, the time it takes to issue the \"Acórdão\" can be derived by calculating the difference between two columns:\n",
    "- `date_first_instance_ruling`: The date when the initial trial decision was made.\n",
    "- `date_appeal_panel_ruling`: The date when the appellate court issued the \"Acórdão\".\n",
    "\n",
    "This calculation will provide an accurate representation of the time duration between the trial and the issuance of the \"Acórdão\".\n",
    "\n",
    "> **Note**: When developing predictive models, it is essential to use only the data available at the time of prediction. Therefore, the `date_appeal_panel_ruling` column cannot be used as an input for the model since it is only available after the trial is completed. This prevents data leakage and ensures the model's practical utility.\n",
    "\n",
    "### Potential Predictive Features\n",
    "\n",
    "To predict the duration between the trial and the issuance of the \"Acórdão,\" consider using features available at the time of the initial trial, such as:\n",
    "- **Case characteristics**: Type of case, complexity, involved parties.\n",
    "- **Court attributes**: Location, workload, historical averages.\n",
    "- **Judge information**: Identity, workload, historical performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Classifying \"Acórdãos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 4 columns):\n",
      " #   Column                    Non-Null Count  Dtype         \n",
      "---  ------                    --------------  -----         \n",
      " 0   case_number               20000 non-null  object        \n",
      " 1   text                      20000 non-null  object        \n",
      " 2   outcome                   20000 non-null  object        \n",
      " 3   date_appeal_panel_ruling  20000 non-null  datetime64[us]\n",
      "dtypes: datetime64[us](1), object(3)\n",
      "memory usage: 625.1+ KB\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataframe to include only rows where the ruling type is \"ACÓRDÃO\"\n",
    "# Select only the columns 'case_number', 'text', and 'outcome'\n",
    "df = df_texts.query('ruling_type == \"ACÓRDÃO\"')[\n",
    "    [\"case_number\", \"text\", \"outcome\"]\n",
    "].copy()\n",
    "\n",
    "# Merge the filtered dataframe with another dataframe containing metadata\n",
    "# Specifically, we are adding the 'date_appeal_panel_ruling' column based on 'case_number'\n",
    "df = df.merge(\n",
    "    df_meta[[\"case_number\", \"date_appeal_panel_ruling\"]], on=\"case_number\", how=\"left\"\n",
    ")\n",
    "\n",
    "# Display the summary information of the resulting dataframe\n",
    "# This includes the number of entries, column names, non-null counts, and data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outcome\n",
       "NÃO PROVIMENTO        0.77905\n",
       "PROVIMENTO            0.15020\n",
       "PROVIMENTO PARCIAL    0.07075\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.outcome.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outcome_int\n",
       "0    15581\n",
       "1     3004\n",
       "2     1415\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the 'outcome' column to a categorical type and then to integer codes\n",
    "# This is useful for machine learning models that require numerical input\n",
    "df[\"outcome_int\"] = df.outcome.astype(\"category\").cat.codes\n",
    "\n",
    "# Display the count of each unique integer code in the 'outcome_int' column\n",
    "# This helps to understand the distribution of different outcomes in the dataset\n",
    "df.outcome_int.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Split the data into training, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16000, 5), (2000, 5), (2000, 5))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataframe into training and testing sets\n",
    "# Use stratified sampling based on the 'outcome' column to ensure balanced classes\n",
    "# Set aside 20% of the data for testing\n",
    "df_train, df_test = train_test_split(\n",
    "    df, test_size=0.2, random_state=271828, stratify=df.outcome\n",
    ")\n",
    "\n",
    "# Further split the testing set into validation and testing sets\n",
    "# Use stratified sampling based on the 'outcome' column to ensure balanced classes\n",
    "# Set aside 50% of the testing set for validation, resulting in 10% of the original data\n",
    "df_test, df_val = train_test_split(\n",
    "    df_test, test_size=0.5, random_state=271828, stratify=df_test.outcome\n",
    ")\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "# This helps to verify the sizes of the training, testing, and validation sets\n",
    "df_train.shape, df_test.shape, df_val.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Importance of Proper Data Splitting in Legal Ruling Analysis\n",
    "\n",
    "When working with datasets containing legal rulings, it's crucial to recognize the innate temporal nature of the data. Legal cases and their corresponding rulings typically follow a chronological order, often organized by case number or date. This temporal aspect introduces unique challenges and considerations for data analysis and model training.\n",
    "\n",
    "### Pitfalls of Random Data Splitting\n",
    "\n",
    "Randomly splitting legal ruling data can lead to several significant issues:\n",
    "\n",
    "1. **Temporal Leakage**: This is the primary concern when randomly splitting time-ordered data. Temporal leakage occurs when a model is inadvertently trained on future information, leading to:\n",
    "    - Overly optimistic performance estimates\n",
    "    - Models that fail to generalize well to truly unseen data\n",
    "    - Unrealistic predictions based on future knowledge\n",
    "\n",
    "2. **Disruption of Legal Precedent**: Legal systems often rely on precedent, where earlier rulings influence later ones. Random splitting can break these important temporal relationships.\n",
    "\n",
    "3. **Misrepresentation of Legal Trends**: Laws and their interpretations progress over time. Random splitting may obscure these trends, leading to a model that doesn't accurately capture the current legal landscape.\n",
    "\n",
    "### Best Practices for Splitting Legal Ruling Data\n",
    "\n",
    "To address these challenges, consider the following approach:\n",
    "\n",
    "1. **Chronological Splitting**: Divide the dataset based on the date of the rulings:\n",
    "    - Training Set: Earlier rulings\n",
    "    - Validation Set: Intermediate rulings\n",
    "    - Test Set: Most recent rulings\n",
    "\n",
    "2. **Preserving Temporal Order**: This method maintains the natural progression of legal precedents and trends.\n",
    "\n",
    "3. **Realistic Model Evaluation**: By testing on the most recent data, you can better assess how well your model will perform on truly future cases.\n",
    "\n",
    "### Benefits of Proper Data Splitting\n",
    "\n",
    "1. **Improved Model Generalization**: Your model learns from historical patterns and is tested on more recent, unseen data, better mimicking real-world applications.\n",
    "\n",
    "2. **Accurate Performance Assessment**: Evaluating on chronologically later data provides a more realistic measure of model performance.\n",
    "\n",
    "\n",
    "### Considerations for Implementation\n",
    "\n",
    "- **Time Window Selection**: Carefully consider the time spans for each split to ensure sufficient data in each set while maintaining temporal integrity.\n",
    "- **Handling Landmark Cases**: Be aware of significant legal changes or landmark cases that might dramatically shift legal interpretations.\n",
    "- **Regular Retraining**: In rapidly evolving legal areas, consider implementing a system for regular model retraining with the most up-to-date data.\n",
    "\n",
    "> **Key Takeaway**: In the analysis of legal rulings, chronological data splitting is not just a best practice—it's essential for creating reliable, ethical, and practically applicable models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16000, 5), (2000, 5), (2000, 5))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of rows for the training set (80% of the total dataframe length)\n",
    "train_length = int(0.8 * len(df))\n",
    "\n",
    "# Calculate the number of rows for the validation set (10% of the total dataframe length)\n",
    "validation_length = int(0.1 * len(df))\n",
    "\n",
    "# Sort the dataframe by the 'date_appeal_panel_ruling' column in ascending order\n",
    "# This ensures that the data is split chronologically, which can be important for time series data\n",
    "df = df.sort_values(by=\"date_appeal_panel_ruling\", ascending=True)\n",
    "\n",
    "# Split the dataframe into training, validation, and testing sets based on the calculated lengths\n",
    "# The training set includes the first 80% of the rows\n",
    "df_train = df.iloc[:train_length].copy()\n",
    "\n",
    "# The validation set includes the next 10% of the rows\n",
    "df_val = df.iloc[train_length : train_length + validation_length].copy()\n",
    "\n",
    "# The testing set includes the remaining 10% of the rows\n",
    "df_test = df.iloc[train_length + validation_length :].copy()\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "# This helps to verify the sizes of the training, validation, and testing sets\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2018-03-26 17:02:45'), Timestamp('2019-09-30 17:20:24'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.date_appeal_panel_ruling.min(), df_train.date_appeal_panel_ruling.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2019-09-30 17:20:24'), Timestamp('2019-12-11 14:57:02'))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.date_appeal_panel_ruling.min(), df_val.date_appeal_panel_ruling.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2019-12-11 14:57:02'), Timestamp('2020-04-01 11:24:06'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.date_appeal_panel_ruling.min(), df_test.date_appeal_panel_ruling.max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Text Processing and vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary functions from the 'helpers.text' module\n",
    "from helpers.text import (\n",
    "    remove_accented_characters,\n",
    "    remove_numbers_punctuation_from_text,\n",
    "    remove_excessive_spaces,\n",
    "    remove_short_words,\n",
    ")\n",
    "\n",
    "# Apply the cleaning functions to the 'text' column of each dataframe\n",
    "for dataframe in [df_train, df_val, df_test]:\n",
    "    dataframe[\"clean_text\"] = dataframe.text.apply(remove_accented_characters)\n",
    "    dataframe[\"clean_text\"] = dataframe.clean_text.apply(\n",
    "        remove_numbers_punctuation_from_text\n",
    "    )\n",
    "    dataframe[\"clean_text\"] = dataframe.clean_text.apply(remove_excessive_spaces)\n",
    "    dataframe[\"clean_text\"] = dataframe.clean_text.apply(remove_short_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RELATORIO Trata recurso interposto pela parte autora face sentenca que julgou improcedente pedido reajuste beneficio com base Indice Precos Consumidor Terceira Idade IPC Instituto Brasileiro Economia Fundacao Getulio Vargas FGV IBRE substituicao Indice Nacional Precos Consumidor INPC Instituto Brasileiro Geografia Estatistica IBGE alegando que este indice aplicado pelo INSS nao preservaria carater permanente valor real dos beneficios previdenciarios que afrontaria comando art VOTO Conforme bem fundamenta sentenca recorrida preservacao valor real dos beneficios assegurada pela aplicacao dos indices estabelecidos pela propria legislacao previdenciaria nao cabendo poder judiciario substituir indice eleito pelo legislador fato forma indices reajustes que devem ser aplicados aos beneficios previdenciarios concedidos apos sao aqueles estabelecidos pela Lei uma vez que Carta Magna remeteu legislador ordinario definicao dos indices serem aplicados aos reajustes dos beneficios para preservacao seu valor real art paragrafo redacao dada pela antigo art paragrafo com identico teor Limita Previdencia Social aplicar legislacao vigor suposta defasagem alegada pela apelante nao decorreu criterio administrativo que procurasse diminuir despesas com custeio dos beneficios Sendo assim correcao possivel injustica escapa aos limites controle Poder Judiciario que pode agir apenas como legislador negativo nao lhe sendo permitido editar dispositivo legal que possa restituir aos beneficiarios diferencas que decorreram exclusivamente aplicacao indices previstos nas proprias normas previdenciarias Desse modo evidencia que preservacao valor real dos beneficios previdenciarios assegurada pela aplicacao dos indices estabelecidos pela propria legislacao previdenciaria seja INPC IRSM IPC IGP outros previstos ordenamento juridico TRF DJE Data Pagina Nesse mesmo sentido Supremo Tribunal Federal julgamento firmou entendimento que criterio reajuste dos beneficios previdenciarios forma preservar lhes valor real art definido pelo legislador ordinario que restou concretizado com Lei sendo vedado reajustamento com base outros criterios Assim analisando atentamente sentenca recorrida constata que Juizo quo formou seu convencimento luz uma analise adequada dos fatos aplicando corretamente normas regencia Por tal razao deve julgado ser mantido todos seus termos pelos proprios fundamentos forma prevista art Lei Tem por expressamente prequestionadas todas questoes constitucionais suscitadas uma vez que para fins prequestionamento desnecessaria indicacao expressa artigos paragrafos Constituicao Federal afigurando suficiente sejam regras neles contidas fundamento decisum objeto discussao como caso ora sob exame AgR Relator Min GILMAR MENDES Segunda Turma Ante exposto NEGO PROVIMENTO recurso para confirmar sentenca que julgou improcedente pedido formulado inicial Condenacao recorrente honorarios advocaticios fixados dez por cento sobre valor corrigido causa art Lei suspensa execucao desta parcela enquanto litigar sob palio gratuidade judiciaria ACORDAO Decide Segunda Turma Recursal Secao Judiciaria Ceara unanimidade negar provimento recurso nos termos voto relator manifestacoes gravadas Participaram julgamento Exmos Srs Juizes Federais Gustavo Melo Barbosa Paula Emilia Moura Aragao Sousa Brasil Dartanhan Vercingetorix Araujo Rocha Fortaleza GUSTAVO MELO BARBOSA JUIZ FEDERAL RELATORIA'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[0][\"clean_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load Portuguese stopwords from NLTK\n",
    "stopwords_nltk = stopwords.words(\"portuguese\")\n",
    "\n",
    "# Create a TF-IDF vectorizer with specific parameters:\n",
    "# - stop_words: remove common Portuguese stopwords\n",
    "# - max_features: limit the number of features to 1000\n",
    "# - ngram_range: consider unigrams and bigrams\n",
    "# - min_df: ignore terms that appear in fewer than 5 documents\n",
    "# - max_df: ignore terms that appear in more than 80% of documents\n",
    "# - lowercase: convert all text to lowercase\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=stopwords_nltk,\n",
    "    max_features=1000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_df=0.8,\n",
    "    lowercase=True,\n",
    ")\n",
    "\n",
    "# Fit the vectorizer using only the training data\n",
    "# This ensures that the model does not have access to the validation or test data during training\n",
    "vectorizer.fit(df_train.clean_text)\n",
    "\n",
    "# Transform the text data into TF-IDF vectors\n",
    "# This converts the text data into numerical form suitable for machine learning models\n",
    "X_train = vectorizer.transform(df_train.clean_text)\n",
    "X_val = vectorizer.transform(df_val.clean_text)\n",
    "X_test = vectorizer.transform(df_test.clean_text)\n",
    "\n",
    "# Extract the target labels for training, validation, and testing sets\n",
    "# These labels will be used to train and evaluate the machine learning model\n",
    "y_train = df_train.outcome_int\n",
    "y_val = df_val.outcome_int\n",
    "y_test = df_test.outcome_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>outcome_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13171</th>\n",
       "      <td>EMENTA PREVIDENCIARIO AUXILIO DOENCA LAUDO DES...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3344</th>\n",
       "      <td>EMENTA PREVIDENCIARIO BENEFICIO ASSISTENCIAL L...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>VOTO Dispensado relatorio nos termos art Lei a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17639</th>\n",
       "      <td>PROCESSO EMENTA PROCESSO CIVIL ASSISTENCIA SOC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17063</th>\n",
       "      <td>PROCESSO EMENTA ADMINISTRATIVO SERVIDOR PUBLIC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15959</th>\n",
       "      <td>EMENTA PREVIDENCIARIO AUXILIO DOENCA APOSENTAD...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>RECURSO INOMINADO DIREITO PREVIDENCIARIO PEDID...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4637</th>\n",
       "      <td>RECURSO INOMINADO DIREITO ADMINISTRATIVO SEGUR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19977</th>\n",
       "      <td>VOTO EMENTA PREVIDENCIARIO BENEFICIO PREVIDENC...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>PROCESSO RECORRENTE MARIA LOURDES CONCEICAO RE...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              clean_text  outcome_int\n",
       "13171  EMENTA PREVIDENCIARIO AUXILIO DOENCA LAUDO DES...            1\n",
       "3344   EMENTA PREVIDENCIARIO BENEFICIO ASSISTENCIAL L...            0\n",
       "2508   VOTO Dispensado relatorio nos termos art Lei a...            0\n",
       "17639  PROCESSO EMENTA PROCESSO CIVIL ASSISTENCIA SOC...            0\n",
       "17063  PROCESSO EMENTA ADMINISTRATIVO SERVIDOR PUBLIC...            0\n",
       "15959  EMENTA PREVIDENCIARIO AUXILIO DOENCA APOSENTAD...            0\n",
       "668    RECURSO INOMINADO DIREITO PREVIDENCIARIO PEDID...            0\n",
       "4637   RECURSO INOMINADO DIREITO ADMINISTRATIVO SEGUR...            1\n",
       "19977  VOTO EMENTA PREVIDENCIARIO BENEFICIO PREVIDENC...            0\n",
       "1950   PROCESSO RECORRENTE MARIA LOURDES CONCEICAO RE...            0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[[\"clean_text\", \"outcome_int\"]].sample(10, random_state=271828)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16000, 1000), (2000, 1000), (2000, 1000))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16000,), (2000,), (2000,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 244 stored elements and shape (1, 1000)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.toarray()\n",
    "X_val = X_val.toarray()\n",
    "X_test = X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.05059559, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.0465296 , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0376958 , 0.        ,\n",
       "       0.02821556, 0.03924224, 0.04661888, 0.04740649, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.04219101, 0.04322569,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.05352028, 0.        ,\n",
       "       0.03443451, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.03712143, 0.04413272, 0.        , 0.        ,\n",
       "       0.        , 0.03417699, 0.        , 0.12682283, 0.05464034,\n",
       "       0.        , 0.04136232, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.044617  , 0.04745553, 0.        ,\n",
       "       0.05451618, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05227756,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.09059519, 0.        , 0.03219794,\n",
       "       0.        , 0.0246106 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.33517606, 0.06283665, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.05406567, 0.        , 0.        ,\n",
       "       0.        , 0.06429373, 0.        , 0.        , 0.        ,\n",
       "       0.04702323, 0.        , 0.03097571, 0.04962608, 0.0404588 ,\n",
       "       0.04128996, 0.0328678 , 0.03925528, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0299464 , 0.05591626,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.02944388, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.05533349, 0.04347378, 0.04735268, 0.        , 0.03589797,\n",
       "       0.03724208, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.04559522, 0.04742119, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.0493922 , 0.        , 0.04427468, 0.        ,\n",
       "       0.05531895, 0.05426459, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.12709299, 0.05378103, 0.        , 0.        ,\n",
       "       0.        , 0.04390106, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.040228  , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.04587916, 0.0474408 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.04406625, 0.0474408 , 0.        , 0.        , 0.        ,\n",
       "       0.04701843, 0.05456323, 0.03916088, 0.        , 0.        ,\n",
       "       0.        , 0.02735704, 0.04705208, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.03435256, 0.03592006,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04458293, 0.04742119, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.06040159, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.0382157 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.04731852,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.04512636, 0.        ,\n",
       "       0.03574673, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.03388275,\n",
       "       0.04740649, 0.04882179, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.04361925, 0.0449176 ,\n",
       "       0.        , 0.        , 0.03571935, 0.05289896, 0.04246481,\n",
       "       0.04740649, 0.04107803, 0.04730877, 0.        , 0.0339628 ,\n",
       "       0.03884511, 0.        , 0.        , 0.        , 0.03843689,\n",
       "       0.        , 0.04601123, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.02544722, 0.        , 0.        , 0.04740159,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.03212691,\n",
       "       0.04516575, 0.05356683, 0.        , 0.        , 0.        ,\n",
       "       0.03530776, 0.04837479, 0.        , 0.08468056, 0.        ,\n",
       "       0.04108875, 0.        , 0.        , 0.        , 0.03744081,\n",
       "       0.04743099, 0.03508917, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.04559075, 0.04566695,\n",
       "       0.        , 0.04269587, 0.04885861, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.02824381, 0.04343758,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07136763, 0.07911548, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.04291395, 0.04734292,\n",
       "       0.19713738, 0.        , 0.29932971, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.04286702,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.11732665, 0.03549626, 0.11204264, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.0314743 , 0.04252233, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.05756893, 0.1128022 , 0.06816717, 0.03297966, 0.04612119,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.03124095,\n",
       "       0.04130803, 0.        , 0.03292484, 0.05575968, 0.        ,\n",
       "       0.02936998, 0.        , 0.        , 0.        , 0.04365985,\n",
       "       0.05814232, 0.0796423 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.04546586,\n",
       "       0.14345159, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.049081  , 0.        , 0.        , 0.0561345 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.08924252, 0.        , 0.        , 0.04539484, 0.04793251,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.03360261, 0.04977434,\n",
       "       0.        , 0.        , 0.        , 0.03813222, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.03967402, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.02834964,\n",
       "       0.02885624, 0.04623175, 0.04654367, 0.        , 0.04735268,\n",
       "       0.04743099, 0.04142766, 0.05184519, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.10054191,\n",
       "       0.        , 0.        , 0.        , 0.04109233, 0.04736734,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.03658304, 0.04717275,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07978351, 0.        , 0.        , 0.        ,\n",
       "       0.09539514, 0.04589278, 0.04747518, 0.        , 0.        ,\n",
       "       0.06098081, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04236932, 0.04271525, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.04997372, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.05495893, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.03562382,\n",
       "       0.        , 0.        , 0.1085154 , 0.12097201, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.04396697, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04727468, 0.04733803, 0.04348989,\n",
       "       0.04748993, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05400433,\n",
       "       0.05586393, 0.10274828, 0.        , 0.        , 0.        ,\n",
       "       0.25453707, 0.        , 0.04776727, 0.05330884, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.08484548, 0.        , 0.03798773,\n",
       "       0.03873989, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.04399588,\n",
       "       0.04736246, 0.05166876, 0.02993614, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.02539155, 0.0520973 ,\n",
       "       0.        , 0.08137054, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.04096046, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04443455, 0.        , 0.03603646,\n",
       "       0.        , 0.04592915, 0.05031054, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05384168,\n",
       "       0.        , 0.        , 0.        , 0.04170223, 0.04747518,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.0512006 , 0.03794795, 0.        ,\n",
       "       0.        , 0.04975781, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.0476926 , 0.        , 0.05818834, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.03702485, 0.        , 0.03038618, 0.03284171, 0.        ,\n",
       "       0.        , 0.        , 0.07300076, 0.07485778, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.0868961 , 0.        , 0.06242942, 0.        ,\n",
       "       0.08673873, 0.        , 0.03570841, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.05779808, 0.04641272, 0.02544066, 0.03106446,\n",
       "       0.        , 0.03645776, 0.        , 0.        , 0.04261084,\n",
       "       0.04351408, 0.        , 0.        , 0.        , 0.06963947,\n",
       "       0.06711995, 0.03441912, 0.04746044, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.04801817, 0.04813465, 0.04713406, 0.04752931, 0.03941252,\n",
       "       0.        , 0.03429387, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.04553713, 0.        , 0.        , 0.05331541, 0.        ,\n",
       "       0.        , 0.        , 0.02849227, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.03815074, 0.04733803, 0.        ,\n",
       "       0.        , 0.03504415, 0.05122547, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.03110137, 0.03687939, 0.        , 0.        , 0.        ,\n",
       "       0.06407899, 0.0379663 , 0.04808899, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.03139695, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.15181088, 0.        ,\n",
       "       0.        , 0.        , 0.06515272, 0.        , 0.        ,\n",
       "       0.        , 0.05557078, 0.04733803, 0.        , 0.        ,\n",
       "       0.        , 0.06348092, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.03302745, 0.        ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['incapacidade', 'beneficio', 'doenca', 'auxilio', 'atividade',\n",
       "       'auxilio doenca', 'aposentadoria', 'segurado', 'laudo',\n",
       "       'concessao', 'prova', 'especial', 'rural', 'fgts', 'anexo',\n",
       "       'autor', 'monetaria', 'inss', 'periodo', 'pericial', 'invalidez',\n",
       "       'correcao', 'trabalho', 'anos', 'tempo', 'aposentadoria invalidez',\n",
       "       'social', 'prazo', 'carencia', 'direito'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_top_ngrams(\n",
    "    X_train: np.ndarray, vectorizer: TfidfVectorizer, top_n: int = 30\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Get the top n most frequent n-grams from the vectorized text data.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): The vectorized text data.\n",
    "        vectorizer (TfidfVectorizer): The vectorizer used to transform the text data.\n",
    "        top_n (int, optional): The number of top n-grams to return. Defaults to 30.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An array of the top n most frequent n-grams.\n",
    "    \"\"\"\n",
    "    # Sum all the columns to get the total frequency of each n-gram\n",
    "    total_ngram_frequencies = np.sum(X_train, axis=0)\n",
    "\n",
    "    # Sort the n-grams by their total frequency\n",
    "    sorted_ngrams_indices = np.argsort(total_ngram_frequencies)[::-1]\n",
    "\n",
    "    # Get the indices of the top n most frequent n-grams\n",
    "    top_ngrams_indices = sorted_ngrams_indices[:top_n]\n",
    "\n",
    "    # Get the names of the n-grams corresponding to the top n indices\n",
    "    ngram_names = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "    return ngram_names[top_ngrams_indices]\n",
    "\n",
    "\n",
    "# Use the function to get the top 30 n-grams from the training data\n",
    "top_ngrams = get_top_ngrams(X_train, vectorizer, top_n=30)\n",
    "top_ngrams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Import various classifiers and utilities from scikit-learn and other libraries\n",
    "\n",
    "# LightGBM classifier, a gradient boosting framework that uses tree-based learning algorithms\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# CalibratedClassifierCV for probability calibration of classifiers\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Ensemble classifiers from scikit-learn\n",
    "# ExtraTreesClassifier and RandomForestClassifier are ensemble methods that use multiple decision trees\n",
    "# StackingClassifier allows combining multiple classifiers to improve performance\n",
    "from sklearn.ensemble import (\n",
    "    ExtraTreesClassifier,\n",
    "    RandomForestClassifier,\n",
    "    StackingClassifier,\n",
    ")\n",
    "\n",
    "# Linear models from scikit-learn\n",
    "# LogisticRegression is a linear model for binary classification\n",
    "# SGDClassifier is a linear classifier using stochastic gradient descent\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "\n",
    "# Metrics for evaluating classification performance\n",
    "# accuracy_score, balanced_accuracy_score, classification_report, confusion_matrix, f1_score, matthews_corrcoef\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    ")\n",
    "\n",
    "# Naive Bayes classifier for multinomially distributed data\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# K-Nearest Neighbors classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Neural network-based classifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Support Vector Machine classifiers\n",
    "# SVC is a support vector classifier with a non-linear kernel\n",
    "# LinearSVC is a support vector classifier with a linear kernel\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "# Decision tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# XGBoost classifier, an optimized distributed gradient boosting library\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Guide to Classifiers\n",
    "\n",
    "This section aims to provide a systematic explanation of several classifiers that can be used for machine learning tasks. Each classifier is described in terms of its working principles and use cases.\n",
    "\n",
    "\n",
    "#### 1. Calibrated-LSVC: CalibratedClassifierCV with LinearSVC\n",
    "**CalibratedClassifierCV** serves as a probability calibration wrapper assisting classification models, which by default do not support the prediction of class probabilities. Complementing this, **LinearSVC** operates as a linear Support Vector Machine (SVM) classifier, separating classes using hyperplanes in a high-dimensional space. When combined, these two elements form a calibrated SVM model capable of predicting class probabilities.\n",
    "\n",
    "- **Key Concepts**:\n",
    "    - Linear SVM: Separates classes with straight-line boundaries (hyperplanes).\n",
    "    - Probability Calibration: Adjusts the output scores of the classifier to produce better probability estimates.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/linear_svm.png\" alt=\"Linear SVM\" style=\"width: 40%; height: 40%\"/>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. LR: Logistic Regression\n",
    "**Logistic Regression** is a classifier that uses a logistic function to predict the probability of an instance belonging to a specific class. It is particularly useful for binary classification tasks but can be extended to multi-class problems.\n",
    "\n",
    "- **Key Concepts**:\n",
    "    - Logistic Function: Maps predicted values to probabilities between 0 and 1.\n",
    "    - Applications: Binary and multi-class classification problems like spam detection, disease diagnosis.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/logistic_regression.png\" alt=\"Logistic Regression\" style=\"width: 40%; height: 40%\"/>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. RF: Random Forest Classifier\n",
    "The **RandomForestClassifier** is an ensemble method that constructs numerous decision trees during training and outputs the class that receives the majority vote from all trees. This technique enhances the robustness and accuracy of the model compared to single decision trees.\n",
    "\n",
    "- **Key Concepts**:\n",
    "    - Ensemble Learning: Combines multiple models to improve performance.\n",
    "    - Decision Trees: Simple models that split data based on feature values.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/random_forest.png\" alt=\"Random Forest\" style=\"width: 40%; height: 40%\"/>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. LGBM: Light Gradient Boosting Machine Classifier\n",
    "The **LGBMClassifier** is a gradient boosting framework that uses tree-based learning algorithms. Designed for efficiency and scalability, it requires less memory and is proficient in handling large datasets with diverse features.\n",
    "\n",
    "- **Key Concepts**:\n",
    "    - Gradient Boosting: Sequentially builds models to correct errors of previous models.\n",
    "    - Efficiency: Optimized for speed and memory usage.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/xgboost_lgbm.png\" alt=\"LightGBM\" style=\"width: 40%; height: 40%\"/>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. XGB: XGBoost Classifier\n",
    "The **XGBClassifier** is an implementation of the eXtreme Gradient Boosting algorithm. It's optimized for speed and performance, using parallelization and regularization techniques to enhance model training and improve generalization.\n",
    "\n",
    "- **Key Concepts**:\n",
    "    - Regularization: Helps prevent overfitting by adding a penalty to complex models.\n",
    "    - Parallelization: Increases computational efficiency by performing operations simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. MLP: Multi-Layer Perceptron Classifier\n",
    "A feedforward neural network classifier, the **MLPClassifier** consists of an input layer, one or more hidden layers, and an output layer. This structure enables it to model complex and non-linear relationships between variables.\n",
    "\n",
    "- **Key Concepts**:\n",
    "    - Neural Networks: Layers of interconnected nodes (neurons) that process data.\n",
    "    - Non-linear Relationships: Can model data that isn't linearly separable.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/mlp.png\" alt=\"MLP\" style=\"width: 40%; height: 40%\"/>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. SGD: Stochastic Gradient Descent Classifier\n",
    "The **SGDClassifier** is a linear classifier optimized using stochastic gradient descent. It can work with several loss functions and regularization techniques, making it flexible and efficient for sparse and large-scale datasets.\n",
    "\n",
    "- **Key Concepts**:\n",
    "    - Stochastic Gradient Descent: An optimization algorithm that updates model parameters incrementally.\n",
    "    - Sparse Data: Datasets with a lot of zero values, common in text and high-dimensional data.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/sgd.png\" alt=\"SGD\" style=\"width: 40%; height: 40%\"/>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "#### 8. NB: Multinomial Naive Bayes\n",
    "The **MultinomialNB** classifier assumes that feature vectors represent counts or frequency data. It is particularly effective for text classification tasks, where each feature indicates the occurrence frequency of a specific word in a document.\n",
    "\n",
    "- **Key Concepts**:\n",
    "    - Naive Bayes: A probabilistic classifier based on Bayes' theorem.\n",
    "    - Multinomial Distribution: Suitable for discrete features like word counts.\n",
    "\n",
    "> The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\n",
    "\n",
    "---\n",
    "\n",
    "#### 9. LSVC: Linear Support Vector Classification\n",
    "**LinearSVC** is another way to implement a linear Support Vector Machine classifier. It operates similarly to the combination of `CalibratedClassifierCV` with `LinearSVC`, but it does not inherently support class probability prediction.\n",
    "\n",
    "- **Key Concepts**:\n",
    "    - Linear SVM: Separates data points through hyperplanes in high-dimensional space.\n",
    "    - Class Probability: Unlike Calibrated-LSVC, it doesn't provide probability estimates.\n",
    "\n",
    "---\n",
    "\n",
    "#### 10. KNN: K-Nearest Neighbors Classifier\n",
    "The **KNeighborsClassifier** uses a non-parametric method to classify new instances based on a majority vote among its k-nearest neighbors in the training set. Its simplicity and effectiveness make it useful for various classification problems, especially with smaller datasets.\n",
    "\n",
    "- **Key Concepts**:\n",
    "    - Non-parametric: Doesn't assume a specific form for the data distribution.\n",
    "    - K-nearest Neighbors: Classifies based on the proximity of training data points.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/knn.png\" alt=\"KNN\" style=\"width: 40%; height: 40%\"/>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "#### 11. DT: Decision Tree Classifier\n",
    "The **DecisionTreeClassifier** is a tree-based classifier that recursively divides input data into subsets, forming decision nodes until reaching a specified termination condition. This results in a hierarchical structure that is easy to understand and visualize.\n",
    "\n",
    "- **Key Concepts**:\n",
    "    - Recursive Partitioning: Splits data based on feature values.\n",
    "    - Overfitting: Can be prone to fitting the training data too closely.\n",
    "\n",
    "> While prone to overfitting, decision trees' performance can significantly be improved when grouped with ensemble methods like Random Forests.\n",
    "\n",
    "---\n",
    "\n",
    "#### 12. ET: Extra Trees Classifier\n",
    "The **ExtraTreesClassifier** is another ensemble method that builds multiple decision trees during training and outputs the class receiving the most votes from all decision trees. It opts for random thresholds for each feature instead of seeking the best split point, leading to faster training times and reduced memory usage.\n",
    "\n",
    "- **Key Concepts**:\n",
    "    - Random Thresholds: Uses random splits rather than the best split.\n",
    "    - Efficiency: Faster training due to random splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics explanation\n",
    "\n",
    "Let's see a clear and thorough explanation of various key metrics used for evaluating classification models. We'll be examining their definition, usage, and range where applicable. This should provide a deeper understanding of how these metrics impact the performance interpretation of your model.\n",
    "\n",
    "#### 1. F1 Score\n",
    "```python\n",
    "f1 = f1_score(y, pred, average='micro')\n",
    "```\n",
    "The **F1 Score** is defined as the harmonic mean of precision and recall. It oscillates between 0 and 1, with 0 representing the worst possible value and 1 symbolizing the best. When using `average='micro'`, this calculates the F1 score across all instances, considering the total number of true positives, false negatives, and false positives.\n",
    "\n",
    "---\n",
    "#### 2. Balanced Accuracy\n",
    "```python\n",
    "bacc = balanced_accuracy_score(y, pred)\n",
    "```\n",
    "**Balanced Accuracy** represents the average of the recall obtained on each class — often referred to as macro-average recall. This metric stands out in its effectiveness when dealing with imbalanced datasets.\n",
    "\n",
    "---\n",
    "#### 3. Accuracy\n",
    "```python\n",
    "acc = accuracy_score(y, pred)\n",
    "```\n",
    "**Accuracy** measures the proportion of correct predictions over the total number of input samples. It can vary from 0 (when there are zero correct predictions) to 1 (when all predictions are correct).\n",
    "\n",
    "---\n",
    "#### 4. Classification Report\n",
    "```python\n",
    "cr = classification_report(y, pred)\n",
    "```\n",
    "A **Classification Report** offers a detailed summary of essential classification metrics, including precision, recall, F1-score, and support (the number of actual occurrences of the class in the specified dataset) per class.\n",
    "\n",
    "---\n",
    "#### 5. Matthews Correlation Coefficient\n",
    "```python\n",
    "mcc = matthews_corrcoef(y, pred)\n",
    "```\n",
    "The **Matthews Correlation Coefficient (MCC)** serves as an indicator of the quality of binary classifications. Taking into account true and false positives as well as negatives, it is often seen as a balanced measure that can be applied even when classes have significantly different sizes. The MCC ranges between -1 and 1, where -1 denotes total disagreement between prediction and observation, 0 implies no better than random prediction, and 1 indicates flawless agreement.\n",
    "\n",
    "---\n",
    "#### 6. Confusion Matrix\n",
    "```python\n",
    "cm = confusion_matrix(y, pred)\n",
    "```\n",
    "The **Confusion Matrix** is a categorized table that describes the performance of a classification model against a dataset with known true values. It provides a visualization of the number of correct and incorrect predictions made by each class.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goodhart's Law and Classification Metrics: Balancing Numbers with Real-World Impact\n",
    "\n",
    "> \"When a measure becomes a target, it ceases to be a good measure.\" - Goodhart's Law\n",
    "\n",
    "#### The Essence of Goodhart's Law in Machine Learning\n",
    "\n",
    "Goodhart's Law, encapsulated in the quote above, is a critical concept in machine learning that reminds us to look beyond mere numbers. This principle is particularly relevant when interpreting and applying classification metrics, as it highlights the potential pitfalls of overly focusing on optimizing a single measure.\n",
    "\n",
    "#### Unpacking Classification Metrics\n",
    "\n",
    "Classification metrics are essential tools for quantifying model performance, but they should be understood as proxies for real-world outcomes rather than ends in themselves. Let's explore some key metrics and their implications:\n",
    "\n",
    "##### Accuracy: A Deceptive Simplicity\n",
    "\n",
    "**Accuracy** measures the proportion of correct predictions across all classes. While seemingly straightforward, it can be misleading, especially in imbalanced datasets. For instance:\n",
    "\n",
    "- In a dataset where 95% of samples belong to class A and 5% to class B, a model always predicting class A would achieve 95% accuracy without providing any valuable insights.\n",
    "- This metric fails to capture the nuances of misclassification costs, which can vary significantly in real-world scenarios.\n",
    "\n",
    "##### Precision and Recall: A Balancing Act\n",
    "\n",
    "**Precision** focuses on the accuracy of positive predictions, while **Recall** measures the model's ability to find all positive instances.\n",
    "\n",
    "- $\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$\n",
    "\n",
    "\n",
    "- $\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$\n",
    "\n",
    "These metrics are particularly useful when the costs of false positives and false negatives differ. For example:\n",
    "\n",
    "- In email spam detection, high precision is crucial to avoid marking important emails as spam (false positives).\n",
    "- In medical diagnosis, high recall is vital to ensure all potential cases of a severe condition are identified, even at the cost of some false alarms.\n",
    "\n",
    "##### F1-Score: Seeking Balance\n",
    "\n",
    "The **F1-score** provides a single metric that balances precision and recall: $F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$\n",
    "\n",
    "This metric is particularly useful when you need a balanced measure of a model's performance, especially with imbalanced datasets.\n",
    "\n",
    "\n",
    "#### Beyond Metrics: The Real-World Perspective\n",
    "\n",
    "While metrics provide valuable insights, it's crucial to consider their real-world implications:\n",
    "\n",
    "1. **Context Matters**: The choice of metric should align with the specific problem and its real-world consequences. For instance, in fraud detection, the cost of missing a fraudulent transaction (false negative) might far outweigh the inconvenience of a false alarm (false positive).\n",
    "\n",
    "2. **Holistic Evaluation**: Rather than fixating on a single metric, consider a combination of measures that provide a more complete view of model performance.\n",
    "\n",
    "3. **Ethical Considerations**: Metrics don't capture ethical consequences or societal impacts. For example, a facial recognition system might have high accuracy but could perpetuate biases if not carefully designed and evaluated.\n",
    "\n",
    "4. **Interpretability**: Some models might achieve high metric scores but lack interpretability, which can be crucial in fields like healthcare or finance where understanding the decision-making process is essential.\n",
    "\n",
    "#### Practical Approaches to Metric Selection\n",
    "\n",
    "1. **Engage with Domain Experts**: Collaborate with subject matter experts to understand the real-world effects of model predictions and choose metrics accordingly.\n",
    "\n",
    "2. **Consider Multiple Metrics**: Use a combination of metrics to get a more complete view of model performance.\n",
    "\n",
    "3. **Custom Metrics**: Develop problem-specific metrics that better align with the actual goals of your project.\n",
    "\n",
    "4. **Evaluate on Multiple Datasets**: Test your model on various datasets to ensure its performance is consistent and generalizable.\n",
    "\n",
    "5. **Monitor Real-World Performance**: Implement systems to track how well your model performs in actual deployments, not just on test sets.\n",
    "\n",
    "\n",
    "> Understanding Goodhart's Law in the context of classification metrics reminds us that our ultimate goal is to solve real-world problems, not just optimize numbers. While metrics are invaluable tools for model evaluation and improvement, they should guide our decisions rather than dictate them. By maintaining a integrated view that considers the broader impact and context of our models, we can develop more effective and responsible machine learning solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def calculate_evaluation_metrics(\n",
    "    y_true: pd.Series, y_pred: pd.Series\n",
    ") -> Tuple[float, float, float, str, float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate evaluation metrics for model predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true (pd.Series): The true labels.\n",
    "        y_pred (pd.Series): The predicted labels.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float, str, float, np.ndarray]: The calculated metrics including F1 score, balanced accuracy, accuracy, classification report, Matthews correlation coefficient, and confusion matrix.\n",
    "    \"\"\"\n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    # Calculate balanced accuracy\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # Generate classification report\n",
    "    classification_report_str = classification_report(y_true, y_pred)\n",
    "    # Calculate Matthews correlation coefficient\n",
    "    matthews_corr_coeff = matthews_corrcoef(y_true, y_pred)\n",
    "    # Generate confusion matrix\n",
    "    confusion_matrix_arr = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    return (\n",
    "        f1,\n",
    "        balanced_accuracy,\n",
    "        accuracy,\n",
    "        classification_report_str,\n",
    "        matthews_corr_coeff,\n",
    "        confusion_matrix_arr,\n",
    "    )\n",
    "\n",
    "\n",
    "def train_and_evaluate_models(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_valid: pd.DataFrame,\n",
    "    y_valid: pd.Series,\n",
    "    n_jobs: int = -1,\n",
    ") -> Tuple[pd.DataFrame, List[List]]:\n",
    "    \"\"\"\n",
    "    Train multiple models and evaluate their performance.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): The training data.\n",
    "        y_train (pd.Series): The training labels.\n",
    "        X_valid (pd.DataFrame): The validation data.\n",
    "        y_valid (pd.Series): The validation labels.\n",
    "        n_jobs (int, optional): The number of jobs to run in parallel. Defaults to -1.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, List[List]]: A dataframe of the evaluation results and a list of classification reports.\n",
    "    \"\"\"\n",
    "    # Define the models to be trained\n",
    "    models = [\n",
    "        (\n",
    "            \"Calibrated-LSVC\",\n",
    "            CalibratedClassifierCV(\n",
    "                LinearSVC(random_state=271828, class_weight=\"balanced\", dual=\"auto\")\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"LR\",\n",
    "            LogisticRegression(\n",
    "                random_state=271828, n_jobs=n_jobs, class_weight=\"balanced\"\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"RF\",\n",
    "            RandomForestClassifier(\n",
    "                random_state=271828, n_jobs=n_jobs, class_weight=\"balanced\"\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"LGBM\",\n",
    "            LGBMClassifier(\n",
    "                random_state=271828, n_jobs=n_jobs, class_weight=\"balanced\", verbose=-1\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"XGB\",\n",
    "            XGBClassifier(\n",
    "                random_state=271828, n_jobs=n_jobs, class_weight=\"balanced\", verbosity=0\n",
    "            ),\n",
    "        ),\n",
    "        (\"MLP\", MLPClassifier(random_state=271828)),\n",
    "        (\n",
    "            \"SGD\",\n",
    "            SGDClassifier(random_state=271828, n_jobs=n_jobs, class_weight=\"balanced\"),\n",
    "        ),\n",
    "        (\"NB\", MultinomialNB()),\n",
    "        (\"LSVC\", LinearSVC(random_state=271828, class_weight=\"balanced\", dual=\"auto\")),\n",
    "        (\"KNN\", KNeighborsClassifier(n_jobs=n_jobs)),\n",
    "        (\"DT\", DecisionTreeClassifier(random_state=271828, class_weight=\"balanced\")),\n",
    "        (\n",
    "            \"ExtraTrees\",\n",
    "            ExtraTreesClassifier(\n",
    "                random_state=271828, n_jobs=n_jobs, class_weight=\"balanced\"\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    evaluation_results = []\n",
    "    classification_reports = []\n",
    "\n",
    "    # Train each model and evaluate its performance\n",
    "    for model_name, model in models:\n",
    "        start_time = time.time()  # Record the start time\n",
    "\n",
    "        try:\n",
    "            # Train the model\n",
    "            model.fit(X_train, y_train)\n",
    "            # Make predictions on the validation set\n",
    "            predictions = model.predict(X_valid)\n",
    "        except Exception as e:\n",
    "            # Handle any exceptions that occur during training or prediction\n",
    "            print(f\"Error {model_name} - {e}\")\n",
    "            continue\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        (\n",
    "            f1,\n",
    "            balanced_accuracy,\n",
    "            accuracy,\n",
    "            classification_report_str,\n",
    "            matthews_corr_coeff,\n",
    "            confusion_matrix_arr,\n",
    "        ) = calculate_evaluation_metrics(y_valid, predictions)\n",
    "        # Store the classification report and confusion matrix\n",
    "        classification_reports.append(\n",
    "            [model_name, classification_report_str, confusion_matrix_arr]\n",
    "        )\n",
    "\n",
    "        elapsed_time = time.time() - start_time  # Calculate the elapsed time\n",
    "        # Append the evaluation results\n",
    "        evaluation_results.append(\n",
    "            [\n",
    "                model_name,\n",
    "                f1,\n",
    "                balanced_accuracy,\n",
    "                accuracy,\n",
    "                matthews_corr_coeff,\n",
    "                elapsed_time,\n",
    "                confusion_matrix_arr,\n",
    "                classification_report_str,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Print the evaluation results\n",
    "        print(\n",
    "            f\"Name: {model_name} - F1: {f1:.4f} - BACC: {balanced_accuracy:.4f} - ACC: {accuracy:.4f} - MCC: {matthews_corr_coeff:.4f} - Elapsed: {elapsed_time:.2f}s\"\n",
    "        )\n",
    "        print(classification_report_str)\n",
    "        print(confusion_matrix_arr)\n",
    "        print(\"*\" * 20, \"\\n\")\n",
    "\n",
    "    # Create a DataFrame to store the evaluation results\n",
    "    results_df = pd.DataFrame(\n",
    "        evaluation_results,\n",
    "        columns=[\n",
    "            \"Model\",\n",
    "            \"F1\",\n",
    "            \"BACC\",\n",
    "            \"ACC\",\n",
    "            \"MCC\",\n",
    "            \"Total Time\",\n",
    "            \"Confusion Matrix\",\n",
    "            \"Classification Report\",\n",
    "        ],\n",
    "    )\n",
    "    # Convert the confusion matrix to a string for better readability in the DataFrame\n",
    "    results_df[\"Confusion Matrix\"] = results_df[\"Confusion Matrix\"].apply(\n",
    "        lambda x: str(x)\n",
    "    )\n",
    "\n",
    "    return results_df, classification_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Calibrated-LSVC - F1: 0.9380 - BACC: 0.8338 - ACC: 0.9380 - MCC: 0.8456 - Elapsed: 8.28s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      1491\n",
      "           1       0.84      0.87      0.86       328\n",
      "           2       0.84      0.64      0.73       181\n",
      "\n",
      "    accuracy                           0.94      2000\n",
      "   macro avg       0.88      0.83      0.85      2000\n",
      "weighted avg       0.94      0.94      0.94      2000\n",
      "\n",
      "[[1474   14    3]\n",
      " [  23  286   19]\n",
      " [  24   41  116]]\n",
      "******************** \n",
      "\n",
      "Name: LR - F1: 0.9255 - BACC: 0.8922 - ACC: 0.9255 - MCC: 0.8309 - Elapsed: 6.71s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97      1491\n",
      "           1       0.82      0.85      0.84       328\n",
      "           2       0.66      0.88      0.75       181\n",
      "\n",
      "    accuracy                           0.93      2000\n",
      "   macro avg       0.83      0.89      0.85      2000\n",
      "weighted avg       0.94      0.93      0.93      2000\n",
      "\n",
      "[[1413   39   39]\n",
      " [   5  279   44]\n",
      " [   1   21  159]]\n",
      "******************** \n",
      "\n",
      "Name: RF - F1: 0.9300 - BACC: 0.7978 - ACC: 0.9300 - MCC: 0.8268 - Elapsed: 0.73s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      1491\n",
      "           1       0.78      0.89      0.83       328\n",
      "           2       0.85      0.51      0.64       181\n",
      "\n",
      "    accuracy                           0.93      2000\n",
      "   macro avg       0.87      0.80      0.82      2000\n",
      "weighted avg       0.93      0.93      0.93      2000\n",
      "\n",
      "[[1475   12    4]\n",
      " [  24  292   12]\n",
      " [  19   69   93]]\n",
      "******************** \n",
      "\n",
      "Name: LGBM - F1: 0.9565 - BACC: 0.9092 - ACC: 0.9565 - MCC: 0.8942 - Elapsed: 12.01s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      1491\n",
      "           1       0.89      0.89      0.89       328\n",
      "           2       0.83      0.86      0.84       181\n",
      "\n",
      "    accuracy                           0.96      2000\n",
      "   macro avg       0.90      0.91      0.91      2000\n",
      "weighted avg       0.96      0.96      0.96      2000\n",
      "\n",
      "[[1467   15    9]\n",
      " [  14  291   23]\n",
      " [   4   22  155]]\n",
      "******************** \n",
      "\n",
      "Name: XGB - F1: 0.9555 - BACC: 0.8957 - ACC: 0.9555 - MCC: 0.8910 - Elapsed: 13.72s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1491\n",
      "           1       0.87      0.89      0.88       328\n",
      "           2       0.87      0.81      0.84       181\n",
      "\n",
      "    accuracy                           0.96      2000\n",
      "   macro avg       0.91      0.90      0.90      2000\n",
      "weighted avg       0.96      0.96      0.96      2000\n",
      "\n",
      "[[1472   14    5]\n",
      " [  18  293   17]\n",
      " [   4   31  146]]\n",
      "******************** \n",
      "\n",
      "Name: MLP - F1: 0.9405 - BACC: 0.8682 - ACC: 0.9405 - MCC: 0.8540 - Elapsed: 49.48s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1491\n",
      "           1       0.85      0.86      0.85       328\n",
      "           2       0.81      0.77      0.79       181\n",
      "\n",
      "    accuracy                           0.94      2000\n",
      "   macro avg       0.88      0.87      0.87      2000\n",
      "weighted avg       0.94      0.94      0.94      2000\n",
      "\n",
      "[[1461   22    8]\n",
      " [  22  281   25]\n",
      " [  13   29  139]]\n",
      "******************** \n",
      "\n",
      "Name: SGD - F1: 0.9460 - BACC: 0.8974 - ACC: 0.9460 - MCC: 0.8705 - Elapsed: 0.83s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      1491\n",
      "           1       0.88      0.84      0.86       328\n",
      "           2       0.74      0.87      0.80       181\n",
      "\n",
      "    accuracy                           0.95      2000\n",
      "   macro avg       0.87      0.90      0.88      2000\n",
      "weighted avg       0.95      0.95      0.95      2000\n",
      "\n",
      "[[1458   15   18]\n",
      " [  14  276   38]\n",
      " [   2   21  158]]\n",
      "******************** \n",
      "\n",
      "Name: NB - F1: 0.7865 - BACC: 0.5186 - ACC: 0.7865 - MCC: 0.4326 - Elapsed: 0.05s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.90      1491\n",
      "           1       0.50      0.40      0.44       328\n",
      "           2       0.30      0.22      0.25       181\n",
      "\n",
      "    accuracy                           0.79      2000\n",
      "   macro avg       0.56      0.52      0.53      2000\n",
      "weighted avg       0.76      0.79      0.77      2000\n",
      "\n",
      "[[1403   31   57]\n",
      " [ 164  131   33]\n",
      " [  43   99   39]]\n",
      "******************** \n",
      "\n",
      "Name: LSVC - F1: 0.9480 - BACC: 0.8949 - ACC: 0.9480 - MCC: 0.8748 - Elapsed: 2.90s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      1491\n",
      "           1       0.86      0.87      0.86       328\n",
      "           2       0.78      0.84      0.81       181\n",
      "\n",
      "    accuracy                           0.95      2000\n",
      "   macro avg       0.88      0.89      0.88      2000\n",
      "weighted avg       0.95      0.95      0.95      2000\n",
      "\n",
      "[[1460   19   12]\n",
      " [  12  284   32]\n",
      " [   2   27  152]]\n",
      "******************** \n",
      "\n",
      "Name: KNN - F1: 0.8320 - BACC: 0.6235 - ACC: 0.8320 - MCC: 0.5455 - Elapsed: 0.27s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91      1491\n",
      "           1       0.67      0.48      0.56       328\n",
      "           2       0.74      0.43      0.54       181\n",
      "\n",
      "    accuracy                           0.83      2000\n",
      "   macro avg       0.76      0.62      0.67      2000\n",
      "weighted avg       0.82      0.83      0.82      2000\n",
      "\n",
      "[[1428   45   18]\n",
      " [ 160  158   10]\n",
      " [  70   33   78]]\n",
      "******************** \n",
      "\n",
      "Name: DT - F1: 0.9105 - BACC: 0.8150 - ACC: 0.9105 - MCC: 0.7792 - Elapsed: 19.70s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97      1491\n",
      "           1       0.78      0.73      0.75       328\n",
      "           2       0.72      0.75      0.73       181\n",
      "\n",
      "    accuracy                           0.91      2000\n",
      "   macro avg       0.82      0.82      0.82      2000\n",
      "weighted avg       0.91      0.91      0.91      2000\n",
      "\n",
      "[[1447   32   12]\n",
      " [  49  239   40]\n",
      " [  10   36  135]]\n",
      "******************** \n",
      "\n",
      "Name: ExtraTrees - F1: 0.9070 - BACC: 0.7229 - ACC: 0.9070 - MCC: 0.7681 - Elapsed: 0.86s\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97      1491\n",
      "           1       0.72      0.86      0.78       328\n",
      "           2       0.79      0.32      0.46       181\n",
      "\n",
      "    accuracy                           0.91      2000\n",
      "   macro avg       0.83      0.72      0.74      2000\n",
      "weighted avg       0.91      0.91      0.90      2000\n",
      "\n",
      "[[1474   14    3]\n",
      " [  34  282   12]\n",
      " [  28   95   58]]\n",
      "******************** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_results, creports = train_and_evaluate_models(\n",
    "    X_train, y_train, X_val, y_val, n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>F1</th>\n",
       "      <th>BACC</th>\n",
       "      <th>ACC</th>\n",
       "      <th>MCC</th>\n",
       "      <th>Total Time</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "      <th>Classification Report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LGBM</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.909151</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.894218</td>\n",
       "      <td>12.007161</td>\n",
       "      <td>[[1467   15    9]\\n [  14  291   23]\\n [   4  ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGB</td>\n",
       "      <td>0.9555</td>\n",
       "      <td>0.895726</td>\n",
       "      <td>0.9555</td>\n",
       "      <td>0.890957</td>\n",
       "      <td>13.716970</td>\n",
       "      <td>[[1472   14    5]\\n [  18  293   17]\\n [   4  ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LSVC</td>\n",
       "      <td>0.9480</td>\n",
       "      <td>0.894947</td>\n",
       "      <td>0.9480</td>\n",
       "      <td>0.874801</td>\n",
       "      <td>2.896014</td>\n",
       "      <td>[[1460   19   12]\\n [  12  284   32]\\n [   2  ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SGD</td>\n",
       "      <td>0.9460</td>\n",
       "      <td>0.897420</td>\n",
       "      <td>0.9460</td>\n",
       "      <td>0.870491</td>\n",
       "      <td>0.833355</td>\n",
       "      <td>[[1458   15   18]\\n [  14  276   38]\\n [   2  ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.9405</td>\n",
       "      <td>0.868181</td>\n",
       "      <td>0.9405</td>\n",
       "      <td>0.853977</td>\n",
       "      <td>49.480205</td>\n",
       "      <td>[[1461   22    8]\\n [  22  281   25]\\n [  13  ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Calibrated-LSVC</td>\n",
       "      <td>0.9380</td>\n",
       "      <td>0.833811</td>\n",
       "      <td>0.9380</td>\n",
       "      <td>0.845626</td>\n",
       "      <td>8.280817</td>\n",
       "      <td>[[1474   14    3]\\n [  23  286   19]\\n [  24  ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR</td>\n",
       "      <td>0.9255</td>\n",
       "      <td>0.892250</td>\n",
       "      <td>0.9255</td>\n",
       "      <td>0.830926</td>\n",
       "      <td>6.708490</td>\n",
       "      <td>[[1413   39   39]\\n [   5  279   44]\\n [   1  ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.797775</td>\n",
       "      <td>0.9300</td>\n",
       "      <td>0.826808</td>\n",
       "      <td>0.732178</td>\n",
       "      <td>[[1475   12    4]\\n [  24  292   12]\\n [  19  ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>DT</td>\n",
       "      <td>0.9105</td>\n",
       "      <td>0.815001</td>\n",
       "      <td>0.9105</td>\n",
       "      <td>0.779225</td>\n",
       "      <td>19.697039</td>\n",
       "      <td>[[1447   32   12]\\n [  49  239   40]\\n [  10  ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>0.9070</td>\n",
       "      <td>0.722932</td>\n",
       "      <td>0.9070</td>\n",
       "      <td>0.768066</td>\n",
       "      <td>0.861079</td>\n",
       "      <td>[[1474   14    3]\\n [  34  282   12]\\n [  28  ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.8320</td>\n",
       "      <td>0.623464</td>\n",
       "      <td>0.8320</td>\n",
       "      <td>0.545465</td>\n",
       "      <td>0.269518</td>\n",
       "      <td>[[1428   45   18]\\n [ 160  158   10]\\n [  70  ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NB</td>\n",
       "      <td>0.7865</td>\n",
       "      <td>0.518613</td>\n",
       "      <td>0.7865</td>\n",
       "      <td>0.432564</td>\n",
       "      <td>0.046721</td>\n",
       "      <td>[[1403   31   57]\\n [ 164  131   33]\\n [  43  ...</td>\n",
       "      <td>precision    recall  f1-score   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Model      F1      BACC     ACC       MCC  Total Time  \\\n",
       "3              LGBM  0.9565  0.909151  0.9565  0.894218   12.007161   \n",
       "4               XGB  0.9555  0.895726  0.9555  0.890957   13.716970   \n",
       "8              LSVC  0.9480  0.894947  0.9480  0.874801    2.896014   \n",
       "6               SGD  0.9460  0.897420  0.9460  0.870491    0.833355   \n",
       "5               MLP  0.9405  0.868181  0.9405  0.853977   49.480205   \n",
       "0   Calibrated-LSVC  0.9380  0.833811  0.9380  0.845626    8.280817   \n",
       "1                LR  0.9255  0.892250  0.9255  0.830926    6.708490   \n",
       "2                RF  0.9300  0.797775  0.9300  0.826808    0.732178   \n",
       "10               DT  0.9105  0.815001  0.9105  0.779225   19.697039   \n",
       "11       ExtraTrees  0.9070  0.722932  0.9070  0.768066    0.861079   \n",
       "9               KNN  0.8320  0.623464  0.8320  0.545465    0.269518   \n",
       "7                NB  0.7865  0.518613  0.7865  0.432564    0.046721   \n",
       "\n",
       "                                     Confusion Matrix  \\\n",
       "3   [[1467   15    9]\\n [  14  291   23]\\n [   4  ...   \n",
       "4   [[1472   14    5]\\n [  18  293   17]\\n [   4  ...   \n",
       "8   [[1460   19   12]\\n [  12  284   32]\\n [   2  ...   \n",
       "6   [[1458   15   18]\\n [  14  276   38]\\n [   2  ...   \n",
       "5   [[1461   22    8]\\n [  22  281   25]\\n [  13  ...   \n",
       "0   [[1474   14    3]\\n [  23  286   19]\\n [  24  ...   \n",
       "1   [[1413   39   39]\\n [   5  279   44]\\n [   1  ...   \n",
       "2   [[1475   12    4]\\n [  24  292   12]\\n [  19  ...   \n",
       "10  [[1447   32   12]\\n [  49  239   40]\\n [  10  ...   \n",
       "11  [[1474   14    3]\\n [  34  282   12]\\n [  28  ...   \n",
       "9   [[1428   45   18]\\n [ 160  158   10]\\n [  70  ...   \n",
       "7   [[1403   31   57]\\n [ 164  131   33]\\n [  43  ...   \n",
       "\n",
       "                                Classification Report  \n",
       "3                 precision    recall  f1-score   ...  \n",
       "4                 precision    recall  f1-score   ...  \n",
       "8                 precision    recall  f1-score   ...  \n",
       "6                 precision    recall  f1-score   ...  \n",
       "5                 precision    recall  f1-score   ...  \n",
       "0                 precision    recall  f1-score   ...  \n",
       "1                 precision    recall  f1-score   ...  \n",
       "2                 precision    recall  f1-score   ...  \n",
       "10                precision    recall  f1-score   ...  \n",
       "11                precision    recall  f1-score   ...  \n",
       "9                 precision    recall  f1-score   ...  \n",
       "7                 precision    recall  f1-score   ...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by=\"MCC\", ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Classifier: Stacking for Enhanced Performance\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/stacking.png\" alt=\"\" style=\"width: 40%; height: 40%\"/>\n",
    "</p>\n",
    "\n",
    "#### Understanding Ensemble Learning\n",
    "\n",
    "Ensemble learning is a powerful technique in machine learning that combines multiple classifiers to achieve superior performance compared to individual models. This approach is particularly effective for:\n",
    "\n",
    "- Tackling complex classification tasks\n",
    "- Mitigating the limitations of single classifiers\n",
    "- Improving overall prediction accuracy and robustness\n",
    "\n",
    "#### The StackingClassifier: A Sophisticated Ensemble Technique\n",
    "\n",
    "The `StackingClassifier` is an advanced ensemble method that leverages the strengths of multiple base classifiers and a meta-classifier:\n",
    "\n",
    "1. **Base Classifiers**: Multiple models trained on the original dataset\n",
    "2. **Meta-Classifier**: A higher-level model that learns to combine predictions from base classifiers\n",
    "\n",
    "> **Key Insight**: The meta-classifier doesn't just average or vote on base classifier outputs. It learns the optimal way to combine these predictions, potentially capturing complex interactions between base models.\n",
    "\n",
    "#### How Stacking Works\n",
    "\n",
    "1. Train multiple base classifiers on the original dataset\n",
    "2. Use these base classifiers to make predictions on a hold-out set or through cross-validation\n",
    "3. Use the predictions from step 2 as features to train the meta-classifier\n",
    "4. For new data, base classifiers make predictions, which are then fed into the meta-classifier for the final prediction\n",
    "\n",
    "#### Choosing Classifiers for Stacking\n",
    "\n",
    "Selecting diverse classifiers is crucial for maximizing the benefits of stacking:\n",
    "\n",
    "- **Diversity in Algorithm Types**: Combine linear (e.g., Logistic Regression) and non-linear (e.g., Decision Trees) classifiers\n",
    "- **Diversity in Strengths**: Include classifiers that excel in different aspects (e.g., handling missing data, feature importance)\n",
    "- **Complementary Weaknesses**: Choose models whose weaknesses are offset by others' strengths\n",
    "\n",
    "> **Note**: While using `scikit-learn`, the meta-classifier should support the `predict_proba()` method for probabilistic predictions. Common choices include Logistic Regression or Random Forest.\n",
    "\n",
    "#### Practical Considerations\n",
    "\n",
    "- **Computational Cost**: Stacking can be computationally expensive, especially with many base classifiers or large datasets\n",
    "- **Overfitting Risk**: Careful cross-validation is necessary to prevent overfitting, particularly in the meta-classifier stage\n",
    "- **Interpretability**: While stacking often improves performance, it can reduce model interpretability\n",
    "\n",
    "#### Implementation Note\n",
    "\n",
    "For classifiers lacking `predict_proba()` (e.g., LinearSVC), the `CalibratedClassifierCV` wrapper can be used:\n",
    "\n",
    "```python\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "calibrated_svc = CalibratedClassifierCV(LinearSVC())\n",
    "```\n",
    "\n",
    "This calibration step adds probability estimation capabilities to otherwise non-probabilistic classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
      "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
      "\n",
      "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
      "This will raise in a future version.\n",
      "\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.9560 - BACC: 0.9227 - ACC: 0.9560 - MCC: 0.8947\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99      1491\n",
      "           1       0.89      0.88      0.88       328\n",
      "           2       0.80      0.91      0.85       181\n",
      "\n",
      "    accuracy                           0.96      2000\n",
      "   macro avg       0.89      0.92      0.91      2000\n",
      "weighted avg       0.96      0.96      0.96      2000\n",
      "\n",
      "[[1459   20   12]\n",
      " [  10  288   30]\n",
      " [   1   15  165]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize individual models with specific parameters\n",
    "random_forest_model = RandomForestClassifier(\n",
    "    random_state=271828, n_jobs=-1, class_weight=\"balanced\"\n",
    ")\n",
    "lgbm_model = LGBMClassifier(\n",
    "    random_state=271828, n_jobs=-1, class_weight=\"balanced\", verbose=-1\n",
    ")\n",
    "lsvc_model = LinearSVC(random_state=271828, class_weight=\"balanced\", dual=\"auto\")\n",
    "\n",
    "# List of base estimators for stacking\n",
    "base_estimators = [\n",
    "    (\"random_forest\", random_forest_model),\n",
    "    (\"calibrated_lsvc\", lsvc_model),\n",
    "    (\"lgbm\", lgbm_model),\n",
    "]\n",
    "\n",
    "# Initialize the stacking classifier with base estimators and a final estimator\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=base_estimators,\n",
    "    final_estimator=LogisticRegression(\n",
    "        random_state=271828, n_jobs=-1, class_weight=\"balanced\"\n",
    "    ),\n",
    "    n_jobs=2,\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "# Fit the stacking model on the training data\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation data\n",
    "predictions = stacking_model.predict(X_val)\n",
    "\n",
    "# Calculate evaluation metrics for the predictions\n",
    "(\n",
    "    f1_score_val,\n",
    "    balanced_accuracy_val,\n",
    "    accuracy_val,\n",
    "    classification_report_val,\n",
    "    matthews_corr_coeff_val,\n",
    "    confusion_matrix_val,\n",
    ") = calculate_evaluation_metrics(y_val, predictions)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\n",
    "    f\"F1: {f1_score_val:.4f} - BACC: {balanced_accuracy_val:.4f} - ACC: {accuracy_val:.4f} - MCC: {matthews_corr_coeff_val:.4f}\"\n",
    ")\n",
    "print(classification_report_val)\n",
    "print(confusion_matrix_val)\n",
    "\n",
    "# Took 2 minutes to run in a 48 core CPU\n",
    "# Best MCC was 0.894218\n",
    "# Best MCC is now 0.8947"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`As we've seen above, we can improve our results at the cost of increased training time by combining multiple classifiers into a single model. `\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/hyperparameter_optimization.png\" alt=\"\" style=\"width: 40%; height: 40%\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "Hyperparameter optimization is the process of selecting the best set of hyperparameters for a machine learning model. These hyperparameters control the model's behavior and directly impact its performance. Finding the optimal values can often lead to better results in terms of accuracy, precision, or other evaluation metrics.\n",
    "\n",
    "Hyperparameter optimization is represented in equation form as $ \\mathcal{x}^* = \\arg \\min_{\\mathcal{x}} f(\\mathcal{x}) $, where:\n",
    "- $ \\mathcal{x}^* $ is the optimal set of hyperparameters\n",
    "- $ \\mathcal{x} $ represents the hyperparameter space\n",
    "- $ f(\\mathcal{x}) $ is the objective function to be minimized or maximized\n",
    "\n",
    "The objective function can be defined based on various criteria, such as maximizing accuracy, minimizing loss, or optimizing a specific metric.\n",
    "\n",
    "There are several methods of hyperparameter optimization, including:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/hyperparameter_optimization2.jpg\" alt=\"\" style=\"width: 40%; height: 40%\"/>\n",
    "</p>\n",
    "\n",
    "#### 1. Grid Search\n",
    "Grid Search is an exhaustive search technique that evaluates all possible combinations of hyperparameter values within a specified range. It explores the entire parameter space but can be computationally expensive, especially for large datasets or complex models.\n",
    "\n",
    "**Key Points:**\n",
    "- **Exhaustive Search:** Evaluates every possible combination within the specified grid.\n",
    "- **Computational Cost:** Can be very high, particularly with many hyperparameters or large datasets.\n",
    "- **Use Cases:** Best for smaller hyperparameter spaces or when computational resources are not a constraint.\n",
    "\n",
    "#### 2. Random Search\n",
    "Random Search involves randomly sampling the hyperparameter space and evaluating the model's performance at each sampled point. This method is less computationally intensive than Grid Search, as it does not explore every possible combination. While there is no guarantee of finding the optimal values, in many cases, Random Search can provide reasonably good results within fewer iterations.\n",
    "\n",
    "**Key Points:**\n",
    "- **Random Sampling:** Selects random combinations of hyperparameters.\n",
    "- **Efficiency:** Often finds good solutions faster than Grid Search.\n",
    "- **Use Cases:** Suitable when the hyperparameter space is large and computational resources are limited.\n",
    "\n",
    "#### 3. Bayesian Optimization\n",
    "Bayesian Optimization is a sequential model-based optimization technique that incorporates prior knowledge about the hyperparameter space. It uses a probabilistic model (e.g., Gaussian Processes) to predict the performance of various hyperparameter settings and selects the next candidate based on a predefined acquisition function. This approach is more efficient than both Grid Search and Random Search, as it intelligently chooses points in the hyperparameter space by considering information from previous evaluations.\n",
    "\n",
    "**Key Points:**\n",
    "- **Probabilistic Model:** Uses models like Gaussian Processes to predict performance.\n",
    "- **Sequential Approach:** Selects new hyperparameters based on past evaluations.\n",
    "- **Efficiency:** Often more efficient than Grid and Random Search, especially for complex models.\n",
    "- **Use Cases:** Ideal when the search space is large and evaluations are expensive.\n",
    "\n",
    " \n",
    "> **TLDR:** Hyperparameter optimization plays a crucial role in improving a model's performance. The choice of optimization method depends on factors such as available computational resources, search space complexity, and the desired balance between exploration and exploitation. Each method has its advantages and limitations, and it's essential to select the one that best suits the specific problem at hand.\n",
    "\n",
    "To save time, we'll perform each kind of optimization on the hyperparameters of our `SGDClassifier` (our fastest top classifiers). You may want to select the best classifier in your problem."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1'}\n",
      "Best score:  0.8545240719127332\n",
      "Validation score:  0.947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, matthews_corrcoef\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from typing import Dict, Any\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def perform_grid_search(\n",
    "    model: Any,\n",
    "    param_grid: Dict[str, Any],\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.Series,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Perform grid search to find the best parameters for the model and evaluate it on the validation set.\n",
    "\n",
    "    Args:\n",
    "        model (Any): The model to be trained.\n",
    "        param_grid (Dict[str, Any]): The parameter grid for the search.\n",
    "        X_train (pd.DataFrame): The training data.\n",
    "        y_train (pd.Series): The training labels.\n",
    "        X_val (pd.DataFrame): The validation data.\n",
    "        y_val (pd.Series): The validation labels.\n",
    "    \"\"\"\n",
    "    # Define the scorer for the grid search using Matthews correlation coefficient\n",
    "    scorer_mcc = make_scorer(matthews_corrcoef)\n",
    "\n",
    "    # Create the GridSearchCV object with the model and parameter grid\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3, scoring=scorer_mcc, n_jobs=-1)\n",
    "\n",
    "    # Perform the grid search by fitting the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best parameters and corresponding MCC score found by the grid search\n",
    "    print(\"Best parameters: \", grid_search.best_params_)\n",
    "    print(\"Best score: \", grid_search.best_score_)\n",
    "\n",
    "    # Evaluate the model with chosen parameters on the validation set\n",
    "    best_model = grid_search.best_estimator_\n",
    "    validation_score = best_model.score(X_val, y_val)\n",
    "    print(\"Validation score: \", validation_score)\n",
    "\n",
    "\n",
    "# Define the SGDClassifier model with specific parameters\n",
    "sgd_model = SGDClassifier(random_state=271828, n_jobs=-1, class_weight=\"balanced\")\n",
    "\n",
    "# Define parameter grid for the search\n",
    "search_parameters = {\n",
    "    \"loss\": [\n",
    "        \"hinge\",\n",
    "        \"log_loss\",\n",
    "        \"squared_hinge\",\n",
    "        \"modified_huber\",\n",
    "    ],  # Different loss functions\n",
    "    \"alpha\": [1e-4, 1e-3, 1e-2, 1e-1, 1e0],  # Regularization strength\n",
    "    \"max_iter\": [1000, 2000, 3000, 4000, 5000],  # Number of iterations\n",
    "    \"penalty\": [\"l2\", \"l1\", \"elasticnet\"],  # Regularization types\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "perform_grid_search(sgd_model, search_parameters, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Took 17 minutes to run in a 48 core CPU\n",
    "# It performs 4 * 5 * 5 * 3 combinations of parameters, which is 300 combinations in total.\n",
    "# Since it uses Cross Validation with 3 folds, it will train 900 models in total!!!!!\n",
    "\n",
    "# Best parameters:  {'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 1000, 'penalty': 'l1'}\n",
    "# Validation MCC:  0.947\n",
    "# The best valid MCC for the SGDClassifier was 0.870491"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing Random Search\n",
    "\n",
    "##### Understanding the 60 Iterations Rule\n",
    "\n",
    "In hyperparameter optimization, a widely-used rule of thumb states that **60 iterations of random search can find the best 5% set of parameters 95% of the time**, regardless of the grid size.\n",
    "\n",
    "This rule can be framed as a probability problem:\n",
    "\n",
    "> What is the minimum number of trials (iterations) needed to have a 95% chance of finding at least one hyperparameter set in the top 5%?\n",
    "\n",
    "This scenario aligns with the **geometric distribution**, which models the number of trials needed to achieve the first success in repeated independent trials. However, our specific question focuses on the number of trials required to be 95% confident of at least one success, rather than the expected number of trials for the first success.\n",
    "\n",
    "##### Mathematical Analysis\n",
    "\n",
    "Let's define our variables:\n",
    "- $\\mathcal{p}$ = probability of success (finding a top 5% hyperparameter set) in each trial = 0.05\n",
    "- $\\mathcal{n}$ = number of trials (iterations)\n",
    "\n",
    "We want to calculate the probability of *not* having a success after n trials, which should be less than or equal to 0.05 (5%) to ensure 95% confidence of at least one success.\n",
    "\n",
    "1. Probability of no success in n trials: $(1 - p)^n \\le 0.05$\n",
    "2. Substituting p = 0.05: $(1 - 0.05)^n \\le 0.05$\n",
    "3. Solving for n using logarithms:\n",
    "    - $n * \\log(0.95) \\le \\log(0.05)$\n",
    "    - $n \\ge \\log(0.05) / \\log(0.95)$\n",
    "\n",
    "Calculating this, we find that $\\mathcal{n}$ ≈ 59.\n",
    "\n",
    "##### Interpretation and Effects\n",
    "\n",
    "- **59 iterations** are needed for 95% confidence of finding at least one top 5% hyperparameter set.\n",
    "- This result is rounded up to 60 in the commonly cited rule of thumb.\n",
    "- The beauty of this rule lies in its **independence from the size of the hyperparameter space**.\n",
    "\n",
    "##### Practical Considerations\n",
    "\n",
    "1. **Efficiency**: Random search can be more efficient than grid search, especially in high-dimensional spaces where many parameters may not significantly impact the model's performance.\n",
    "\n",
    "2. **Adaptability**: This method works well when you're unsure which hyperparameters are most important, as it samples from the entire space.\n",
    "\n",
    "3. **Scalability**: The number of iterations needed grows logarithmically with the desired confidence level and inversely with the target percentile.\n",
    "\n",
    "> For example, to be 99% confident of finding a top 1% set, you would need approximately 459 iterations.\n",
    "\n",
    "4. **Limitations**: While efficient, random search doesn't exploit information from previous trials to inform future searches, unlike more advanced methods like Bayesian optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'penalty': 'l1', 'max_iter': 3000, 'loss': 'hinge', 'alpha': 0.0001}\n",
      "Best score:  0.8545240719127332\n",
      "Validation MCC:  0.947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "def perform_random_search(\n",
    "    model: SGDClassifier,\n",
    "    param_dist: Dict[str, Any],\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.Series,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Perform random search to find the best parameters for the model and evaluate it on the validation set.\n",
    "\n",
    "    Args:\n",
    "        model (SGDClassifier): The model to be trained.\n",
    "        param_dist (Dict[str, Any]): The parameter distribution for the search.\n",
    "        X_train (pd.DataFrame): The training data.\n",
    "        y_train (pd.Series): The training labels.\n",
    "        X_val (pd.DataFrame): The validation data.\n",
    "        y_val (pd.Series): The validation labels.\n",
    "    \"\"\"\n",
    "    # Define the scorer for the random search\n",
    "    mcc_scorer = make_scorer(matthews_corrcoef)\n",
    "\n",
    "    # Create the RandomizedSearchCV object with the SGDClassifier and parameter distribution\n",
    "    random_search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_dist,\n",
    "        cv=3,\n",
    "        scoring=mcc_scorer,\n",
    "        n_jobs=-1,\n",
    "        n_iter=60,\n",
    "        random_state=271828,\n",
    "    )\n",
    "\n",
    "    # Perform the random search by fitting training data\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best parameters and corresponding MCC score found by the random search\n",
    "    print(\"Best parameters: \", random_search.best_params_)\n",
    "    print(\"Best score: \", random_search.best_score_)\n",
    "\n",
    "    # Evaluate the model with chosen parameters on the validation set\n",
    "    best_model = random_search.best_estimator_\n",
    "    validation_mcc = best_model.score(X_val, y_val)\n",
    "    print(\"Validation MCC: \", validation_mcc)\n",
    "\n",
    "\n",
    "# Define the SGDClassifier model\n",
    "sgd_model = SGDClassifier(random_state=271828, n_jobs=-1, class_weight=\"balanced\")\n",
    "\n",
    "# Define parameter distribution for the search\n",
    "search_parameters = {\n",
    "    \"loss\": [\"hinge\", \"log_loss\", \"squared_hinge\", \"modified_huber\"],\n",
    "    \"alpha\": [1e-4, 1e-3, 1e-2, 1e-1, 1e0],\n",
    "    \"max_iter\": [1000, 2000, 3000, 4000, 5000],\n",
    "    \"penalty\": [\"l2\", \"l1\", \"elasticnet\"],\n",
    "}\n",
    "\n",
    "# Perform random search\n",
    "perform_random_search(sgd_model, search_parameters, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# This will typically run faster than GridSearchCV due to the reduced number of parameter combinations.\n",
    "# Time to run: 4 minutes in a 48 core CPU\n",
    "\n",
    "# Best parameters:  {'penalty': 'l1', 'max_iter': 3000, 'loss': 'hinge', 'alpha': 0.0001}\n",
    "# Validation MCC:  0.947\n",
    "# The best valid MCC for the SGDClassifier was 0.870491"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Optimization for Hyperparameter Tuning\n",
    "\n",
    "Bayesian optimization is a sophisticated approach to hyperparameter tuning, particularly valuable when dealing with complex models and computationally expensive evaluations. This method intelligently navigates the hyperparameter space by balancing exploration of unknown regions with exploitation of promising areas.\n",
    "\n",
    "The fundamental principle of Bayesian optimization lies in its ability to learn from previous evaluations and make informed decisions about which hyperparameters to try next. This approach significantly reduces the number of evaluations needed to find optimal or near-optimal hyperparameters.\n",
    "\n",
    "> **Key Insight**: Bayesian model-based methods can discover superior hyperparameters more efficiently by reasoning about the most promising configurations based on past trials.\n",
    "\n",
    "##### Visual Understanding\n",
    "\n",
    "To grasp the essence of Bayesian optimization, consider the following visual representations:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/bayesian1.webp\" alt=\"\" style=\"width: 40%; height: 40%\"/>\n",
    "</p>\n",
    "\n",
    "This image illustrates the initial state of the surrogate model (black line with gray uncertainty) after only two evaluations. At this stage, the surrogate model poorly approximates the true objective function (red line).\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/bayesian2.webp\" alt=\"\" style=\"width: 40%; height: 40%\"/>\n",
    "</p>\n",
    "\n",
    "After eight evaluations, the surrogate model closely matches the true function. This improved approximation allows the algorithm to select hyperparameters that are likely to yield excellent results on the actual evaluation function.\n",
    "\n",
    "For a nice conceptual understanding of Bayesian optimization, [check this link](https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f)\n",
    "\n",
    "##### The Bayesian Approach\n",
    "\n",
    "Bayesian methods mirror human learning processes:\n",
    "1. Form an initial view (prior)\n",
    "2. Update the model based on new experiences (posterior)\n",
    "\n",
    "In the context of hyperparameter optimization, this framework is applied to discover optimal model settings iteratively.\n",
    "\n",
    "##### Key Components of Bayesian Optimization\n",
    "\n",
    "1. **Objective Function**\n",
    "    - Evaluates model performance for a given set of hyperparameters\n",
    "    - Typically uses metrics like accuracy, loss, or cross-validation scores\n",
    "    - Serves as the ground truth for comparing different configurations\n",
    "\n",
    "2. **Surrogate Model**\n",
    "    - Probabilistic approximation of the objective function\n",
    "    - Often uses Gaussian Processes (GPs) for their ability to provide both predictions and uncertainty estimates\n",
    "    - Predicts performance of untested hyperparameter sets based on observed results\n",
    "    - Significantly reduces computational costs by minimizing actual evaluations of the (costly) objective function\n",
    "\n",
    "3. **Acquisition Function**\n",
    "    - Guides the selection of the next hyperparameter set to evaluate\n",
    "    - Balances exploration (investigating new areas) and exploitation (focusing on known good regions)\n",
    "    - Common types include:\n",
    "    - Expected Improvement (EI)\n",
    "    - Probability of Improvement (PI)\n",
    "    - Upper Confidence Bound (UCB)\n",
    "\n",
    "##### The Optimization Process\n",
    "\n",
    "1. **Initialization**: Randomly select and evaluate a few initial hyperparameter sets.\n",
    "\n",
    "2. **Surrogate Model Creation**: Fit a Gaussian Process to the initial data points.\n",
    "\n",
    "3. **Acquisition Function Calculation**: Compute the acquisition function values across the hyperparameter space.\n",
    "\n",
    "4. **Next Point Selection**: Choose the hyperparameter set with the highest acquisition function value.\n",
    "\n",
    "5. **Evaluation**: Assess the chosen hyperparameters using the objective function.\n",
    "\n",
    "6. **Model Update**: Refit the Gaussian Process with the new data point.\n",
    "\n",
    "7. **Iteration**: Repeat steps 3-6 until a stopping criterion is met (e.g., performance threshold or iteration limit).\n",
    "\n",
    "##### Advantages and Considerations\n",
    "\n",
    "- **Efficiency**: Particularly beneficial for expensive-to-evaluate models, minimizing the number of evaluations needed.\n",
    "- **Adaptability**: Learns from previous evaluations to make informed decisions about future trials.\n",
    "- **Uncertainty Handling**: Incorporates uncertainty in its decision-making process, leading to a more robust exploration of the hyperparameter space.\n",
    "\n",
    "> **Note**: While highly effective, Bayesian optimization does not guarantee finding the global optimum, especially in complex or noisy objective functions.\n",
    "\n",
    "##### Illustrative Analogies\n",
    "\n",
    "- **Surrogate Model as a Treasure Map**: Initially, you have a rough sketch of an island. As you explore and mark landmarks (data points), your map becomes more detailed and accurate, guiding your search more effectively.\n",
    "\n",
    "- **Acquisition Function as Oil Drilling**: Deciding whether to drill deeper in a promising spot (exploitation) or start a new drill elsewhere (exploration) based on current knowledge and potential rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:744: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n",
      "/home/jacob/miniforge3/envs/datacentric_ai/lib/python3.12/site-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [0.001, 'modified_huber', 3000, 'l2'] before, using random point [1.0, 'log_loss', 2000, 'l1']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  OrderedDict({'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 5000, 'penalty': 'l1'})\n",
      "Best score:  0.8545240719127332\n",
      "Validation MCC:  0.947\n"
     ]
    }
   ],
   "source": [
    "from skopt import BayesSearchCV\n",
    "\n",
    "\n",
    "def perform_bayesian_optimization(\n",
    "    model: SGDClassifier,\n",
    "    param_space: Dict[str, Any],\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.Series,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Perform Bayesian optimization to find the best parameters for the model and evaluate it on the validation set.\n",
    "\n",
    "    Args:\n",
    "        model (SGDClassifier): The model to be trained.\n",
    "        param_space (Dict[str, Any]): The parameter space for the search.\n",
    "        X_train (pd.DataFrame): The training data.\n",
    "        y_train (pd.Series): The training labels.\n",
    "        X_val (pd.DataFrame): The validation data.\n",
    "        y_val (pd.Series): The validation labels.\n",
    "    \"\"\"\n",
    "    # Define the scorer for the Bayesian optimization\n",
    "    mcc_scorer = make_scorer(matthews_corrcoef)\n",
    "\n",
    "    # Create the BayesSearchCV object with the SGDClassifier and parameter space\n",
    "    bayesian_search = BayesSearchCV(\n",
    "        model,\n",
    "        param_space,\n",
    "        cv=3,\n",
    "        scoring=mcc_scorer,\n",
    "        n_jobs=-1,\n",
    "        n_iter=30,\n",
    "        random_state=271828,\n",
    "        n_points=10,\n",
    "    )\n",
    "\n",
    "    # Perform the Bayesian optimization by fitting training data\n",
    "    bayesian_search.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best parameters and corresponding MCC score found by the Bayesian search\n",
    "    print(\"Best parameters: \", bayesian_search.best_params_)\n",
    "    print(\"Best score: \", bayesian_search.best_score_)\n",
    "\n",
    "    # Evaluate the model with chosen parameters on the validation set\n",
    "    best_model = bayesian_search.best_estimator_\n",
    "    validation_mcc = best_model.score(X_val, y_val)\n",
    "    print(\"Validation MCC: \", validation_mcc)\n",
    "\n",
    "\n",
    "# Define the SGDClassifier model\n",
    "sgd_model = SGDClassifier(random_state=271828, n_jobs=-1, class_weight=\"balanced\")\n",
    "\n",
    "# Define parameter space for the search\n",
    "search_parameters = {\n",
    "    \"loss\": [\"hinge\", \"log_loss\", \"squared_hinge\", \"modified_huber\"],\n",
    "    \"alpha\": [1e-4, 1e-3, 1e-2, 1e-1, 1e0],\n",
    "    \"max_iter\": [1000, 2000, 3000, 4000, 5000],\n",
    "    \"penalty\": [\"l2\", \"l1\", \"elasticnet\"],\n",
    "}\n",
    "\n",
    "# Perform Bayesian optimization\n",
    "perform_bayesian_optimization(\n",
    "    sgd_model, search_parameters, X_train, y_train, X_val, y_val\n",
    ")\n",
    "\n",
    "# This cell took 1 minute do run\n",
    "# Best parameters:  OrderedDict({'alpha': 0.0001, 'loss': 'hinge', 'max_iter': 5000, 'penalty': 'l1'})\n",
    "# Validation MCC:  0.947\n",
    "# The best valid MCC for the SGDClassifier was 0.870491"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Predicting the Time it Takes to Issue an \"Acórdão\"\n",
    "Your turn to build a model! In this problem, you'll use the data from the previous problem to predict the time it takes for an appeal to be judged. This is a regression problem, and you'll use the same features as in the previous problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our data.\n",
    "import pandas as pd\n",
    "\n",
    "df_texts = pd.read_parquet(\"data/brcad5/texts_sample.parquet.gz\")\n",
    "df_meta = pd.read_parquet(\"data/brcad5/metadata_sample.parquet.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_number</th>\n",
       "      <th>filing_date</th>\n",
       "      <th>defendant_normalized</th>\n",
       "      <th>date_first_instance_ruling</th>\n",
       "      <th>date_appeal_panel_ruling</th>\n",
       "      <th>case_topic_code</th>\n",
       "      <th>case_topic_1st_level</th>\n",
       "      <th>case_topic_2nd_level</th>\n",
       "      <th>case_topic_3rd_level</th>\n",
       "      <th>court_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0510491-75.2017.4.05.8103</td>\n",
       "      <td>2017-10-11 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2018-01-16 11:16:19</td>\n",
       "      <td>2018-03-26 17:07:16</td>\n",
       "      <td>6095</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Aposentadoria por Invalidez</td>\n",
       "      <td>19-CE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0509917-52.2017.4.05.8103</td>\n",
       "      <td>2017-09-26 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2018-01-15 18:07:55</td>\n",
       "      <td>2018-03-26 17:07:16</td>\n",
       "      <td>6101</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Auxílio-Doença Previdenciário</td>\n",
       "      <td>31-CE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0501051-55.2017.4.05.8103</td>\n",
       "      <td>2017-02-03 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2017-11-21 11:44:40</td>\n",
       "      <td>2018-03-26 17:07:16</td>\n",
       "      <td>6103</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Salário-Maternidade (Art. 71/73)</td>\n",
       "      <td>31-CE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0511681-76.2017.4.05.8102</td>\n",
       "      <td>2017-09-27 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2017-12-29 02:18:24</td>\n",
       "      <td>2018-03-26 17:07:16</td>\n",
       "      <td>6104</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Pensão por Morte (Art. 74/9)</td>\n",
       "      <td>30-CE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0500777-79.2017.4.05.8107</td>\n",
       "      <td>2017-02-23 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2017-11-27 17:58:46</td>\n",
       "      <td>2018-03-26 17:07:16</td>\n",
       "      <td>6103</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Salário-Maternidade (Art. 71/73)</td>\n",
       "      <td>25-CE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152600</th>\n",
       "      <td>0511177-90.2019.4.05.8202</td>\n",
       "      <td>2019-11-08 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2019-12-16 13:09:08</td>\n",
       "      <td>2020-03-31 11:47:59</td>\n",
       "      <td>6176</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Pedidos Genéricos Relativos aos Benefícios em ...</td>\n",
       "      <td>Parcelas de benefício não pagas</td>\n",
       "      <td>15-PB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152605</th>\n",
       "      <td>0508370-06.2019.4.05.8200</td>\n",
       "      <td>2019-06-13 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2019-10-03 18:01:23</td>\n",
       "      <td>2020-03-31 15:22:15</td>\n",
       "      <td>6096</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Aposentadoria por Idade (Art. 48/51)</td>\n",
       "      <td>7-PB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152610</th>\n",
       "      <td>0508717-73.2018.4.05.8200</td>\n",
       "      <td>2018-06-20 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2019-08-29 16:53:58</td>\n",
       "      <td>2020-03-31 15:22:15</td>\n",
       "      <td>6114</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Benefício Assistencial (Art. 203,V CF/88)</td>\n",
       "      <td>13-PB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152628</th>\n",
       "      <td>0507511-81.2019.4.05.8202</td>\n",
       "      <td>2019-08-22 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2019-11-05 17:10:31</td>\n",
       "      <td>2020-04-01 11:24:06</td>\n",
       "      <td>6176</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Pedidos Genéricos Relativos aos Benefícios em ...</td>\n",
       "      <td>Parcelas de benefício não pagas</td>\n",
       "      <td>15-PB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152635</th>\n",
       "      <td>0507364-55.2019.4.05.8202</td>\n",
       "      <td>2019-08-21 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2019-10-30 16:18:03</td>\n",
       "      <td>2020-04-01 11:24:06</td>\n",
       "      <td>6176</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Pedidos Genéricos Relativos aos Benefícios em ...</td>\n",
       "      <td>Parcelas de benefício não pagas</td>\n",
       "      <td>15-PB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      case_number          filing_date defendant_normalized  \\\n",
       "11      0510491-75.2017.4.05.8103  2017-10-11 00:00:00                 INSS   \n",
       "16      0509917-52.2017.4.05.8103  2017-09-26 00:00:00                 INSS   \n",
       "19      0501051-55.2017.4.05.8103  2017-02-03 00:00:00                 INSS   \n",
       "23      0511681-76.2017.4.05.8102  2017-09-27 00:00:00                 INSS   \n",
       "33      0500777-79.2017.4.05.8107  2017-02-23 00:00:00                 INSS   \n",
       "...                           ...                  ...                  ...   \n",
       "152600  0511177-90.2019.4.05.8202  2019-11-08 00:00:00                 INSS   \n",
       "152605  0508370-06.2019.4.05.8200  2019-06-13 00:00:00                 INSS   \n",
       "152610  0508717-73.2018.4.05.8200  2018-06-20 00:00:00                 INSS   \n",
       "152628  0507511-81.2019.4.05.8202  2019-08-22 00:00:00                 INSS   \n",
       "152635  0507364-55.2019.4.05.8202  2019-08-21 00:00:00                 INSS   \n",
       "\n",
       "       date_first_instance_ruling date_appeal_panel_ruling  case_topic_code  \\\n",
       "11            2018-01-16 11:16:19      2018-03-26 17:07:16             6095   \n",
       "16            2018-01-15 18:07:55      2018-03-26 17:07:16             6101   \n",
       "19            2017-11-21 11:44:40      2018-03-26 17:07:16             6103   \n",
       "23            2017-12-29 02:18:24      2018-03-26 17:07:16             6104   \n",
       "33            2017-11-27 17:58:46      2018-03-26 17:07:16             6103   \n",
       "...                           ...                      ...              ...   \n",
       "152600        2019-12-16 13:09:08      2020-03-31 11:47:59             6176   \n",
       "152605        2019-10-03 18:01:23      2020-03-31 15:22:15             6096   \n",
       "152610        2019-08-29 16:53:58      2020-03-31 15:22:15             6114   \n",
       "152628        2019-11-05 17:10:31      2020-04-01 11:24:06             6176   \n",
       "152635        2019-10-30 16:18:03      2020-04-01 11:24:06             6176   \n",
       "\n",
       "          case_topic_1st_level  \\\n",
       "11      Direito Previdenciário   \n",
       "16      Direito Previdenciário   \n",
       "19      Direito Previdenciário   \n",
       "23      Direito Previdenciário   \n",
       "33      Direito Previdenciário   \n",
       "...                        ...   \n",
       "152600  Direito Previdenciário   \n",
       "152605  Direito Previdenciário   \n",
       "152610  Direito Previdenciário   \n",
       "152628  Direito Previdenciário   \n",
       "152635  Direito Previdenciário   \n",
       "\n",
       "                                     case_topic_2nd_level  \\\n",
       "11                                  Benefícios em Espécie   \n",
       "16                                  Benefícios em Espécie   \n",
       "19                                  Benefícios em Espécie   \n",
       "23                                  Benefícios em Espécie   \n",
       "33                                  Benefícios em Espécie   \n",
       "...                                                   ...   \n",
       "152600  Pedidos Genéricos Relativos aos Benefícios em ...   \n",
       "152605                              Benefícios em Espécie   \n",
       "152610                              Benefícios em Espécie   \n",
       "152628  Pedidos Genéricos Relativos aos Benefícios em ...   \n",
       "152635  Pedidos Genéricos Relativos aos Benefícios em ...   \n",
       "\n",
       "                             case_topic_3rd_level court_id  \n",
       "11                    Aposentadoria por Invalidez    19-CE  \n",
       "16                  Auxílio-Doença Previdenciário    31-CE  \n",
       "19               Salário-Maternidade (Art. 71/73)    31-CE  \n",
       "23                   Pensão por Morte (Art. 74/9)    30-CE  \n",
       "33               Salário-Maternidade (Art. 71/73)    25-CE  \n",
       "...                                           ...      ...  \n",
       "152600            Parcelas de benefício não pagas    15-PB  \n",
       "152605       Aposentadoria por Idade (Art. 48/51)     7-PB  \n",
       "152610  Benefício Assistencial (Art. 203,V CF/88)    13-PB  \n",
       "152628            Parcelas de benefício não pagas    15-PB  \n",
       "152635            Parcelas de benefício não pagas    15-PB  \n",
       "\n",
       "[20000 rows x 10 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 40000 entries, 2 to 305281\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   case_number  40000 non-null  object\n",
      " 1   text         40000 non-null  object\n",
      " 2   outcome      40000 non-null  object\n",
      " 3   ruling_type  40000 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_texts.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_number</th>\n",
       "      <th>text</th>\n",
       "      <th>outcome</th>\n",
       "      <th>date_appeal_panel_ruling</th>\n",
       "      <th>outcome_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5318</th>\n",
       "      <td>0529055-14.2017.4.05.8100</td>\n",
       "      <td>RELATÓRIO Trata-se de recurso interposto pela ...</td>\n",
       "      <td>NÃO PROVIMENTO</td>\n",
       "      <td>2018-03-26 17:02:45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19114</th>\n",
       "      <td>0504805-96.2017.4.05.8105</td>\n",
       "      <td>DIREITO PREVIDENCIÁRIO. AUXÍLIO-DOENÇA. LAUDO ...</td>\n",
       "      <td>NÃO PROVIMENTO</td>\n",
       "      <td>2018-03-26 17:07:16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13460</th>\n",
       "      <td>0511681-76.2017.4.05.8102</td>\n",
       "      <td>RECURSO INOMINADO. DIREITO PREVIDENCIÁRIO. PEN...</td>\n",
       "      <td>PROVIMENTO</td>\n",
       "      <td>2018-03-26 17:07:16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2359</th>\n",
       "      <td>0502738-58.2017.4.05.8106</td>\n",
       "      <td>ADMINISTRATIVO. GRATIFICAÇÃO DE DESEMPENHO. GD...</td>\n",
       "      <td>NÃO PROVIMENTO</td>\n",
       "      <td>2018-03-26 17:07:16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1706</th>\n",
       "      <td>0509527-91.2017.4.05.8100</td>\n",
       "      <td>EMENTA: APLICAÇÃO DO INPC À CORREÇÃO MONETÁRIA...</td>\n",
       "      <td>PROVIMENTO</td>\n",
       "      <td>2018-03-26 17:07:16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2880</th>\n",
       "      <td>0500987-74.2019.4.05.8200</td>\n",
       "      <td>VOTO-EMENTA ADMINISTRATIVO. CONVERSÃO DE LICEN...</td>\n",
       "      <td>PROVIMENTO</td>\n",
       "      <td>2020-03-31 15:22:15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12876</th>\n",
       "      <td>0508370-06.2019.4.05.8200</td>\n",
       "      <td>VOTO – EMENTA PREVIDENCIÁRIO. APOSENTADORIA PO...</td>\n",
       "      <td>NÃO PROVIMENTO</td>\n",
       "      <td>2020-03-31 15:22:15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0507364-55.2019.4.05.8202</td>\n",
       "      <td>VOTO - EMENTA PREVIDENCIÁRIO. EXTENSÃO DO PERÍ...</td>\n",
       "      <td>PROVIMENTO PARCIAL</td>\n",
       "      <td>2020-04-01 11:24:06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19799</th>\n",
       "      <td>0508286-96.2019.4.05.8202</td>\n",
       "      <td>VOTO - EMENTA PREVIDENCIÁRIO. EXTENSÃO DO PERÍ...</td>\n",
       "      <td>PROVIMENTO PARCIAL</td>\n",
       "      <td>2020-04-01 11:24:06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6239</th>\n",
       "      <td>0507511-81.2019.4.05.8202</td>\n",
       "      <td>VOTO - EMENTA PREVIDENCIÁRIO. EXTENSÃO DO PERÍ...</td>\n",
       "      <td>PROVIMENTO PARCIAL</td>\n",
       "      <td>2020-04-01 11:24:06</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     case_number  \\\n",
       "5318   0529055-14.2017.4.05.8100   \n",
       "19114  0504805-96.2017.4.05.8105   \n",
       "13460  0511681-76.2017.4.05.8102   \n",
       "2359   0502738-58.2017.4.05.8106   \n",
       "1706   0509527-91.2017.4.05.8100   \n",
       "...                          ...   \n",
       "2880   0500987-74.2019.4.05.8200   \n",
       "12876  0508370-06.2019.4.05.8200   \n",
       "183    0507364-55.2019.4.05.8202   \n",
       "19799  0508286-96.2019.4.05.8202   \n",
       "6239   0507511-81.2019.4.05.8202   \n",
       "\n",
       "                                                    text             outcome  \\\n",
       "5318   RELATÓRIO Trata-se de recurso interposto pela ...      NÃO PROVIMENTO   \n",
       "19114  DIREITO PREVIDENCIÁRIO. AUXÍLIO-DOENÇA. LAUDO ...      NÃO PROVIMENTO   \n",
       "13460  RECURSO INOMINADO. DIREITO PREVIDENCIÁRIO. PEN...          PROVIMENTO   \n",
       "2359   ADMINISTRATIVO. GRATIFICAÇÃO DE DESEMPENHO. GD...      NÃO PROVIMENTO   \n",
       "1706   EMENTA: APLICAÇÃO DO INPC À CORREÇÃO MONETÁRIA...          PROVIMENTO   \n",
       "...                                                  ...                 ...   \n",
       "2880   VOTO-EMENTA ADMINISTRATIVO. CONVERSÃO DE LICEN...          PROVIMENTO   \n",
       "12876  VOTO – EMENTA PREVIDENCIÁRIO. APOSENTADORIA PO...      NÃO PROVIMENTO   \n",
       "183    VOTO - EMENTA PREVIDENCIÁRIO. EXTENSÃO DO PERÍ...  PROVIMENTO PARCIAL   \n",
       "19799  VOTO - EMENTA PREVIDENCIÁRIO. EXTENSÃO DO PERÍ...  PROVIMENTO PARCIAL   \n",
       "6239   VOTO - EMENTA PREVIDENCIÁRIO. EXTENSÃO DO PERÍ...  PROVIMENTO PARCIAL   \n",
       "\n",
       "      date_appeal_panel_ruling  outcome_int  \n",
       "5318       2018-03-26 17:02:45            0  \n",
       "19114      2018-03-26 17:07:16            0  \n",
       "13460      2018-03-26 17:07:16            1  \n",
       "2359       2018-03-26 17:07:16            0  \n",
       "1706       2018-03-26 17:07:16            1  \n",
       "...                        ...          ...  \n",
       "2880       2020-03-31 15:22:15            1  \n",
       "12876      2020-03-31 15:22:15            0  \n",
       "183        2020-04-01 11:24:06            2  \n",
       "19799      2020-04-01 11:24:06            2  \n",
       "6239       2020-04-01 11:24:06            2  \n",
       "\n",
       "[20000 rows x 5 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 20000 entries, 11 to 152635\n",
      "Data columns (total 10 columns):\n",
      " #   Column                      Non-Null Count  Dtype         \n",
      "---  ------                      --------------  -----         \n",
      " 0   case_number                 20000 non-null  object        \n",
      " 1   filing_date                 20000 non-null  object        \n",
      " 2   defendant_normalized        20000 non-null  object        \n",
      " 3   date_first_instance_ruling  20000 non-null  datetime64[ns]\n",
      " 4   date_appeal_panel_ruling    20000 non-null  datetime64[us]\n",
      " 5   case_topic_code             20000 non-null  int64         \n",
      " 6   case_topic_1st_level        20000 non-null  object        \n",
      " 7   case_topic_2nd_level        20000 non-null  object        \n",
      " 8   case_topic_3rd_level        19993 non-null  object        \n",
      " 9   court_id                    20000 non-null  object        \n",
      "dtypes: datetime64[ns](1), datetime64[us](1), int64(1), object(7)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df_meta.date_first_instance_ruling = pd.to_datetime(\n",
    "    df_meta.date_first_instance_ruling, yearfirst=True\n",
    ")\n",
    "df_meta.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    20000.000000\n",
       "mean       153.045100\n",
       "std        267.485493\n",
       "min          8.000000\n",
       "25%         55.000000\n",
       "50%         80.000000\n",
       "75%        127.000000\n",
       "max       3599.000000\n",
       "Name: time_to_trial_appeal, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_meta[\"time_to_trial_appeal\"] = (\n",
    "    df_meta[\"date_appeal_panel_ruling\"] - df_meta[\"date_first_instance_ruling\"]\n",
    ")\n",
    "df_meta[\"time_to_trial_appeal\"] = df_meta[\"time_to_trial_appeal\"].dt.days\n",
    "df_meta[\"time_to_trial_appeal\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_number</th>\n",
       "      <th>text</th>\n",
       "      <th>outcome</th>\n",
       "      <th>ruling_type</th>\n",
       "      <th>filing_date</th>\n",
       "      <th>defendant_normalized</th>\n",
       "      <th>date_first_instance_ruling</th>\n",
       "      <th>date_appeal_panel_ruling</th>\n",
       "      <th>case_topic_code</th>\n",
       "      <th>case_topic_1st_level</th>\n",
       "      <th>case_topic_2nd_level</th>\n",
       "      <th>case_topic_3rd_level</th>\n",
       "      <th>court_id</th>\n",
       "      <th>time_to_trial_appeal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0515165-56.2018.4.05.8202</td>\n",
       "      <td>SENTENÇA I - RELATÓRIO Dispensada a feitura do...</td>\n",
       "      <td>IMPROCEDENTE</td>\n",
       "      <td>SENTENÇA</td>\n",
       "      <td>2018-10-19 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2019-04-25 15:55:52</td>\n",
       "      <td>2019-08-20 10:11:35</td>\n",
       "      <td>6114</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Benefício Assistencial (Art. 203,V CF/88)</td>\n",
       "      <td>15-PB</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0506287-42.2018.4.05.8300</td>\n",
       "      <td>SENTENÇA I - RELATÓRIO Dispensado, nos termos ...</td>\n",
       "      <td>PARCIALMENTE PROCEDENTE</td>\n",
       "      <td>SENTENÇA</td>\n",
       "      <td>2018-04-20 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2019-05-02 23:48:08</td>\n",
       "      <td>2019-07-03 18:08:09</td>\n",
       "      <td>6099</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Aposentadoria por Tempo de Serviço (Art. 52/4)</td>\n",
       "      <td>14-PE</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0505610-87.2019.4.05.8102</td>\n",
       "      <td>SENTENÇA I – RELATÓRIO Por força do disposto n...</td>\n",
       "      <td>PROCEDENTE</td>\n",
       "      <td>SENTENÇA</td>\n",
       "      <td>2019-05-06 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2019-07-28 09:57:06</td>\n",
       "      <td>2019-09-26 14:16:00</td>\n",
       "      <td>6101</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Auxílio-Doença Previdenciário</td>\n",
       "      <td>30-CE</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0501720-83.2018.4.05.8100</td>\n",
       "      <td>SENTENÇA Dispensado o relatório, nos termos do...</td>\n",
       "      <td>IMPROCEDENTE</td>\n",
       "      <td>SENTENÇA</td>\n",
       "      <td>2018-01-24 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2018-08-27 12:39:45</td>\n",
       "      <td>2018-10-31 15:36:41</td>\n",
       "      <td>6114</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Benefício Assistencial (Art. 203,V CF/88)</td>\n",
       "      <td>13-CE</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0501533-63.2018.4.05.8104</td>\n",
       "      <td>TERMO DE AUDIÊNCIA Aos 18 de setembro de 2018,...</td>\n",
       "      <td>PROCEDENTE</td>\n",
       "      <td>SENTENÇA</td>\n",
       "      <td>2018-05-02 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2018-09-18 14:33:01</td>\n",
       "      <td>2018-11-14 13:50:04</td>\n",
       "      <td>6103</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Salário-Maternidade (Art. 71/73)</td>\n",
       "      <td>22-CE</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>0500473-94.2019.4.05.8306</td>\n",
       "      <td>SENTENÇA (Tipo C – Sem Resolução do Mérito) I ...</td>\n",
       "      <td>EXTINTO SEM MÉRITO</td>\n",
       "      <td>SENTENÇA</td>\n",
       "      <td>2019-03-14 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2019-04-15 09:12:31</td>\n",
       "      <td>2019-07-22 15:38:11</td>\n",
       "      <td>6114</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Benefício Assistencial (Art. 203,V CF/88)</td>\n",
       "      <td>25-PE</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>0505814-16.2014.4.05.8100</td>\n",
       "      <td>SENTENÇA Por força do disposto no art. 38 da L...</td>\n",
       "      <td>IMPROCEDENTE</td>\n",
       "      <td>SENTENÇA</td>\n",
       "      <td>2014-03-21 00:00:00</td>\n",
       "      <td>CEF</td>\n",
       "      <td>2014-03-28 10:32:04</td>\n",
       "      <td>2018-07-06 11:33:24</td>\n",
       "      <td>7691</td>\n",
       "      <td>Direito Civil</td>\n",
       "      <td>Obrigações</td>\n",
       "      <td>Inadimplemento</td>\n",
       "      <td>14-CE</td>\n",
       "      <td>1561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>0500153-84.2018.4.05.8304</td>\n",
       "      <td>SENTENÇA I. Relatório Ante o disposto no art. ...</td>\n",
       "      <td>IMPROCEDENTE</td>\n",
       "      <td>SENTENÇA</td>\n",
       "      <td>2018-01-24 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2018-03-23 08:35:39</td>\n",
       "      <td>2018-05-23 14:42:26</td>\n",
       "      <td>6177</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Pedidos Genéricos Relativos aos Benefícios em ...</td>\n",
       "      <td>Concessão</td>\n",
       "      <td>20-PE</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>0511210-80.2019.4.05.8202</td>\n",
       "      <td>SENTENÇA I – RELATÓRIO Trata-se de ação de ind...</td>\n",
       "      <td>IMPROCEDENTE</td>\n",
       "      <td>SENTENÇA</td>\n",
       "      <td>2019-11-08 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2019-12-16 13:09:08</td>\n",
       "      <td>2020-03-16 14:46:57</td>\n",
       "      <td>6176</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Pedidos Genéricos Relativos aos Benefícios em ...</td>\n",
       "      <td>Parcelas de benefício não pagas</td>\n",
       "      <td>15-PB</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>0503936-39.2017.4.05.8201</td>\n",
       "      <td>SENTENÇA Dispensado o relatório por força do d...</td>\n",
       "      <td>PROCEDENTE</td>\n",
       "      <td>SENTENÇA</td>\n",
       "      <td>2017-05-18 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2017-09-27 15:04:01</td>\n",
       "      <td>2018-07-16 11:49:04</td>\n",
       "      <td>6101</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Auxílio-Doença Previdenciário</td>\n",
       "      <td>9-PB</td>\n",
       "      <td>291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     case_number  \\\n",
       "0      0515165-56.2018.4.05.8202   \n",
       "1      0506287-42.2018.4.05.8300   \n",
       "2      0505610-87.2019.4.05.8102   \n",
       "3      0501720-83.2018.4.05.8100   \n",
       "4      0501533-63.2018.4.05.8104   \n",
       "...                          ...   \n",
       "19995  0500473-94.2019.4.05.8306   \n",
       "19996  0505814-16.2014.4.05.8100   \n",
       "19997  0500153-84.2018.4.05.8304   \n",
       "19998  0511210-80.2019.4.05.8202   \n",
       "19999  0503936-39.2017.4.05.8201   \n",
       "\n",
       "                                                    text  \\\n",
       "0      SENTENÇA I - RELATÓRIO Dispensada a feitura do...   \n",
       "1      SENTENÇA I - RELATÓRIO Dispensado, nos termos ...   \n",
       "2      SENTENÇA I – RELATÓRIO Por força do disposto n...   \n",
       "3      SENTENÇA Dispensado o relatório, nos termos do...   \n",
       "4      TERMO DE AUDIÊNCIA Aos 18 de setembro de 2018,...   \n",
       "...                                                  ...   \n",
       "19995  SENTENÇA (Tipo C – Sem Resolução do Mérito) I ...   \n",
       "19996  SENTENÇA Por força do disposto no art. 38 da L...   \n",
       "19997  SENTENÇA I. Relatório Ante o disposto no art. ...   \n",
       "19998  SENTENÇA I – RELATÓRIO Trata-se de ação de ind...   \n",
       "19999  SENTENÇA Dispensado o relatório por força do d...   \n",
       "\n",
       "                       outcome ruling_type          filing_date  \\\n",
       "0                 IMPROCEDENTE    SENTENÇA  2018-10-19 00:00:00   \n",
       "1      PARCIALMENTE PROCEDENTE    SENTENÇA  2018-04-20 00:00:00   \n",
       "2                   PROCEDENTE    SENTENÇA  2019-05-06 00:00:00   \n",
       "3                 IMPROCEDENTE    SENTENÇA  2018-01-24 00:00:00   \n",
       "4                   PROCEDENTE    SENTENÇA  2018-05-02 00:00:00   \n",
       "...                        ...         ...                  ...   \n",
       "19995       EXTINTO SEM MÉRITO    SENTENÇA  2019-03-14 00:00:00   \n",
       "19996             IMPROCEDENTE    SENTENÇA  2014-03-21 00:00:00   \n",
       "19997             IMPROCEDENTE    SENTENÇA  2018-01-24 00:00:00   \n",
       "19998             IMPROCEDENTE    SENTENÇA  2019-11-08 00:00:00   \n",
       "19999               PROCEDENTE    SENTENÇA  2017-05-18 00:00:00   \n",
       "\n",
       "      defendant_normalized date_first_instance_ruling  \\\n",
       "0                     INSS        2019-04-25 15:55:52   \n",
       "1                     INSS        2019-05-02 23:48:08   \n",
       "2                     INSS        2019-07-28 09:57:06   \n",
       "3                     INSS        2018-08-27 12:39:45   \n",
       "4                     INSS        2018-09-18 14:33:01   \n",
       "...                    ...                        ...   \n",
       "19995                 INSS        2019-04-15 09:12:31   \n",
       "19996                  CEF        2014-03-28 10:32:04   \n",
       "19997                 INSS        2018-03-23 08:35:39   \n",
       "19998                 INSS        2019-12-16 13:09:08   \n",
       "19999                 INSS        2017-09-27 15:04:01   \n",
       "\n",
       "      date_appeal_panel_ruling  case_topic_code    case_topic_1st_level  \\\n",
       "0          2019-08-20 10:11:35             6114  Direito Previdenciário   \n",
       "1          2019-07-03 18:08:09             6099  Direito Previdenciário   \n",
       "2          2019-09-26 14:16:00             6101  Direito Previdenciário   \n",
       "3          2018-10-31 15:36:41             6114  Direito Previdenciário   \n",
       "4          2018-11-14 13:50:04             6103  Direito Previdenciário   \n",
       "...                        ...              ...                     ...   \n",
       "19995      2019-07-22 15:38:11             6114  Direito Previdenciário   \n",
       "19996      2018-07-06 11:33:24             7691           Direito Civil   \n",
       "19997      2018-05-23 14:42:26             6177  Direito Previdenciário   \n",
       "19998      2020-03-16 14:46:57             6176  Direito Previdenciário   \n",
       "19999      2018-07-16 11:49:04             6101  Direito Previdenciário   \n",
       "\n",
       "                                    case_topic_2nd_level  \\\n",
       "0                                  Benefícios em Espécie   \n",
       "1                                  Benefícios em Espécie   \n",
       "2                                  Benefícios em Espécie   \n",
       "3                                  Benefícios em Espécie   \n",
       "4                                  Benefícios em Espécie   \n",
       "...                                                  ...   \n",
       "19995                              Benefícios em Espécie   \n",
       "19996                                         Obrigações   \n",
       "19997  Pedidos Genéricos Relativos aos Benefícios em ...   \n",
       "19998  Pedidos Genéricos Relativos aos Benefícios em ...   \n",
       "19999                              Benefícios em Espécie   \n",
       "\n",
       "                                 case_topic_3rd_level court_id  \\\n",
       "0           Benefício Assistencial (Art. 203,V CF/88)    15-PB   \n",
       "1      Aposentadoria por Tempo de Serviço (Art. 52/4)    14-PE   \n",
       "2                       Auxílio-Doença Previdenciário    30-CE   \n",
       "3           Benefício Assistencial (Art. 203,V CF/88)    13-CE   \n",
       "4                    Salário-Maternidade (Art. 71/73)    22-CE   \n",
       "...                                               ...      ...   \n",
       "19995       Benefício Assistencial (Art. 203,V CF/88)    25-PE   \n",
       "19996                                  Inadimplemento    14-CE   \n",
       "19997                                       Concessão    20-PE   \n",
       "19998                 Parcelas de benefício não pagas    15-PB   \n",
       "19999                   Auxílio-Doença Previdenciário     9-PB   \n",
       "\n",
       "       time_to_trial_appeal  \n",
       "0                       116  \n",
       "1                        61  \n",
       "2                        60  \n",
       "3                        65  \n",
       "4                        56  \n",
       "...                     ...  \n",
       "19995                    98  \n",
       "19996                  1561  \n",
       "19997                    61  \n",
       "19998                    91  \n",
       "19999                   291  \n",
       "\n",
       "[20000 rows x 14 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_texts.query('ruling_type == \"SENTENÇA\"').merge(\n",
    "    df_meta, on=\"case_number\", how=\"left\"\n",
    ")\n",
    "df.drop_duplicates(subset=\"case_number\", inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_number</th>\n",
       "      <th>text</th>\n",
       "      <th>outcome</th>\n",
       "      <th>ruling_type</th>\n",
       "      <th>filing_date</th>\n",
       "      <th>defendant_normalized</th>\n",
       "      <th>date_first_instance_ruling</th>\n",
       "      <th>date_appeal_panel_ruling</th>\n",
       "      <th>case_topic_code</th>\n",
       "      <th>case_topic_1st_level</th>\n",
       "      <th>case_topic_2nd_level</th>\n",
       "      <th>case_topic_3rd_level</th>\n",
       "      <th>court_id</th>\n",
       "      <th>time_to_trial_appeal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0529055-14.2017.4.05.8100</td>\n",
       "      <td>SENTENÇA Dispensado o relatório (art. 1o da Le...</td>\n",
       "      <td>IMPROCEDENTE</td>\n",
       "      <td>SENTENÇA</td>\n",
       "      <td>2017-12-22 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2018-01-30 12:09:11</td>\n",
       "      <td>2018-03-26 17:02:45</td>\n",
       "      <td>6138</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>RMI - Renda Mensal Inicial, Reajustes e Revisõ...</td>\n",
       "      <td>Reajustes e Revisões Específicos</td>\n",
       "      <td>14-CE</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0505725-70.2017.4.05.8105</td>\n",
       "      <td>JUSTIÇA FEDERAL SEÇÃO JUDICIÁRIA DO ESTADO DO ...</td>\n",
       "      <td>IMPROCEDENTE</td>\n",
       "      <td>SENTENÇA</td>\n",
       "      <td>2017-11-21 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2018-02-06 15:50:11</td>\n",
       "      <td>2018-03-26 17:07:16</td>\n",
       "      <td>6101</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Auxílio-Doença Previdenciário</td>\n",
       "      <td>23-CE</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0501051-55.2017.4.05.8103</td>\n",
       "      <td>SENTENÇA I. RELATÓRIO Trata-se de demanda prev...</td>\n",
       "      <td>IMPROCEDENTE</td>\n",
       "      <td>SENTENÇA</td>\n",
       "      <td>2017-02-03 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2017-11-21 11:44:40</td>\n",
       "      <td>2018-03-26 17:07:16</td>\n",
       "      <td>6103</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Salário-Maternidade (Art. 71/73)</td>\n",
       "      <td>31-CE</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0512841-39.2017.4.05.8102</td>\n",
       "      <td>PODER JUDICIÁRIO JUSTIÇA FEDERAL DE PRIMEIRA I...</td>\n",
       "      <td>PROCEDENTE</td>\n",
       "      <td>SENTENÇA</td>\n",
       "      <td>2017-10-25 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2017-11-09 10:58:36</td>\n",
       "      <td>2018-03-26 17:07:16</td>\n",
       "      <td>10288</td>\n",
       "      <td>Direito Administrativo e outras matérias do Di...</td>\n",
       "      <td>Servidor Público Civil</td>\n",
       "      <td>Sistema Remuneratório e Benefícios</td>\n",
       "      <td>17-CE</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0509917-52.2017.4.05.8103</td>\n",
       "      <td>SENTENÇA I. RELATÓRIO Trata-sede ação de rito ...</td>\n",
       "      <td>IMPROCEDENTE</td>\n",
       "      <td>SENTENÇA</td>\n",
       "      <td>2017-09-26 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2018-01-15 18:07:55</td>\n",
       "      <td>2018-03-26 17:07:16</td>\n",
       "      <td>6101</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Auxílio-Doença Previdenciário</td>\n",
       "      <td>31-CE</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>0508370-06.2019.4.05.8200</td>\n",
       "      <td>SENTENÇA – TIPO A Vistos etc. Trata-se de ação...</td>\n",
       "      <td>PROCEDENTE</td>\n",
       "      <td>SENTENÇA</td>\n",
       "      <td>2019-06-13 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2019-10-03 18:01:23</td>\n",
       "      <td>2020-03-31 15:22:15</td>\n",
       "      <td>6096</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Aposentadoria por Idade (Art. 48/51)</td>\n",
       "      <td>7-PB</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>0508717-73.2018.4.05.8200</td>\n",
       "      <td>SENTENÇA Dispensado o relatório nos termos do ...</td>\n",
       "      <td>IMPROCEDENTE</td>\n",
       "      <td>SENTENÇA</td>\n",
       "      <td>2018-06-20 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2019-08-29 16:53:58</td>\n",
       "      <td>2020-03-31 15:22:15</td>\n",
       "      <td>6114</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Benefícios em Espécie</td>\n",
       "      <td>Benefício Assistencial (Art. 203,V CF/88)</td>\n",
       "      <td>13-PB</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>0507511-81.2019.4.05.8202</td>\n",
       "      <td>SENTENÇA I – RELATÓRIO Trata-se de ação de ind...</td>\n",
       "      <td>IMPROCEDENTE</td>\n",
       "      <td>SENTENÇA</td>\n",
       "      <td>2019-08-22 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2019-11-05 17:10:31</td>\n",
       "      <td>2020-04-01 11:24:06</td>\n",
       "      <td>6176</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Pedidos Genéricos Relativos aos Benefícios em ...</td>\n",
       "      <td>Parcelas de benefício não pagas</td>\n",
       "      <td>15-PB</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>0507364-55.2019.4.05.8202</td>\n",
       "      <td>SENTENÇA I – RELATÓRIO Trata-se de ação de ind...</td>\n",
       "      <td>IMPROCEDENTE</td>\n",
       "      <td>SENTENÇA</td>\n",
       "      <td>2019-08-21 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2019-10-30 16:18:03</td>\n",
       "      <td>2020-04-01 11:24:06</td>\n",
       "      <td>6176</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Pedidos Genéricos Relativos aos Benefícios em ...</td>\n",
       "      <td>Parcelas de benefício não pagas</td>\n",
       "      <td>15-PB</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>0508286-96.2019.4.05.8202</td>\n",
       "      <td>SENTENÇA I – RELATÓRIO Trata-se de ação de ind...</td>\n",
       "      <td>IMPROCEDENTE</td>\n",
       "      <td>SENTENÇA</td>\n",
       "      <td>2019-09-05 00:00:00</td>\n",
       "      <td>INSS</td>\n",
       "      <td>2019-11-03 17:39:57</td>\n",
       "      <td>2020-04-01 11:24:06</td>\n",
       "      <td>6176</td>\n",
       "      <td>Direito Previdenciário</td>\n",
       "      <td>Pedidos Genéricos Relativos aos Benefícios em ...</td>\n",
       "      <td>Parcelas de benefício não pagas</td>\n",
       "      <td>15-PB</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     case_number  \\\n",
       "0      0529055-14.2017.4.05.8100   \n",
       "1      0505725-70.2017.4.05.8105   \n",
       "2      0501051-55.2017.4.05.8103   \n",
       "3      0512841-39.2017.4.05.8102   \n",
       "4      0509917-52.2017.4.05.8103   \n",
       "...                          ...   \n",
       "19995  0508370-06.2019.4.05.8200   \n",
       "19996  0508717-73.2018.4.05.8200   \n",
       "19997  0507511-81.2019.4.05.8202   \n",
       "19998  0507364-55.2019.4.05.8202   \n",
       "19999  0508286-96.2019.4.05.8202   \n",
       "\n",
       "                                                    text       outcome  \\\n",
       "0      SENTENÇA Dispensado o relatório (art. 1o da Le...  IMPROCEDENTE   \n",
       "1      JUSTIÇA FEDERAL SEÇÃO JUDICIÁRIA DO ESTADO DO ...  IMPROCEDENTE   \n",
       "2      SENTENÇA I. RELATÓRIO Trata-se de demanda prev...  IMPROCEDENTE   \n",
       "3      PODER JUDICIÁRIO JUSTIÇA FEDERAL DE PRIMEIRA I...    PROCEDENTE   \n",
       "4      SENTENÇA I. RELATÓRIO Trata-sede ação de rito ...  IMPROCEDENTE   \n",
       "...                                                  ...           ...   \n",
       "19995  SENTENÇA – TIPO A Vistos etc. Trata-se de ação...    PROCEDENTE   \n",
       "19996  SENTENÇA Dispensado o relatório nos termos do ...  IMPROCEDENTE   \n",
       "19997  SENTENÇA I – RELATÓRIO Trata-se de ação de ind...  IMPROCEDENTE   \n",
       "19998  SENTENÇA I – RELATÓRIO Trata-se de ação de ind...  IMPROCEDENTE   \n",
       "19999  SENTENÇA I – RELATÓRIO Trata-se de ação de ind...  IMPROCEDENTE   \n",
       "\n",
       "      ruling_type          filing_date defendant_normalized  \\\n",
       "0        SENTENÇA  2017-12-22 00:00:00                 INSS   \n",
       "1        SENTENÇA  2017-11-21 00:00:00                 INSS   \n",
       "2        SENTENÇA  2017-02-03 00:00:00                 INSS   \n",
       "3        SENTENÇA  2017-10-25 00:00:00                 INSS   \n",
       "4        SENTENÇA  2017-09-26 00:00:00                 INSS   \n",
       "...           ...                  ...                  ...   \n",
       "19995    SENTENÇA  2019-06-13 00:00:00                 INSS   \n",
       "19996    SENTENÇA  2018-06-20 00:00:00                 INSS   \n",
       "19997    SENTENÇA  2019-08-22 00:00:00                 INSS   \n",
       "19998    SENTENÇA  2019-08-21 00:00:00                 INSS   \n",
       "19999    SENTENÇA  2019-09-05 00:00:00                 INSS   \n",
       "\n",
       "      date_first_instance_ruling date_appeal_panel_ruling  case_topic_code  \\\n",
       "0            2018-01-30 12:09:11      2018-03-26 17:02:45             6138   \n",
       "1            2018-02-06 15:50:11      2018-03-26 17:07:16             6101   \n",
       "2            2017-11-21 11:44:40      2018-03-26 17:07:16             6103   \n",
       "3            2017-11-09 10:58:36      2018-03-26 17:07:16            10288   \n",
       "4            2018-01-15 18:07:55      2018-03-26 17:07:16             6101   \n",
       "...                          ...                      ...              ...   \n",
       "19995        2019-10-03 18:01:23      2020-03-31 15:22:15             6096   \n",
       "19996        2019-08-29 16:53:58      2020-03-31 15:22:15             6114   \n",
       "19997        2019-11-05 17:10:31      2020-04-01 11:24:06             6176   \n",
       "19998        2019-10-30 16:18:03      2020-04-01 11:24:06             6176   \n",
       "19999        2019-11-03 17:39:57      2020-04-01 11:24:06             6176   \n",
       "\n",
       "                                    case_topic_1st_level  \\\n",
       "0                                 Direito Previdenciário   \n",
       "1                                 Direito Previdenciário   \n",
       "2                                 Direito Previdenciário   \n",
       "3      Direito Administrativo e outras matérias do Di...   \n",
       "4                                 Direito Previdenciário   \n",
       "...                                                  ...   \n",
       "19995                             Direito Previdenciário   \n",
       "19996                             Direito Previdenciário   \n",
       "19997                             Direito Previdenciário   \n",
       "19998                             Direito Previdenciário   \n",
       "19999                             Direito Previdenciário   \n",
       "\n",
       "                                    case_topic_2nd_level  \\\n",
       "0      RMI - Renda Mensal Inicial, Reajustes e Revisõ...   \n",
       "1                                  Benefícios em Espécie   \n",
       "2                                  Benefícios em Espécie   \n",
       "3                                 Servidor Público Civil   \n",
       "4                                  Benefícios em Espécie   \n",
       "...                                                  ...   \n",
       "19995                              Benefícios em Espécie   \n",
       "19996                              Benefícios em Espécie   \n",
       "19997  Pedidos Genéricos Relativos aos Benefícios em ...   \n",
       "19998  Pedidos Genéricos Relativos aos Benefícios em ...   \n",
       "19999  Pedidos Genéricos Relativos aos Benefícios em ...   \n",
       "\n",
       "                            case_topic_3rd_level court_id  \\\n",
       "0               Reajustes e Revisões Específicos    14-CE   \n",
       "1                  Auxílio-Doença Previdenciário    23-CE   \n",
       "2               Salário-Maternidade (Art. 71/73)    31-CE   \n",
       "3             Sistema Remuneratório e Benefícios    17-CE   \n",
       "4                  Auxílio-Doença Previdenciário    31-CE   \n",
       "...                                          ...      ...   \n",
       "19995       Aposentadoria por Idade (Art. 48/51)     7-PB   \n",
       "19996  Benefício Assistencial (Art. 203,V CF/88)    13-PB   \n",
       "19997            Parcelas de benefício não pagas    15-PB   \n",
       "19998            Parcelas de benefício não pagas    15-PB   \n",
       "19999            Parcelas de benefício não pagas    15-PB   \n",
       "\n",
       "       time_to_trial_appeal  \n",
       "0                        55  \n",
       "1                        48  \n",
       "2                       125  \n",
       "3                       137  \n",
       "4                        69  \n",
       "...                     ...  \n",
       "19995                   179  \n",
       "19996                   214  \n",
       "19997                   147  \n",
       "19998                   153  \n",
       "19999                   149  \n",
       "\n",
       "[20000 rows x 14 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sort_values(by=\"date_appeal_panel_ruling\", ascending=True).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16000, 14), (2000, 14), (2000, 14))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the length of the training set (80% of the total data)\n",
    "train_len = int(0.8 * len(df))\n",
    "\n",
    "# Determine the length of the validation set (10% of the total data)\n",
    "val_len = int(0.1 * len(df))\n",
    "\n",
    "# Create the training set by selecting the first 'train_len' rows from the dataframe\n",
    "df_train = df.iloc[:train_len].copy()\n",
    "\n",
    "# Create the validation set by selecting the next 'val_len' rows after the training set\n",
    "df_val = df.iloc[train_len : train_len + val_len].copy()\n",
    "\n",
    "# Create the test set by selecting the remaining rows after the training and validation sets\n",
    "df_test = df.iloc[train_len + val_len :].copy()\n",
    "\n",
    "# Print the shapes of the training, validation, and test sets to verify the splits\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in [df_train, df_val, df_test]:\n",
    "    dataframe[\"clean_text\"] = dataframe.text.apply(remove_accented_characters)\n",
    "    dataframe[\"clean_text\"] = dataframe.clean_text.apply(\n",
    "        remove_numbers_punctuation_from_text\n",
    "    )\n",
    "    dataframe[\"clean_text\"] = dataframe.clean_text.apply(remove_excessive_spaces)\n",
    "    dataframe[\"clean_text\"] = dataframe.clean_text.apply(remove_short_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, it's up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. What are the three key steps in text processing mentioned in the NLP pipeline?\n",
    "\n",
    "2. What is the \"60 iterations rule\" in the context of random search for hyperparameter optimization?\n",
    "\n",
    "3. What are the three main components of Bayesian Optimization for hyperparameter tuning?\n",
    "\n",
    "4. How does the notebook split the data for training, validation and testing?\n",
    "\n",
    "5. What is the purpose of the StackingClassifier in the context of this notebook?\n",
    "\n",
    "6. What evaluation metrics are used to assess the performance of the classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Answers are commented inside this cell.`\n",
    "\n",
    "<!-- 1. The three key steps in text processing mentioned in the NLP pipeline are: Normalization (standardizing text), Tokenization (breaking text into smaller units), and Numericalization (converting tokens to numerical representations).\n",
    "\n",
    "2. The \"60 iterations rule\" states that 60 iterations of random search can find the best 5% set of parameters 95% of the time, regardless of the grid size. This rule provides an efficient approach to hyperparameter optimization.\n",
    "\n",
    "3. The three main components of Bayesian Optimization for hyperparameter tuning are: Objective Function (evaluates model performance), Surrogate Model (probabilistic approximation of the objective function), and Acquisition Function (guides selection of next hyperparameter set to evaluate).\n",
    "\n",
    "4. The notebook splits the data chronologically: the first 80% of the data (sorted by date) is used for training, the next 10% for validation, and the final 10% for testing.\n",
    "\n",
    "5. The StackingClassifier is used to combine multiple base classifiers with a meta-classifier, aiming to achieve superior performance compared to individual models by leveraging their collective strengths.\n",
    "\n",
    "6. The evaluation metrics used include F1 score, balanced accuracy, accuracy, Matthews Correlation Coefficient (MCC), and confusion matrix. The notebook particularly emphasizes MCC as a key metric.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacentric_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
