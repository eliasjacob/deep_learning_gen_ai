{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "## Deep Learning and Generative AI\n",
    "### [Dr. Elias Jacob de Menezes Neto](https://docente.ufrn.br/elias.jacob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "## Keypoints\n",
    "\n",
    "1. **Transformers revolutionize NLP by enabling parallel processing and capturing long-range dependencies** through self-attention mechanisms, overcoming the limitations of sequential models like RNNs and LSTMs.\n",
    "\n",
    "2. **Self-attention allows transformers to weigh the importance of each token relative to all others**, capturing contextual relationships across the entire input sequence.\n",
    "\n",
    "3. **Transformer architecture consists of encoder and decoder components** built with layers of self-attention, feed-forward networks, residual connections, and layer normalization.\n",
    "\n",
    "4. **Positional encoding integrates sequence order into transformers**, typically using sine and cosine functions, ensuring the model recognizes the position of tokens within the input.\n",
    "\n",
    "5. **The quadratic complexity of self-attention limits transformers to processing sequences up to 512 tokens**, posing computational challenges for tasks involving longer texts.\n",
    "\n",
    "6. **Transformers have been successfully adapted beyond NLP**, applied in domains such as computer vision (Vision Transformers), music generation, speech recognition, and video processing.\n",
    "\n",
    "7. **Key transformer architectures include** BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pre-trained Transformer), T5 (Text-to-Text Transfer Transformer), and Transformer-XL, each tailored to specific tasks and data types.\n",
    "\n",
    "8. **Transformers can be used as feature extractors**, providing rich contextual embeddings (e.g., from the `[CLS]` token) for downstream tasks like classification.\n",
    "\n",
    "9. **Key steps for using transformers include selecting a pretrained model**, optionally fine-tuning it on domain-specific data, and conducting task-specific training by adding appropriate heads or layers.\n",
    "\n",
    "10. **Strategies are needed to handle the 512-token limit**, such as adjusting the truncation side to retain important information or using models designed to process longer sequences.\n",
    "\n",
    "11. **Fine-tuning transformers on domain-specific text enhances performance** by adapting the model to the specific language patterns and styles of the domain.\n",
    "\n",
    "12. **Perplexity is a crucial metric for assessing language models**, with lower perplexity indicating better predictive performance and understanding of the language data.\n",
    "\n",
    "## Takeaways\n",
    "\n",
    "1. **Transformers are foundational in modern NLP**, offering powerful capabilities to model complex language patterns and dependencies through their self-attention mechanisms.\n",
    "\n",
    "2. **Understanding transformer architecture and components is essential** for effectively applying them to various tasks, including awareness of their limitations and computational considerations.\n",
    "\n",
    "3. **Adapting transformers to domain-specific data through fine-tuning significantly improves their effectiveness**, enabling models to handle specialized vocabularies and styles.\n",
    "\n",
    "4. **Practitioners must address the 512-token limit** by employing strategies to handle long texts, such as focusing on the most relevant information to prevent loss due to truncation.\n",
    "\n",
    "5. **The versatility of transformers across different domains underscores their power**, demonstrating applicability beyond NLP to areas like computer vision and speech processing.\n",
    "\n",
    "6. **Selecting the appropriate transformer architecture is crucial**, as different models have unique strengths suited to specific tasks, whether understanding context, generating text, or handling sequence-to-sequence transformations.\n",
    "\n",
    "7. **Using transformers as feature extractors provides rich embeddings for downstream tasks**, leveraging the deep contextual understanding captured by the models.\n",
    "\n",
    "8. **Effective use of transformers involves leveraging pretrained models**, considering domain adaptation through fine-tuning, and being mindful of limitations such as sequence length and computational resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "\n",
    "Transformers have significantly advanced the field of Natural Language Processing (NLP) by enabling models to capture long-range dependencies and complex contextual relationships within text data. Since their introduction by Vaswani et al. in 2017, transformers have become the foundational architecture for a wide array of NLP tasks such as machine translation, text summarization, and question answering.\n",
    "\n",
    "## Moving Beyond Sequential Processing\n",
    "\n",
    "Traditional models like Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs) process sequences token by token, maintaining an internal state that captures previous information. This sequential processing limits their ability to capture long-range dependencies due to issues like the vanishing gradient problem and inhibits parallelization during training, leading to increased computational time.\n",
    "\n",
    "Transformers address these limitations by processing entire sequences simultaneously, allowing them to model relationships between all tokens in a sequence regardless of their positions. This parallel processing capability not only improves computational efficiency but also enhances the model's ability to understand complex dependencies within the data.\n",
    "\n",
    "## The Attention Mechanism: Focusing on What Matters\n",
    "\n",
    "At the core of the transformer architecture is the **attention mechanism**. Attention allows the model to weigh the relevance of different parts of the input data when generating each part of the output. This mechanism enables the model to focus on the most pertinent elements, enhancing its ability to capture context and relationships between tokens.\n",
    "\n",
    "For instance, consider machine translation from English to French. When translating the English pronoun \"it,\" the correct French translation (*\"il\"* or *\"elle\"*) depends on the grammatical gender of the noun it refers to. The attention mechanism helps the model focus on the relevant noun, ensuring correct gender agreement in the translation.\n",
    "\n",
    "## Anatomy of a Transformer\n",
    "\n",
    "A typical transformer model consists of two main components:\n",
    "\n",
    "- **Encoder:** Processes the input sequence and generates a contextualized representation for each token.\n",
    "- **Decoder:** Takes the encoder's output and generates the output sequence, one token at a time, while attending to both the encoder's output and the previously generated tokens.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/transformers_basic.png\" alt=\"\" style=\"width: 40%; height: 40%\"/>\n",
    "</p>\n",
    "\n",
    "Each of these components is composed of multiple layers that include critical sub-components:\n",
    "\n",
    "### 1. Self-Attention Mechanism\n",
    "\n",
    "The **self-attention** mechanism allows the model to consider the relationship between a token and all other tokens in the sequence. This is crucial for understanding context, as the meaning of a word often depends on the surrounding words.\n",
    "\n",
    "#### How Self-Attention Works\n",
    "\n",
    "1. **Input Transformations:**\n",
    "\n",
    "   - For each token in the sequence, the model computes three vectors:\n",
    "\n",
    "     - **Query (Q) Vector**\n",
    "     - **Key (K) Vector**\n",
    "     - **Value (V) Vector**\n",
    "\n",
    "     These vectors are linear transformations of the input embeddings, capturing different aspects of the token's representation.\n",
    "\n",
    "2. **Calculating Attention Scores:**\n",
    "\n",
    "   - The attention score between two tokens is computed by taking the dot product of their Query and Key vectors:\n",
    "\n",
    "     $$\n",
    "     \\text{Attention Score} = Q \\cdot K^T\n",
    "     $$\n",
    "\n",
    "     This score reflects how much attention the model should pay to one token when processing another.\n",
    "\n",
    "3. **Scaling and Normalization:**\n",
    "\n",
    "   - The attention scores are scaled by dividing by the square root of the dimensionality of the Key vectors to mitigate issues with large dot product values:\n",
    "\n",
    "     $$\n",
    "     \\text{Scaled Attention Score} = \\frac{Q \\cdot K^T}{\\sqrt{d_k}}\n",
    "     $$\n",
    "\n",
    "   - A softmax function is then applied to obtain normalized attention weights that sum to one.\n",
    "\n",
    "4. **Weighted Sum of Values:**\n",
    "\n",
    "   - The attention weights are used to compute a weighted sum of the Value vectors, producing an output vector that captures aggregated information from the entire sequence.\n",
    "\n",
    "This process allows the model to dynamically weight the influence of each token based on its relevance to others, effectively capturing context.\n",
    "\n",
    "#### Multi-Head Attention\n",
    "\n",
    "To enhance the model's ability to capture different types of relationships, transformers employ **multi-head attention**. This involves running multiple self-attention mechanisms, or \"heads,\" in parallel. Each head learns to focus on different aspects or positions in the sequence.\n",
    "\n",
    "- The outputs from all heads are concatenated and linearly transformed to form the final output of the self-attention layer.\n",
    "\n",
    "> **Note:** Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions, enriching the model's capacity to understand complex patterns.\n",
    "\n",
    "### 2. Position-Wise Feed-Forward Networks\n",
    "\n",
    "After the self-attention layer, each position in the sequence is passed through a fully connected feed-forward network (FFN). This network consists of two linear transformations with a non-linear activation function (usually ReLU) in between.\n",
    "\n",
    "- **Operations:**\n",
    "\n",
    "  $$\n",
    "  \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
    "  $$\n",
    "\n",
    "- **Characteristics:**\n",
    "\n",
    "  - The same feed-forward network is applied independently to each position.\n",
    "  - Adds depth and non-linearity to the model, enabling it to learn more complex functions.\n",
    "\n",
    "### 3. Positional Encoding: Injecting Order into the Model\n",
    "\n",
    "Since transformers process all tokens simultaneously without built-in regard to their position, they need a method to incorporate information about the order of the sequence. **Positional encoding** achieves this by adding a unique positional vector to each token's embedding.\n",
    "\n",
    "#### Implementation of Positional Encoding\n",
    "\n",
    "A common approach uses sine and cosine functions of different frequencies:\n",
    "\n",
    "- For position $ pos $ and dimension $ i $:\n",
    "\n",
    "  $$\n",
    "  \\text{PE}_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "  $$\n",
    "  $$\n",
    "  \\text{PE}_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "  $$\n",
    "\n",
    "- **Advantages:**\n",
    "\n",
    "  - Provides the model with a sense of relative positioning.\n",
    "  - Enables the model to generalize to sequences longer than those seen during training.\n",
    "  - The periodic nature allows for easy computation of relative positions.\n",
    "\n",
    "> **Example Analogy:** Think of positional encoding as adding a unique rhythm to each word based on its position, allowing the model to distinguish between \"the cat sat on the mat\" and \"on the mat sat the cat,\" despite having the same words.\n",
    "\n",
    "### 4. Residual Connections and Layer Normalization\n",
    "\n",
    "To facilitate training deeper models, transformers use **residual connections** around the sub-layers, followed by **layer normalization**.\n",
    "\n",
    "- **Residual Connections:**\n",
    "\n",
    "  - Add the input of a sub-layer to its output:\n",
    "\n",
    "    $$\n",
    "    \\text{Output} = \\text{LayerNorm}(x + \\text{SubLayer}(x))\n",
    "    $$\n",
    "\n",
    "  - Helps in mitigating the vanishing gradient problem by allowing gradients to flow directly through the network.\n",
    "\n",
    "- **Layer Normalization:**\n",
    "\n",
    "  - Normalizes the output across the features for each position, stabilizing and speeding up training.\n",
    "\n",
    "## Advantages of Transformers\n",
    "\n",
    "- **Parallelization:** By processing entire sequences at once, transformers make efficient use of modern hardware, reducing training times significantly compared to sequential models.\n",
    "\n",
    "- **Handling Long-Range Dependencies:** The attention mechanism allows direct connections between any two tokens in a sequence, regardless of their distance, effectively capturing long-range dependencies.\n",
    "\n",
    "- **Flexibility in Modeling Relationships:** Multi-head attention enables the model to learn nuanced relationships from different subspaces.\n",
    "\n",
    "## FAQ\n",
    "\n",
    "- **Do Transformers Ignore Sequence Order?**\n",
    "\n",
    "  - **No.** While transformers process tokens in parallel, positional encoding ensures that the model is aware of the order of tokens, allowing it to capture sequential information.\n",
    "\n",
    "- **Are Transformers Only for NLP?**\n",
    "\n",
    "  - **Not anymore.** While initially developed for language tasks, the transformer architecture has been adapted for various domains, including computer vision (e.g., Vision Transformers) and reinforcement learning.\n",
    "\n",
    "- **Is Attention All You Need?**\n",
    "\n",
    "  - The catchphrase from the original paper suggests the power of attention mechanisms, but transformers also rely on other components like feed-forward networks, residual connections, and proper regularization to perform effectively.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Complexity in Transformers\n",
    "\n",
    "Transformers have dramatically advanced the field of natural language processing (NLP), achieving state-of-the-art performance in a variety of tasks. A key innovation in Transformers is the self-attention mechanism, which allows the model to capture contextual relationships between all elements in an input sequence. However, despite their successes, Transformers face a significant limitation: when processing long sequences, their computational complexity becomes prohibitive due to its quadratic nature with respect to the sequence length.\n",
    "\n",
    "### Understanding the Quadratic Complexity\n",
    "\n",
    "The computational complexity of the self-attention mechanism in Transformers is **O(n²)**, where *n* is the length of the input sequence. This quadratic complexity arises because, in the self-attention layer, each token (word or sub-word unit) attends to every other token in the sequence. Specifically, for each token, the model computes attention scores (dot products) with all other tokens to determine the relevance of each other token to itself.\n",
    "\n",
    "As a result, for a sequence of length *n*, the number of attention score computations is *n* × *n* = *n²*. This is illustrated in the following figure:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"images/transformer_quadratic.webp\" alt=\"Quadratic complexity in self-attention\" style=\"width: 50%; height: 50%\"/>\n",
    "</p>\n",
    "\n",
    "*Figure: For a sequence of length 9, the self-attention mechanism results in 9² = 81 computations.*\n",
    "\n",
    "This quadratic scaling affects both:\n",
    "\n",
    "- **Computational Time**: As the sequence length *n* increases, the time required to compute all attention scores grows quadratically, leading to longer processing times.\n",
    "  \n",
    "- **Memory Usage**: The model must store the *n × n* attention matrix, which holds all the attention scores between tokens. This storage requirement can quickly exceed available memory for large *n*.\n",
    "\n",
    "An analogy to understand this complexity is to consider a meeting where every person needs to directly communicate with every other person. In small meetings, this is manageable, but as the number of people increases, the number of direct interactions grows rapidly, making the meeting chaotic and unmanageable.\n",
    "\n",
    "#### Mathematical Explanation\n",
    "\n",
    "The self-attention operation involves computing a matrix of attention scores, **A**, where each element **A<sub>i,j</sub>** represents the attention from token *i* to token *j*. Computing **A** requires **O(n²)** operations, and storing **A** requires **O(n²)** memory.\n",
    "\n",
    "### Impact on Real-World Applications\n",
    "\n",
    "The quadratic complexity imposes practical limitations when applying Transformers to tasks involving long sequences. Some real-world applications affected include:\n",
    "\n",
    "1. **Question Answering**\n",
    "\n",
    "   In question answering, especially open-domain or extractive tasks, the model needs to consider a lengthy context to find the correct answer within large documents or multiple passages. Processing these long contexts with standard Transformers is computationally intensive.\n",
    "\n",
    "2. **Machine Translation**\n",
    "\n",
    "   Translating long sentences or entire documents requires the model to capture dependencies across the entire text. The quadratic complexity makes it challenging to efficiently process long input or output sequences, potentially limiting the translation of lengthy texts.\n",
    "\n",
    "3. **Summarization**\n",
    "\n",
    "   Summarizing long articles or reports requires understanding and condensing information from extensive input. The computational demands of processing long sequences can impede the use of Transformers for such tasks.\n",
    "\n",
    "4. **Language Modeling**\n",
    "\n",
    "   In language modeling and generation tasks, the model often needs to consider long-range dependencies to generate coherent text. The quadratic scaling limits the feasible context length that the model can handle.\n",
    "\n",
    "5. **Text Classification**\n",
    "\n",
    "   Classifying documents like legal briefs or research articles requires processing the entire text to capture nuanced information that determines the correct class. The memory and computational constraints can hinder model performance on these tasks.\n",
    "\n",
    "In all these scenarios, the quadratic complexity leads to:\n",
    "\n",
    "- **Higher Computational Costs**: Training and inference times increase significantly with sequence length.\n",
    "  \n",
    "- **Increased Memory Consumption**: Memory requirements may exceed hardware limitations, leading to the need for model adjustments or specialized hardware.\n",
    "\n",
    "These limitations constrain the applicability of Transformers in processing long texts, which is a significant drawback given the importance of understanding long-range dependencies in many NLP tasks.\n",
    "\n",
    "### Addressing the Quadratic Complexity\n",
    "\n",
    "To mitigate the challenges posed by the quadratic complexity in Transformers, researchers have proposed several approaches:\n",
    "\n",
    "1. **Sparse Attention Mechanisms**\n",
    "\n",
    "   Sparse attention mechanisms limit the number of tokens each token attends to, reducing the computational burden. Rather than computing attention scores with all tokens, each token attends to a subset of relevant tokens.\n",
    "\n",
    "   Examples include:\n",
    "\n",
    "   - **Local Attention**: Tokens attend only to a fixed window of neighboring tokens. This approach assumes that relevant information is often located nearby in the sequence.\n",
    "   \n",
    "   - **Strided Attention**: Tokens attend to a subset of tokens at regular intervals, capturing long-range dependencies without exhaustive computations.\n",
    "   \n",
    "   - **Adaptive Sparsity**: The model dynamically selects which tokens to attend to based on learned importance scores.\n",
    "\n",
    "   By reducing the number of attention calculations, sparse attention mechanisms lower both computation and memory requirements from **O(n²)** to **O(n)** or **O(n log n)**, depending on the method.\n",
    "\n",
    "2. **Efficient Attention Algorithms**\n",
    "\n",
    "   Several algorithms have been developed to compute attention more efficiently:\n",
    "\n",
    "   - **Linformer**: Projects the sequence into a lower-dimensional space, reducing the attention matrix size.\n",
    "   \n",
    "   - **Reformer**: Uses locality-sensitive hashing (LSH) to approximate attention computations, focusing on similar tokens.\n",
    "   \n",
    "   - **Performer**: Employs random feature methods to approximate the softmax function in attention, achieving linear complexity.\n",
    "\n",
    "3. **Segmented or Chunked Processing**\n",
    "\n",
    "   The sequence is divided into smaller segments or chunks processed separately. While this reduces computational load, it may limit the model's ability to capture dependencies across segments. Techniques like windowed attention combined with cross-chunk connections aim to mitigate this issue.\n",
    "\n",
    "4. **Memory-Augmented Models**\n",
    "\n",
    "   Models incorporate external memory structures to store and retrieve information without processing the entire sequence at each step. This allows the model to access important information from previous tokens without the need for full self-attention over long sequences.\n",
    "\n",
    "5. **Long Range Arena (LRA) Benchmark**\n",
    "\n",
    "   The **Long Range Arena** is a benchmark designed to evaluate models' ability to handle long-context sequences efficiently. It includes tasks that require capturing long-range dependencies, providing a standardized way to assess and compare the performance of various efficient Transformer models.\n",
    "\n",
    "   Researchers use LRA to test new architectures and attention mechanisms, pushing the development of models that can process longer sequences without prohibitive computational costs.\n",
    "\n",
    "### Ongoing Research and Future Directions\n",
    "\n",
    "The challenge of quadratic complexity in Transformers is an active area of research. Key goals include:\n",
    "\n",
    "- **Scaling to Longer Sequences**: Developing models that can handle sequences of tens of thousands of tokens efficiently, enabling applications like full-document understanding.\n",
    "\n",
    "- **Balancing Efficiency and Performance**: Designing attention mechanisms that reduce computational demands while maintaining or improving model accuracy.\n",
    "\n",
    "- **Hardware Optimizations**: Leveraging specialized hardware, such as GPUs and TPUs, and optimization techniques to improve computational efficiency.\n",
    "\n",
    "- **Theoretical Advances**: Understanding the fundamental limits of attention mechanisms and exploring alternative architectures that capture long-range dependencies with lower complexity.\n",
    "\n",
    "> \n",
    "> **Note**: While quadratic complexity poses challenges, Transformers continue to be highly effective in capturing contextual information in sequences. The development of more efficient variants and attention mechanisms is crucial for expanding their applicability to tasks involving longer texts. As research progresses, we can expect Transformers to become more capable of handling long sequences efficiently, enhancing their impact in natural language processing and beyond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Transformers Beyond NLP\n",
    "\n",
    "Transformers, initially introduced for Natural Language Processing (NLP) tasks, have revolutionized how we model sequential data. Their core innovation, the self-attention mechanism, allows the modeling of complex patterns and long-range dependencies without relying on recurrence. This capability extends beyond text, enabling Transformers to excel in various domains. In this section, we explore how Transformers have been adapted for tasks in computer vision, music generation, speech recognition, and video processing.\n",
    "\n",
    "### Computer Vision\n",
    "\n",
    "Understanding images requires capturing both local and global patterns. Traditional Convolutional Neural Networks (CNNs) are effective at modeling local features through convolutions but can struggle with global dependencies due to limited receptive fields.\n",
    "\n",
    "#### Vision Transformer (ViT)\n",
    "\n",
    "The **Vision Transformer (ViT)** adapts the Transformer architecture for image classification. Instead of using convolutions, ViT divides an image into a sequence of patches, treating each patch as a token:\n",
    "\n",
    "1. **Image Patchification**: An image $ \\mathbf{I} \\in \\mathbb{R}^{H \\times W \\times C} $ is split into $ N $ patches $ \\{ \\mathbf{x}_p^i \\} $, each of size $ P \\times P $, where $ H $ and $ W $ are image dimensions, and $ C $ is the number of channels.\n",
    "\n",
    "2. **Patch Embedding**: Each patch $ \\mathbf{x}_p^i $ is flattened and projected to a latent vector $ \\mathbf{z}_0^i $ using a learnable linear transformation:\n",
    "\n",
    "   $$\n",
    "   \\mathbf{z}_0^i = \\mathbf{E} \\cdot \\mathrm{flatten}(\\mathbf{x}_p^i) + \\mathbf{E}_{pos}^i,\n",
    "   $$\n",
    "\n",
    "   where $ \\mathbf{E} $ is the embedding matrix and $ \\mathbf{E}_{pos}^i $ is the position embedding for patch $ i $.\n",
    "\n",
    "3. **Transformer Encoding**: The sequence $ \\{\\mathbf{z}_0^i\\} $ is input to standard Transformer encoder layers to model relationships between patches.\n",
    "\n",
    "4. **Classification**: A special classification token $ \\mathbf{z}_0^0 $ is prepended to the sequence, whose final representation $ \\mathbf{z}_L^0 $ is used for classification.\n",
    "\n",
    "By using self-attention, ViT captures both local and global dependencies across all patches, enabling effective image recognition without convolutions. ViT has achieved state-of-the-art results on image classification benchmarks, especially when pre-trained on large datasets.\n",
    "\n",
    "#### Image Transformer\n",
    "\n",
    "The **Image Transformer** extends Transformers to image generation and manipulation tasks. It treats image pixels or regions as sequences and models the conditional probability of each pixel given previous ones:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x}) = \\prod_{i=1}^{N} P(x_i \\mid x_1, x_2, \\dots, x_{i-1}),\n",
    "$$\n",
    "\n",
    "where $ x_i $ represents the $ i $-th pixel or region. The self-attention mechanism efficiently captures dependencies across the entire image, allowing for high-quality image synthesis and inpainting.\n",
    "\n",
    "### Music Generation\n",
    "\n",
    "Music is inherently sequential and hierarchical, with long-range dependencies such as repeating melodies and harmonic progressions.\n",
    "\n",
    "#### MuseNet\n",
    "\n",
    "**MuseNet** applies Transformers to music composition by treating musical elements as tokens:\n",
    "\n",
    "- **Tokens**: Notes, durations, instruments, and other musical attributes are encoded as tokens.\n",
    "- **Sequence Modeling**: The model learns the probability distribution over sequences of tokens, capturing musical structure.\n",
    "\n",
    "The self-attention mechanism allows MuseNet to consider the entire context of the composition when generating each note, enabling:\n",
    "\n",
    "- **Polyphony**: Handling multiple simultaneous notes (chords) across different instruments.\n",
    "- **Style Blending**: Combining elements from various musical genres.\n",
    "\n",
    "MuseNet can generate complex compositions up to four minutes long, showcasing the Transformer’s ability to model complex, long-range dependencies in music.\n",
    "\n",
    "### Speech Recognition\n",
    "\n",
    "Speech recognition involves mapping acoustic signals to text, requiring models to capture temporal interactions over variable-length sequences.\n",
    "\n",
    "#### Speech-Transformer\n",
    "\n",
    "The **Speech-Transformer** brings the Transformer architecture to Automatic Speech Recognition (ASR):\n",
    "\n",
    "1. **Input Representation**: The raw audio waveform is converted into a sequence of acoustic feature vectors $ \\{\\mathbf{x}_t\\} $, such as Mel-frequency cepstral coefficients (MFCCs).\n",
    "\n",
    "2. **Positional Encoding**: Since Transformers lack built-in sequence ordering, positional encodings $ \\mathbf{E}_{pos}^t $ are added to the input features:\n",
    "\n",
    "   $$\n",
    "   \\mathbf{z}_0^t = \\mathbf{x}_t + \\mathbf{E}_{pos}^t.\n",
    "   $$\n",
    "\n",
    "3. **Encoder-Decoder Architecture**: The model uses a Transformer encoder to process the input sequence and a decoder to generate the corresponding text.\n",
    "\n",
    "4. **Attention Mechanism**: Self-attention in the encoder captures temporal relationships in speech, while encoder-decoder attention aligns acoustic features with linguistic outputs.\n",
    "\n",
    "The Speech-Transformer simplifies ASR by eliminating the need for components like Hidden Markov Models (HMMs) or Connectionist Temporal Classification (CTC), while achieving competitive performance.\n",
    "\n",
    "### Video Processing\n",
    "\n",
    "Videos present spatiotemporal data, requiring models to capture dependencies both within and across frames.\n",
    "\n",
    "The **Video Transformer** applies Transformers to video understanding tasks by extending self-attention to spatiotemporal tokens:\n",
    "\n",
    "1. **Spatiotemporal Tokenization**:\n",
    "\n",
    "   - **Spatial Patches**: Each frame is divided into patches, similar to ViT.\n",
    "   - **Temporal Segmentation**: Sequences of patches over time are considered.\n",
    "\n",
    "2. **Embedding and Position Encoding**:\n",
    "\n",
    "   $$\n",
    "   \\mathbf{z}_0^{(t,i)} = \\mathbf{E} \\cdot \\mathrm{flatten}(\\mathbf{x}_{p}^{(t,i)}) + \\mathbf{E}_{pos}^{(t,i)},\n",
    "   $$\n",
    "\n",
    "   where $ \\mathbf{x}_{p}^{(t,i)} $ is patch $ i $ at time $ t $.\n",
    "\n",
    "3. **Attention Across Space and Time**: Self-attention computes relationships across all patches and frames:\n",
    "\n",
    "   $$\n",
    "   \\mathrm{Attention(Q,K,V)} = \\mathrm{softmax}\\left( \\frac{QK^\\top}{\\sqrt{d_k}} \\right) V,\n",
    "   $$\n",
    "\n",
    "   where queries $ Q $, keys $ K $, and values $ V $ aggregate information from both spatial and temporal dimensions.\n",
    "\n",
    "This approach allows the model to understand motions, actions, and interactions in videos, making it effective for tasks like action recognition and video captioning.\n",
    "\n",
    "### Other Domains\n",
    "\n",
    "Transformers have been adapted for various other fields:\n",
    "\n",
    "- **Protein Structure Prediction**: Modeling amino acid sequences to predict 3D structures.\n",
    "- **Reinforcement Learning**: Representing policies and value functions as sequences.\n",
    "- **Multi-Modal Processing**: Combining text, vision, and audio data for tasks like image captioning and audiovisual speech recognition.\n",
    "\n",
    "> **Key Insight**: Transformers excel in any domain requiring the modeling of complex, long-range dependencies in structured data. Their flexibility and scalability have made them a cornerstone in advancing machine learning across diverse fields.\n",
    "\n",
    "### FAQ\n",
    "\n",
    "- **Why use Transformers over traditional models in these domains?**\n",
    "  \n",
    "  - Transformers handle long-range dependencies more effectively than models like RNNs or CNNs, especially in scenarios where context from distant parts of the input is crucial.\n",
    "\n",
    "- **How do positional encodings impact performance?**\n",
    "  \n",
    "  - Positional encodings provide sequence order information to the model. Different domains may use absolute or relative positional encodings to better capture the structure of the data.\n",
    "\n",
    "- **Are Transformers computationally intensive?**\n",
    "  \n",
    "  - While self-attention has $ \\mathcal{O}(n^2) $ complexity with respect to sequence length $ n $, techniques like sparse attention and patching reduce computational load.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Transformer Architectures for NLP\n",
    "\n",
    "Transformers have transformed (sorry, pun intended) Natural Language Processing (NLP) with their ability to effectively capture dependencies in sequence data. This has led to the development of several powerful transformer architectures tailored for various NLP tasks. In this section, we will explore five commonly used transformer architectures and discuss their unique characteristics and strengths.\n",
    "\n",
    "### 1. BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "**BERT**, developed by Google, is a pre-trained transformer model that has significantly advanced NLP. Its key innovation lies in its bidirectional approach to language understanding. Unlike traditional models that process text in a unidirectional (left-to-right or right-to-left) manner, BERT considers both the preceding and following context simultaneously. This bidirectional context allows BERT to capture a more nuanced understanding of language semantics, leading to improved performance on a wide range of NLP tasks.\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "BERT's architecture consists of multiple transformer **encoder** layers stacked together. Specifically, BERT employs only the encoder part of the original transformer model, focusing on understanding the input text.\n",
    "\n",
    "#### Pre-training Tasks\n",
    "\n",
    "During pre-training, BERT is trained on large amounts of unlabeled text data using two unsupervised learning tasks:\n",
    "\n",
    "1. **Masked Language Modeling (MLM)**:\n",
    "   \n",
    "   - A certain percentage of input tokens are randomly masked, and the model is tasked with predicting the original tokens based on the surrounding context.\n",
    "   - Formally, given an input sequence $ X = \\{x_1, x_2, \\dots, x_n\\} $, where some tokens are replaced with a special `[MASK]` token, the objective is to minimize the cross-entropy loss:\n",
    "\n",
    "     $$\n",
    "     \\mathcal{L}_{\\text{MLM}} = - \\sum_{i \\in M} \\log P(x_i \\mid X_{\\setminus i}),\n",
    "     $$\n",
    "\n",
    "     where $ M $ is the set of masked positions, and $ X_{\\setminus i} $ represents the input sequence with masked tokens.\n",
    "\n",
    "2. **Next Sentence Prediction (NSP)**:\n",
    "\n",
    "   - The model is trained to predict whether a given pair of sentences $ (A, B) $ are consecutive in the original text.\n",
    "   - The objective is to minimize the binary classification loss:\n",
    "\n",
    "     $$\n",
    "     \\mathcal{L}_{\\text{NSP}} = - \\left[ y \\log P(\\text{IsNext} \\mid A, B) + (1 - y) \\log P(\\text{NotNext} \\mid A, B) \\right],\n",
    "     $$\n",
    "\n",
    "     where $ y = 1 $ if $ B $ follows $ A $ in the original text, and $ y = 0 $ otherwise.\n",
    "\n",
    "These pre-training tasks enable BERT to learn deep bidirectional representations, capturing both syntactic and semantic relationships in language.\n",
    "\n",
    "#### Fine-tuning\n",
    "\n",
    "After pre-training, BERT can be fine-tuned for specific downstream tasks (e.g., question answering, named entity recognition) by adding a simple classification layer on top and training on task-specific data.\n",
    "\n",
    "### 2. GPT (Generative Pre-trained Transformer)\n",
    "\n",
    "**GPT**, introduced by OpenAI, is another influential transformer architecture in NLP. Unlike BERT, GPT adopts a unidirectional approach, processing text from left to right, or in a forward manner. This unidirectional nature makes GPT particularly well-suited for tasks that require text generation, such as language modeling, text completion, and conversational AI.\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "GPT's architecture consists of multiple transformer **decoder** layers stacked together. It uses only the decoder part of the original transformer model and includes masked self-attention to prevent the model from seeing future tokens during training.\n",
    "\n",
    "#### Pre-training Objective\n",
    "\n",
    "GPT is pre-trained using a standard language modeling objective:\n",
    "\n",
    "- **Language Modeling (LM)**:\n",
    "\n",
    "  - The model learns to predict the next token $ x_{t} $ given the previous tokens $ x_{1:t-1} $.\n",
    "  - The objective is to minimize the negative log-likelihood:\n",
    "\n",
    "    $$\n",
    "    \\mathcal{L}_{\\text{LM}} = - \\sum_{t=1}^{n} \\log P(x_t \\mid x_{1:t-1}).\n",
    "    $$\n",
    "\n",
    "This training enables GPT to capture the statistical properties of language, including syntax and semantics.\n",
    "\n",
    "#### Strengths\n",
    "\n",
    "- **Text Generation**: GPT excels at generating coherent and fluent text that closely resembles human writing.\n",
    "- **Few-Shot Learning**: By conditioning on a prompt or a few examples, GPT can perform tasks without task-specific fine-tuning, demonstrating emergent few-shot capabilities.\n",
    "  \n",
    "#### Limitations\n",
    "\n",
    "- GPT relies on left-to-right context, which may limit its understanding of information that appears later in the text.\n",
    "\n",
    "### 3. RoBERTa (A Robustly Optimized BERT Pretraining Approach)\n",
    "\n",
    "**RoBERTa**, developed by Facebook AI, is a variant of BERT that aims to improve upon the original by optimizing the training procedure.\n",
    "\n",
    "#### Key Modifications\n",
    "\n",
    "1. **Dynamic Masking**:\n",
    "\n",
    "   - Instead of using a fixed mask for each training instance, RoBERTa applies masking dynamically during training. Each epoch sees a new masking pattern.\n",
    "   - This exposes the model to more varied contexts, enhancing generalization.\n",
    "\n",
    "2. **Removal of NSP Task**:\n",
    "\n",
    "   - RoBERTa eliminates the Next Sentence Prediction task, as studies showed it did not benefit performance significantly.\n",
    "\n",
    "3. **Training with Larger Batches and More Data**:\n",
    "\n",
    "   - RoBERTa is trained on a larger corpus and with bigger batch sizes, improving the robustness of the learned representations.\n",
    "\n",
    "4. **Longer Training Duration**:\n",
    "\n",
    "   - Extended training allows the model to converge to better optima.\n",
    "\n",
    "#### Impact\n",
    "\n",
    "These enhancements result in RoBERTa achieving state-of-the-art results on various NLP benchmarks, often outperforming BERT. This highlights the importance of training strategies and hyperparameter optimization in transformer models.\n",
    "\n",
    "### 4. T5 (Text-to-Text Transfer Transformer)\n",
    "\n",
    "**T5**, introduced by Google, presents a unified framework by framing every NLP task as a text-to-text problem. This means both the input and output are text strings.\n",
    "\n",
    "#### Unified Text-to-Text Framework\n",
    "\n",
    "- **Task Formulation**:\n",
    "\n",
    "  - All tasks are converted into text-to-text formats.\n",
    "  - For example:\n",
    "    - **Translation**: Input: `\"translate English to Portuguese: That is good.\"`, Output: `\"Isso é bom.\"`\n",
    "    - **Summarization**: Input: `\"summarize: O rato branco roedor roeu a roupa do rei de Roma`, Output: `\"O rato roeu a roupa do rei de Roma.\"`\n",
    "    - **Sentiment Analysis**: Input: `\"sst2 sentence: Essa aula é maravilhosa\"`, Output: `\"positivo\"`\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "- T5 uses the standard encoder-decoder Transformer architecture.\n",
    "- Both the encoder and decoder are composed of several transformer layers.\n",
    "\n",
    "#### Pre-training Objective\n",
    "\n",
    "- **Span Corruption (Denoising Objective)**:\n",
    "\n",
    "  - Random spans of text are replaced with a single mask token.\n",
    "  - The model is tasked to reconstruct the original text.\n",
    "  - This generalizes the MLM task by masking contiguous spans rather than individual tokens.\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- **Flexibility**: The text-to-text approach allows T5 to be applied universally across tasks without changing the architecture.\n",
    "- **Transfer Learning**: Pre-training on multiple tasks improves performance on individual tasks due to shared representations.\n",
    "\n",
    "### 5. XLNet\n",
    "\n",
    "**XLNet**, jointly developed by Google Brain and Carnegie Mellon University, combines the strengths of BERT and auto-regressive models like GPT.\n",
    "\n",
    "#### Key Innovations\n",
    "\n",
    "1. **Permutation Language Modeling (PLM)**:\n",
    "\n",
    "   - XLNet uses a novel training objective where all possible permutations of the input sequence are considered, modeling bidirectional context while maintaining the auto-regressive property.\n",
    "   - For a sequence $ X = \\{x_1, x_2, \\dots, x_n\\} $, the model maximizes the likelihood over all possible permutations $ \\mathcal{Z}_n $:\n",
    "\n",
    "     $$\n",
    "     \\mathcal{L}_{\\text{PLM}} = \\sum_{z \\in \\mathcal{Z}_n} \\log P(x_{z_t} \\mid X_{< z_t}),\n",
    "     $$\n",
    "\n",
    "     where $ z $ is a permutation of indices, and $ X_{< z_t} $ are the tokens preceding $ x_{z_t} $ in the permutation.\n",
    "\n",
    "2. **Two-Stream Self-Attention**:\n",
    "\n",
    "   - Introduces a content stream and a query stream to handle the dependency on target words during training.\n",
    "\n",
    "#### Benefits\n",
    "\n",
    "- **Bidirectional Context**: Captures context from both past and future tokens.\n",
    "- **Auto-regressive Property**: Retains the benefits of autoregressive modeling, improving the ability to generate coherent sequences.\n",
    "\n",
    "#### Performance\n",
    "\n",
    "XLNet has demonstrated improved performance over BERT on several NLP benchmarks, particularly in tasks that benefit from modeling long-range dependencies.\n",
    "\n",
    "\n",
    "### Model Sizes: Base vs. Large\n",
    "\n",
    "Transformer architectures often come in different sizes, commonly referred to as **Base** and **Large**, indicating the model's depth and number of parameters.\n",
    "\n",
    "#### Base Models\n",
    "\n",
    "- **Architecture**:\n",
    "\n",
    "  - Moderate depth, e.g., 12 layers.\n",
    "  - Hidden size (number of units per layer), e.g., 768.\n",
    "  - Number of attention heads, e.g., 12.\n",
    "\n",
    "- **Parameters**:\n",
    "\n",
    "  - Typically around 110 million parameters (e.g., BERT-base).\n",
    "\n",
    "- **Usage**:\n",
    "\n",
    "  - Suitable for tasks with limited computational resources.\n",
    "  - Faster training and inference.\n",
    "\n",
    "#### Large Models\n",
    "\n",
    "- **Architecture**:\n",
    "\n",
    "  - Greater depth, e.g., 24 layers.\n",
    "  - Larger hidden size, e.g., 1024.\n",
    "  - More attention heads, e.g., 16.\n",
    "\n",
    "- **Parameters**:\n",
    "\n",
    "  - Significantly more parameters, e.g., around 340 million (BERT-large).\n",
    "\n",
    "- **Usage**:\n",
    "\n",
    "  - Achieve better performance on complex tasks due to increased capacity.\n",
    "  - Require more computational resources and memory.\n",
    "\n",
    "#### Considerations\n",
    "\n",
    "- **Performance vs. Efficiency Trade-off**:\n",
    "\n",
    "  - Larger models generally perform better due to their higher capacity to capture complex patterns.\n",
    "  - However, they are more computationally intensive and may overfit on small datasets.\n",
    "\n",
    "- **Overfitting Risk**:\n",
    "\n",
    "  - With limited training data, large models can memorize noise, harming generalization.\n",
    "  - Techniques like regularization and data augmentation can mitigate overfitting.\n",
    "\n",
    "- **Practical Approach**:\n",
    "\n",
    "  - Start with a base model to establish a baseline.\n",
    "  - Scale up to a large model if higher performance is needed and resources allow.\n",
    "\n",
    "### FAQ\n",
    "\n",
    "- **Why are different pre-training tasks important?**\n",
    "\n",
    "  - Different pre-training tasks help the model learn varied aspects of language:\n",
    "    - **MLM** captures contextual relationships.\n",
    "    - **NSP** helps with sentence-level understanding.\n",
    "    - **PLM** allows modeling of bidirectional context without masking tokens.\n",
    "\n",
    "- **What is the impact of bidirectionality in models like BERT and XLNet?**\n",
    "\n",
    "  - Bidirectional models can use both past and future context, leading to better understanding of language semantics.\n",
    "  - This is particularly beneficial for tasks requiring comprehension of the entire context.\n",
    "\n",
    "- **How does the choice of model size affect performance?**\n",
    "\n",
    "  - Larger models can capture more complex patterns but require more data and computational power.\n",
    "  - Smaller models are faster and require fewer resources but may underperform on complex tasks.\n",
    "\n",
    "- **Why is T5's text-to-text framework advantageous?**\n",
    "\n",
    "  - It provides a unified approach to various tasks, enabling the same architecture to be applied universally.\n",
    "  - Simplifies the learning process and reduces the need for task-specific adjustments.\n",
    "\n",
    "> **Note:** Understanding the nuances of these transformer architectures empowers practitioners to select appropriate models for their specific NLP tasks. As the field continues to develop, staying informed about these advancements is crucial for leveraging the full potential of transformer-based models in natural language understanding and generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Transformers as Feature Extractors\n",
    "\n",
    "Transformers have revolutionized the field of Natural Language Processing (NLP) by providing a powerful tool for extracting rich syntactical and contextual information from text data. These extracted features can be leveraged for a wide range of downstream tasks, such as classification, regression, and more. In this section, we will explore the concept of features and how transformers can be effectively applied as feature extractors.\n",
    "\n",
    "### Understanding Features and Feature Extraction\n",
    "\n",
    "Features are the distinctive properties or characteristics that are extracted from a dataset to capture its essential information. In the context of NLP, features can represent various aspects of text data, such as word frequencies, sentence structure, or semantic relationships. These features serve as the foundation for building accurate and insightful models for various tasks.\n",
    "\n",
    "Feature extractors are sophisticated algorithms designed to automatically derive meaningful features from raw data, regardless of its modality (e.g., text, images, or audio). They are capable of identifying and capturing relevant patterns, structures, and relationships within the data, enabling more effective analysis and prediction.\n",
    "\n",
    "### Transformers as Feature Extractors\n",
    "\n",
    "Transformers are a class of neural networks that have proven to be exceptionally well-suited for extracting features from textual data. At their central, transformers operate by taking a sequence of words as input and generating a numeric vector representation that encapsulates the semantic meaning of the entire text.\n",
    "\n",
    "When using transformers as feature extractors, the process involves feeding a word sequence into the transformer model and obtaining a dense numeric vector that captures the essential semantic information of the input text. This vector representation can then be used as input to other machine learning algorithms or neural networks, enabling them to perform various prediction and analysis tasks effectively.\n",
    "\n",
    "### Benefits and Limitations of Transformers as Feature Extractors\n",
    "\n",
    "One of the key advantages of using transformers as feature extractors is their ability to handle textual data effectively, making them particularly useful for tasks such as text classification or regression. Transformers also excel at unsupervised learning, meaning they can extract meaningful features from unlabeled text data, reducing the reliance on labeled datasets.\n",
    "\n",
    "Also, transformers have the capacity to capture complex semantic relationships and contextual information within the text, enabling them to generate rich and informative feature representations. This ability to capture elaborate patterns and dependencies makes transformers a powerful tool for various NLP applications.\n",
    "\n",
    "However, it is important to note that transformers also have some limitations. They are computationally intensive and require substantial amounts of data for training. Additionally, transformers have a significant memory footprint due to the need to store the weights of the neural network. This can make them challenging to deploy on resource-constrained devices such as mobile phones or embedded systems with limited memory.\n",
    "\n",
    "<br>\n",
    "\n",
    "> While transformers have their limitations in terms of computational complexity and memory requirements, their benefits in terms of feature extraction and unsupervised learning make them an indispensable tool in the NLP toolkit. As research in this area continues to advance, we can expect to see further improvements and innovations in the use of transformers as feature extractors, pushing the boundaries of what is possible in natural language understanding and processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Steps for Using Transformers\n",
    "\n",
    "To use transformers on a specific task, we need to follow these steps:\n",
    "\n",
    "### Step 1: Start with a Pretrained Model\n",
    "\n",
    "The first step is to select a pretrained model from the [Hugging Face Transformers library](https://huggingface.co/transformers/pretrained_models.html). These pretrained models have been trained on large amounts of text data and have learned general language representations. Using a pretrained model provides a strong foundation for your specific task.\n",
    "\n",
    "*Note:* Training a model from scratch is an advanced topic and rarely necessary. In most cases, you can warm-start your model from a pretrained model. If you're interested in learning more about training your own model from scratch, refer to [this resource](https://huggingface.co/blog/how-to-train).\n",
    "\n",
    "### Step 2: Fine-tune the Model on Domain-Specific Text (Optional)\n",
    "\n",
    "Fine-tuning involves adapting a pretrained model to a new domain by training it on domain-specific text. This step is optional but can enhance the model's performance on your specific task. By exposing the model to text that is similar to your target domain, it can learn domain-specific language patterns and representations.\n",
    "\n",
    "### Step 3: Train the Model for Your Task\n",
    "\n",
    "Once you have a fine-tuned model (or a pretrained model if you skipped step 2), you can train it for your specific task. This typically involves adding a classification or regression head on top of the model and training it using your task-specific data.\n",
    "\n",
    "Alternatively, you can use the model as a feature extractor for your task, which is a more advanced approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Classifying Court Decision Labels\n",
    "\n",
    "Let's explore these steps using a subset of the [BrCAD-5](https://www.kaggle.com/datasets/eliasjacob/brcad5) dataset, which contains over 765,000 legal case information from Brazilian Federal Courts. Our goal is to train a model to predict the label for a court decision based on its text.\n",
    "\n",
    "1. **Select a Pretrained Model**: We'll choose a suitable pretrained model from the Hugging Face Transformers library that aligns with our task requirements, such as language support and model architecture.\n",
    "\n",
    "2. **Fine-tune the Model (Optional)**: If we have a sufficient amount of domain-specific text (legal case information in this case), we can fine-tune the pretrained model on this data to capture domain-specific language patterns.\n",
    "\n",
    "3. **Train the Model for Label Prediction**: We'll add a classification head on top of the model and train it using the labeled court decision data from BrCAD-5. The model will learn to predict the appropriate label based on the text of the court decision.\n",
    "\n",
    "<br>\n",
    "\n",
    "> Remember, the key is to start with a strong pretrained model and adapt it to your specific task through fine-tuning and task-specific training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 11:21:29.159338: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733840489.180297 4187509 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733840489.186643 4187509 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-10 11:21:29.207685: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at neuralmind/bert-large-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Define the model checkpoints for the base and large versions of the BERT model\n",
    "model_checkpoint_base = \"neuralmind/bert-base-portuguese-cased\"\n",
    "model_checkpoint_large = \"neuralmind/bert-large-portuguese-cased\"\n",
    "\n",
    "# Load the tokenizer for the base BERT model\n",
    "# The tokenizer is responsible for converting text into tokens that the model can understand\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(model_checkpoint_base)\n",
    "\n",
    "# Load the masked language model (MLM) for the base BERT model\n",
    "# The MLM is used for tasks like predicting masked words in a sentence\n",
    "model_mlm_base = AutoModelForMaskedLM.from_pretrained(model_checkpoint_base)\n",
    "\n",
    "# Load the tokenizer for the large BERT model\n",
    "# This tokenizer works similarly to the base tokenizer but is tailored for the large model\n",
    "tokenizer_large = AutoTokenizer.from_pretrained(model_checkpoint_large)\n",
    "\n",
    "# Load the masked language model (MLM) for the large BERT model\n",
    "# This MLM is used for tasks like predicting masked words in a sentence, similar to the base model but with more parameters\n",
    "model_mlm_large = AutoModelForMaskedLM.from_pretrained(model_checkpoint_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_base.is_fast  # A fast tokenizer from HF Transformers uses Rust under the hood for faster tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_large.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 108,954,466 trainable parameters\n",
      "The model has 334,428,258 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Define a function to count the number of trainable parameters in a model\n",
    "def count_parameters(model):\n",
    "    # Sum the number of elements (numel) for each parameter in the model\n",
    "    # Only include parameters that require gradients (i.e., are trainable)\n",
    "    n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    # Print the number of trainable parameters in a human-readable format with commas\n",
    "    print(f\"The model has {n_parameters:,} trainable parameters\")\n",
    "\n",
    "\n",
    "# Count and print the number of trainable parameters for the base BERT model\n",
    "count_parameters(model_mlm_base)\n",
    "\n",
    "# Count and print the number of trainable parameters for the large BERT model\n",
    "count_parameters(model_mlm_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29794, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=29794, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlm_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Above, you can see the model architecture summary of a BERT (Bidirectional Encoder Representations from Transformers) model specifically designed for masked language modeling (MLM) tasks. Let's break it down:\n",
    ">\n",
    "> 1. `BertForMaskedLM`: This is the main class that represents the BERT model for masked language modeling.\n",
    ">\n",
    "> 2. `BertModel`: This is the fundamental BERT model that consists of the following components:\n",
    ">       - `BertEmbeddings`: This module handles the input embeddings, including word embeddings, position embeddings, and token type embeddings. It also applies layer normalization and dropout.\n",
    ">       - `BertEncoder`: This is the main encoder component of BERT, which consists of a stack of `BertLayer` modules.\n",
    ">       - `BertLayer`: Each layer in the encoder consists of a self-attention mechanism (`BertAttention`), an intermediate feed-forward network (`BertIntermediate`), and an output projection (`BertOutput`).\n",
    ">       - `BertAttention`: This module performs self-attention on the input representations using query, key, and value linear transformations, followed by dropout.\n",
    ">       - `BertIntermediate`: This is a feed-forward network with a GELU activation function.\n",
    ">       - `BertOutput`: This module applies a dense linear transformation, layer normalization, and dropout to the output of the intermediate layer.\n",
    ">\n",
    "> 3. `BertOnlyMLMHead`: This module is specific to the masked language modeling task and consists of the following components:\n",
    ">       - `BertLMPredictionHead`: This module performs the final prediction for the masked tokens.\n",
    ">       - `BertPredictionHeadTransform`: This module applies a dense linear transformation, GELU activation, and layer normalization to the output of the BERT encoder.\n",
    ">       - `decoder`: This is a linear layer that maps the transformed representations to the vocabulary size for predicting the masked tokens.\n",
    ">\n",
    "> The model architecture summary provides details about the dimensions of the embeddings, the number of layers in the encoder, and the sizes of the intermediate and output layers.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(29794, 1024, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 1024)\n",
       "      (token_type_embeddings): Embedding(2, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=1024, out_features=29794, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mlm_large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The main differences between the two BERT models are in the model size and architecture:\n",
    ">\n",
    "> 1. Embedding dimensions:\n",
    ">       - In the first model, the word embeddings, position embeddings, and token type embeddings have a dimension of 768.\n",
    ">       - In the second model, these embeddings have a dimension of 1024, indicating a larger embedding size.\n",
    ">\n",
    "> 2. Number of encoder layers:\n",
    ">       - The first model has 12 encoder layers (`(0-11): 12 x BertLayer`).\n",
    ">       - The second model has 24 encoder layers (`(0-23): 24 x BertLayer`)\n",
    ">\n",
    "> 3. Intermediate layer dimensions:\n",
    ">       - In the first model, the intermediate layer (`BertIntermediate`) has an output dimension of 3072.\n",
    ">       - In the second model, the intermediate layer has an output dimension of 4096, which is larger than the first model.\n",
    ">\n",
    "> 4. Hidden state dimensions:\n",
    ">       - The first model uses hidden states with a dimension of 768 throughout the architecture, including the self-attention layers, intermediate layers, and output layers.\n",
    ">       - The second model uses hidden states with a dimension of 1024 throughout the architecture.\n",
    ">\n",
    "> The rest of the architecture, including the self-attention mechanism, layer normalization, dropout, and the MLM head, remains the same between the two models.\n",
    ">\n",
    "> The large model has higher-dimensional embeddings, more encoder layers, and larger intermediate layer dimensions. This suggests that the large model has a higher capacity and can potentially capture more complex patterns and representations from the input data. However, the larger model size also means increased computational requirements and longer training times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and creating a train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((58529, 1), (6504, 1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the unlabeled dataset from a Parquet file\n",
    "# Only the 'text' column is read from the file\n",
    "df_unlabeled = pd.read_parquet(\"data/legal/unlabeled_texts.parquet\", columns=[\"text\"])\n",
    "\n",
    "# Split the unlabeled dataset into training and validation sets\n",
    "# 10% of the data is used for validation, and the split is reproducible with a fixed random state\n",
    "df_unlabeled_train, df_unlabeled_valid = train_test_split(\n",
    "    df_unlabeled, test_size=0.10, random_state=271828\n",
    ")\n",
    "\n",
    "# Display the shapes of the training and validation sets\n",
    "# This shows the number of rows and columns in each set\n",
    "df_unlabeled_train.shape, df_unlabeled_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Convert the pandas DataFrame containing the unlabeled training data into a Hugging Face Dataset\n",
    "# This allows for easier manipulation and integration with Hugging Face's tools and models\n",
    "dataset_unlabeled_train = datasets.Dataset.from_pandas(df_unlabeled_train)\n",
    "\n",
    "# Convert the pandas DataFrame containing the unlabeled validation data into a Hugging Face Dataset\n",
    "# This allows for easier manipulation and integration with Hugging Face's tools and models\n",
    "dataset_unlabeled_valid = datasets.Dataset.from_pandas(df_unlabeled_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', '__index_level_0__'],\n",
       "    num_rows: 58529\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_unlabeled_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', '__index_level_0__'],\n",
       "    num_rows: 6504\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_unlabeled_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define the path to save the outputs of the base BERT masked language model\n",
    "path_to_save_lm_base = Path(\"./outputs/transformers_basics/bert_masked_lm_base\")\n",
    "# Create the directory (and any necessary parent directories) if it doesn't already exist\n",
    "path_to_save_lm_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the path to save the outputs of the large BERT masked language model\n",
    "path_to_save_lm_large = Path(\"./outputs/transformers_basics/bert_masked_lm_large\")\n",
    "# Create the directory (and any necessary parent directories) if it doesn't already exist\n",
    "path_to_save_lm_large.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tune the Language Model on the Domani Text\n",
    "\n",
    "Remember our transfer learning class. During this stage, the general-domain language model adapts itself to the idiosyncrasies of the domain-specific text. This is done by training the model on the domain-specific text. This step is optional, but it can improve the performance of the model on your task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90c524c81594661b603a39a269a9e32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1926b6bfc94b1fa3ad9a9b7c847290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6504 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4bc31b6cc8b4d0d99df0fa5352098c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ba710f16a364e90aa3b5ae466a033b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6504 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from functools import partial\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenizes the input text in the given examples using the tokenizer object.\n",
    "\n",
    "    Args:\n",
    "    - examples: A dictionary containing the input text to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing the tokenized input text.\n",
    "    \"\"\"\n",
    "    result = tokenizer(examples[\"text\"])  # Tokenize the input text\n",
    "    if tokenizer.is_fast:\n",
    "        # If the tokenizer is a fast tokenizer, add word IDs to the result\n",
    "        result[\"word_ids\"] = [\n",
    "            result.word_ids(i) for i in range(len(result[\"input_ids\"]))\n",
    "        ]\n",
    "    return result\n",
    "\n",
    "\n",
    "# Create partial functions for tokenizing using the base and large tokenizers\n",
    "# This allows us to pass the tokenizer as a fixed argument to the tokenize_function\n",
    "tokenize_function_base = partial(tokenize_function, tokenizer=tokenizer_base)\n",
    "tokenize_function_large = partial(tokenize_function, tokenizer=tokenizer_large)\n",
    "\n",
    "# Tokenize the training dataset using the base tokenizer\n",
    "# The map function applies the tokenize_function_base to each example in the dataset\n",
    "# The batched=True argument processes the examples in batches for efficiency\n",
    "# The remove_columns argument removes the specified columns from the dataset after tokenization\n",
    "dataset_train_tokenized_mlm_base = dataset_unlabeled_train.map(\n",
    "    tokenize_function_base, batched=True, remove_columns=[\"text\", \"__index_level_0__\"]\n",
    ")\n",
    "\n",
    "# Tokenize the validation dataset using the base tokenizer\n",
    "dataset_valid_tokenized_mlm_base = dataset_unlabeled_valid.map(\n",
    "    tokenize_function_base, batched=True, remove_columns=[\"text\", \"__index_level_0__\"]\n",
    ")\n",
    "\n",
    "# Tokenize the training dataset using the large tokenizer\n",
    "dataset_train_tokenized_mlm_large = dataset_unlabeled_train.map(\n",
    "    tokenize_function_large, batched=True, remove_columns=[\"text\", \"__index_level_0__\"]\n",
    ")\n",
    "\n",
    "# Tokenize the validation dataset using the large tokenizer\n",
    "dataset_valid_tokenized_mlm_large = dataset_unlabeled_valid.map(\n",
    "    tokenize_function_large, batched=True, remove_columns=[\"text\", \"__index_level_0__\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fba6ed2221949759fa5e7b37c8e470e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ad2d0c2c26443fbb82616d4eae97e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6504 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6895e94fbf4f4a40a4d5c657d6b32ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d963f39c46b243018acb777982939a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6504 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    \"\"\"\n",
    "    This function groups together a set of texts as contiguous text of fixed length (chunk_size).\n",
    "    It's useful for training masked language models.\n",
    "\n",
    "    Args:\n",
    "    - examples: A dictionary containing the examples to group. Each key corresponds to a feature,\n",
    "                and each value is a list of lists of tokens.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary containing the grouped examples. Each key corresponds to a feature,\n",
    "      and each value is a list of lists of tokens.\n",
    "    \"\"\"\n",
    "    # Concatenate all texts for each feature\n",
    "    concatenated_examples = {k: np.concatenate(examples[k]) for k in examples.keys()}\n",
    "\n",
    "    # Compute the total length of the concatenated texts\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "    # Adjust the total length to be a multiple of chunk_size, dropping the last chunk if it's smaller than chunk_size\n",
    "    total_length = (total_length // chunk_size) * chunk_size\n",
    "\n",
    "    # Split the concatenated texts into chunks of size chunk_size using NumPy\n",
    "    result = {\n",
    "        k: np.split(t[:total_length], total_length // chunk_size)\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "\n",
    "    # Create a new 'labels' column that is a copy of the 'input_ids' column\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Define the chunk size for grouping texts\n",
    "chunk_size = 512\n",
    "\n",
    "# Apply the group_texts function to the tokenized training dataset for the base BERT model\n",
    "dataset_train_tokenized_mlm_base = dataset_train_tokenized_mlm_base.map(\n",
    "    group_texts,\n",
    "    batched=True,  # Process the examples in batches for efficiency\n",
    ")\n",
    "\n",
    "# Apply the group_texts function to the tokenized validation dataset for the base BERT model\n",
    "dataset_valid_tokenized_mlm_base = dataset_valid_tokenized_mlm_base.map(\n",
    "    group_texts,\n",
    "    batched=True,  # Process the examples in batches for efficiency\n",
    ")\n",
    "\n",
    "# Apply the group_texts function to the tokenized training dataset for the large BERT model\n",
    "dataset_train_tokenized_mlm_large = dataset_train_tokenized_mlm_large.map(\n",
    "    group_texts,\n",
    "    batched=True,  # Process the examples in batches for efficiency\n",
    ")\n",
    "\n",
    "# Apply the group_texts function to the tokenized validation dataset for the large BERT model\n",
    "dataset_valid_tokenized_mlm_large = dataset_valid_tokenized_mlm_large.map(\n",
    "    group_texts,\n",
    "    batched=True,  # Process the examples in batches for efficiency\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Create a data collator for masked language modeling (MLM) using the base BERT tokenizer\n",
    "# The data collator will dynamically mask tokens in the input text with a probability of 0.15\n",
    "data_collator_mlm_base = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer_base, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# Create a data collator for masked language modeling (MLM) using the large BERT tokenizer\n",
    "# The data collator will dynamically mask tokens in the input text with a probability of 0.15\n",
    "data_collator_mlm_large = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer_large, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define the batch size for training and evaluation using the base BERT model\n",
    "batch_size_base = 20\n",
    "\n",
    "# Extract the model name from the model checkpoint path for the base BERT model\n",
    "model_name_base = model_checkpoint_base.split(\"/\")[-1]\n",
    "\n",
    "# Set up the training arguments for fine-tuning the base BERT model on a masked language modeling task\n",
    "training_args_mlm_base = TrainingArguments(\n",
    "    output_dir=path_to_save_lm_base\n",
    "    / f\"{model_name_base}-finetuned-mlm\",  # Directory to save the model checkpoints\n",
    "    overwrite_output_dir=True,  # Overwrite the output directory if it exists\n",
    "    learning_rate=5e-5,  # Learning rate for the optimizer\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    per_device_train_batch_size=batch_size_base,  # Batch size for training\n",
    "    per_device_eval_batch_size=batch_size_base,  # Batch size for evaluation\n",
    "    bf16=True,  # Use bfloat16 precision (change to \"fp16\" if using a free GPU)\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    save_total_limit=1,  # Limit the total number of saved checkpoints\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "    logging_steps=1,  # Log the training loss after every 1 epoch\n",
    "    eval_steps=1,  # Evaluate the model after every 1 epoch\n",
    "    save_steps=1,  # Save the model after every 1 epoch\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"eval_loss\",  # Metric to use for selecting the best model\n",
    "    greater_is_better=False,  # Lower evaluation loss is better\n",
    "    gradient_accumulation_steps=3,  # Number of gradient accumulation steps\n",
    "    seed=271828,  # Random seed for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4187509/1057577359.py:5: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_mlm_base = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Initialize the Trainer for the base BERT model\n",
    "# The Trainer class provides an easy-to-use API for training and evaluating models\n",
    "trainer_mlm_base = Trainer(\n",
    "    model=model_mlm_base,  # The model to be trained (base BERT masked language model)\n",
    "    args=training_args_mlm_base,  # Training arguments defined earlier\n",
    "    train_dataset=dataset_train_tokenized_mlm_base,  # Tokenized training dataset\n",
    "    eval_dataset=dataset_valid_tokenized_mlm_base,  # Tokenized validation dataset\n",
    "    data_collator=data_collator_mlm_base,  # Data collator for dynamically masking tokens\n",
    "    tokenizer=tokenizer_base,  # Tokenizer for processing the input text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meliasjacob\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/nas-elias/drive/UFRN/Disciplinas/2024-2/deep_learning_gen_ai/wandb/run-20241210_114010-ora8fbnt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/eliasjacob/huggingface/runs/ora8fbnt' target=\"_blank\">outputs/transformers_basics/bert_masked_lm_base/bert-base-portuguese-cased-finetuned-mlm</a></strong> to <a href='https://wandb.ai/eliasjacob/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/eliasjacob/huggingface' target=\"_blank\">https://wandb.ai/eliasjacob/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/eliasjacob/huggingface/runs/ora8fbnt' target=\"_blank\">https://wandb.ai/eliasjacob/huggingface/runs/ora8fbnt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='6786' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  25/6786 00:53 < 4:20:20, 0.43 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6786, training_loss=1.7849110120035188e+29, metrics={'train_runtime': 19426.6794, 'train_samples_per_second': 41.93, 'train_steps_per_second': 0.349, 'total_flos': 2.1433112200101888e+17, 'train_loss': 1.7849110120035188e+29, 'epoch': 2.9994107248084854})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This took around 5 hours to train on 2 x NVIDIA RTX 3090 GPUs\n",
    "trainer_mlm_base.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='746' max='746' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [746/746 04:57]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.38383054733276367,\n",
       " 'eval_runtime': 297.876,\n",
       " 'eval_samples_per_second': 100.119,\n",
       " 'eval_steps_per_second': 2.504,\n",
       " 'epoch': 2.9994107248084854}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model\n",
    "trainer_mlm_base.save_model(path_to_save_lm_base / f\"{model_name_base}-finetuned-mlm\")\n",
    "tokenizer_base.save_pretrained(\n",
    "    path_to_save_lm_base / f\"{model_name_base}-finetuned-mlm\"\n",
    ")\n",
    "\n",
    "trainer_mlm_base.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/transformers_basics/bert_masked_lm_base/bert-base-portuguese-cased-finetuned-mlm\n"
     ]
    }
   ],
   "source": [
    "print(path_to_save_lm_base / f\"{model_name_base}-finetuned-mlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Set the trainer, model, and tokenizer for the base BERT model to None\n",
    "# This helps free up memory by removing references to these objects\n",
    "trainer_mlm_base = None\n",
    "model_mlm_base = None\n",
    "tokenizer_base = None\n",
    "\n",
    "# Force garbage collection to free up memory\n",
    "gc.collect()\n",
    "\n",
    "# Clear the CUDA memory cache to free up GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Define the batch size for training and evaluation\n",
    "batch_size_large = 14\n",
    "\n",
    "# Extract the model name from the model checkpoint path\n",
    "# This will be used to name the output directory for the trained model\n",
    "model_name_large = model_checkpoint_large.split(\"/\")[-1]\n",
    "\n",
    "# Define the training arguments for the large masked language model (MLM)\n",
    "training_args_mlm_large = TrainingArguments(\n",
    "    output_dir=path_to_save_lm_large\n",
    "    / f\"{model_name_large}-finetuned-mlm\",  # Output directory for the trained model\n",
    "    overwrite_output_dir=True,  # Overwrite the output directory if it already exists\n",
    "    learning_rate=5e-5,  # Learning rate for the optimizer\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    per_device_train_batch_size=batch_size_large,  # Batch size for training\n",
    "    per_device_eval_batch_size=batch_size_large,  # Batch size for evaluation\n",
    "    bf16=True,  # Use bf16 precision. Change to \"fp16\" if using a free GPU\n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    save_total_limit=1,  # Limit the total amount of checkpoints and delete the older ones\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model at the end of each epoch\n",
    "    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n",
    "    logging_steps=1,  # Log the training loss after every 1 step\n",
    "    eval_steps=1,  # Evaluate the model after every 1 step\n",
    "    save_steps=1,  # Save the model after every 1 step\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    metric_for_best_model=\"eval_loss\",  # Use the evaluation loss to determine the best model\n",
    "    greater_is_better=False,  # Lower evaluation loss is better\n",
    "    gradient_accumulation_steps=4,  # Number of steps to accumulate gradients before updating the model parameters\n",
    "    seed=271828,  # Random seed for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4187509/2263667952.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_mlm_large = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "# Initialize the Trainer for the large masked language model (MLM)\n",
    "trainer_mlm_large = Trainer(\n",
    "    model=model_mlm_large,  # The pre-trained large BERT model for masked language modeling\n",
    "    args=training_args_mlm_large,  # The training arguments defined earlier for the large model\n",
    "    train_dataset=dataset_train_tokenized_mlm_large,  # The tokenized training dataset for the large model\n",
    "    eval_dataset=dataset_valid_tokenized_mlm_large,  # The tokenized validation dataset for the large model\n",
    "    data_collator=data_collator_mlm_large,  # The data collator for dynamic masking during training\n",
    "    tokenizer=tokenizer_large,  # The tokenizer used to process the input text for the large model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='7272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  10/7272 00:47 < 12:06:10, 0.17 it/s, Epoch 0.00/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7272, training_loss=0.41708476434495584, metrics={'train_runtime': 49389.7092, 'train_samples_per_second': 16.492, 'train_steps_per_second': 0.147, 'total_flos': 7.590524853366497e+17, 'train_loss': 0.41708476434495584, 'epoch': 2.999381315735203})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the large masked language model (MLM)\n",
    "# This process involves multiple epochs of training on the training dataset\n",
    "# Note: This training process took almost 14 hours on 2 x NVIDIA RTX 3090 GPUs\n",
    "trainer_mlm_large.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='1066' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  68/1066 00:44 < 11:00, 1.51 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.30418580770492554,\n",
       " 'eval_runtime': 714.0408,\n",
       " 'eval_samples_per_second': 41.767,\n",
       " 'eval_steps_per_second': 1.493,\n",
       " 'epoch': 2.999381315735203}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained large masked language model (MLM) to the specified directory\n",
    "trainer_mlm_large.save_model(\n",
    "    path_to_save_lm_large / f\"{model_name_large}-finetuned-mlm\"\n",
    ")\n",
    "\n",
    "# Save the tokenizer used for the large MLM to the same directory\n",
    "tokenizer_large.save_pretrained(\n",
    "    path_to_save_lm_large / f\"{model_name_large}-finetuned-mlm\"\n",
    ")\n",
    "\n",
    "# Evaluate the trained large MLM on the validation dataset\n",
    "# This will return a dictionary containing the evaluation metrics\n",
    "trainer_mlm_large.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs/transformers_basics/bert_masked_lm_large/bert-large-portuguese-cased-finetuned-mlm\n"
     ]
    }
   ],
   "source": [
    "print(path_to_save_lm_large / f\"{model_name_large}-finetuned-mlm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Language Models with Perplexity\n",
    "\n",
    "To evaluate the performance of a language model, it's essential to measure how effectively it predicts words in a sentence. One key metric for this purpose is **perplexity**, which provides insight into the model's ability to understand and generate language.\n",
    "\n",
    "### Understanding Perplexity\n",
    "\n",
    "**Perplexity** is a quantitative measure of how well a probability model predicts a sample of data. In the context of language models, it assesses how \"surprised\" the model is when encountering new data. Essentially, perplexity measures the uncertainty of the model in predicting the next word in a sequence.\n",
    "\n",
    "- **Lower Perplexity**: Indicates that the model is less surprised by the test data, suggesting it has a good grasp of the language patterns.\n",
    "- **Higher Perplexity**: Implies that the model is more surprised by the test data, indicating less effective learning.\n",
    "\n",
    "> **Note:** Perplexity can be thought of as the average number of choices the model has when predicting the next word.\n",
    "\n",
    "### Mathematical Definition of Perplexity\n",
    "\n",
    "Perplexity is mathematically defined using entropy or the cross-entropy loss function used during training. For a language model that assigns a probability $ p(w_i) $ to each word $ w_i $ in a test set of size $ N $, perplexity is calculated as:\n",
    "\n",
    "$$ \\text{Perplexity} = e^{\\text{loss}} $$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $ e $ is Euler's number (approximately 2.71828).\n",
    "- $ \\text{loss} $ is the average cross-entropy loss over the test set.\n",
    "\n",
    "Expanding the cross-entropy loss, we have:\n",
    "\n",
    "$$ \\text{loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\log p(w_i) $$\n",
    "\n",
    "Therefore, perplexity becomes:\n",
    "\n",
    "$$ \\text{Perplexity} = e^{ -\\frac{1}{N} \\sum_{i=1}^{N} \\log p(w_i) } $$\n",
    "\n",
    "#### Relationship to Entropy\n",
    "\n",
    "Entropy $ H $ is a measure of the average uncertainty innate in the possible outcomes of a random variable. For a probability distribution $ p $ over a vocabulary $ V $:\n",
    "\n",
    "$$ H(p) = -\\sum_{w \\in V} p(w) \\log p(w) $$\n",
    "\n",
    "Perplexity is then the exponential of the entropy:\n",
    "\n",
    "$$ \\text{Perplexity} = e^{H(p)} $$\n",
    "\n",
    "This formulation shows that perplexity reflects the model's uncertainty: the higher the entropy, the higher the perplexity.\n",
    "\n",
    "### Interpreting Perplexity\n",
    "\n",
    "Perplexity provides an interpretable metric:\n",
    "\n",
    "- If a language model has a perplexity of **10**, it is as uncertain as if it had to choose uniformly among **10** possible words at each step.\n",
    "- Lower perplexity values mean the model's predictions are closer to the actual data distribution.\n",
    "\n",
    "#### Example Analogy\n",
    "\n",
    "Consider a language model trying to predict the next word in a sentence:\n",
    "\n",
    "- **High Perplexity Scenario**: The model considers many plausible words (e.g., it could be any one of 100 words), indicating high uncertainty.\n",
    "- **Low Perplexity Scenario**: The model confidently narrows down the next word to a few options (e.g., between 2 or 3 words), showing low uncertainty.\n",
    "\n",
    "### Choice of Logarithm Base\n",
    "\n",
    "The base of the logarithm used in entropy and perplexity calculations affects the units of measurement:\n",
    "\n",
    "- **Base-2 Logarithm ($ \\log_2 $)**: Measures entropy in **bits**. Common in information theory, reflecting binary decisions.\n",
    "  \n",
    "  $$ H_2(p) = -\\sum_{w \\in V} p(w) \\log_2 p(w) $$\n",
    "\n",
    "- **Natural Logarithm ($ \\ln $ or $ \\log_e $)**: Measures entropy in **nats**, leveraging mathematical properties of $ e $.\n",
    "\n",
    "  $$ H_e(p) = -\\sum_{w \\in V} p(w) \\ln p(w) $$\n",
    "\n",
    "> **Important:** The choice of logarithm base is a scaling factor and does not alter the fundamental interpretation of perplexity. Consistency in the base used is key when comparing models.\n",
    "\n",
    "### Addressing Potential Misconceptions\n",
    "\n",
    "- **Perplexity vs. Accuracy**: A low perplexity does not necessarily translate to higher accuracy in practical applications. Perplexity measures the probability distribution learned by the model, not the correctness of specific predictions.\n",
    "  \n",
    "- **Perplexity Values**: There is no \"ideal\" perplexity value universally applicable. It's context-dependent and should be compared relative to other models or baselines on the same dataset.\n",
    "\n",
    "- **Impact of Vocabulary Size**: A larger vocabulary can lead to higher perplexity simply because there are more possible words to predict. Techniques like subword tokenization can help mitigate this effect.\n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "- **Model Comparison**: When evaluating different language models, perplexity allows for quantifiable comparisons of their predictive capabilities.\n",
    "  \n",
    "- **Training and Evaluation**: Monitoring perplexity during training helps identify overfitting:\n",
    "  - **Decreasing Training Perplexity with Increasing Validation Perplexity**: May indicate overfitting to the training data.\n",
    "  - **Consistent Perplexity Across Datasets**: Suggests the model generalizes well.\n",
    "\n",
    "- **Units Consistency**: Always ensure consistency in the logarithm base and units (bits vs. nats) when reporting perplexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# Set the trainer for the large masked language model (MLM) to None to free up memory\n",
    "trainer_mlm_large = None\n",
    "\n",
    "# Set the large masked language model (MLM) to None to free up memory\n",
    "model_mlm_large = None\n",
    "\n",
    "# Set the tokenizer for the large MLM to None to free up memory\n",
    "tokenizer_large = None\n",
    "\n",
    "# Collect garbage to free up memory\n",
    "gc.collect()\n",
    "\n",
    "# Empty the CUDA cache to free up GPU memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity for the base model is 1.4678966815981307\n",
      "The perplexity for the large model is 1.3555208989189849\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print(f\"The perplexity for the base model is {math.exp(0.38383054733276367)}\")\n",
    "print(f\"The perplexity for the large model is {math.exp(0.30418580770492554)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from transformers import pipeline, AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "# Define the path to save the base masked language model (MLM)\n",
    "# This path points to the directory where the base MLM model will be saved\n",
    "path_to_save_lm_base = Path(\"./outputs/transformers_basics/bert_masked_lm_base\")\n",
    "\n",
    "# Define the path to save the large masked language model (MLM)\n",
    "# This path points to the directory where the large MLM model will be saved\n",
    "path_to_save_lm_large = Path(\"./outputs/transformers_basics/bert_masked_lm_large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "# Load the fine-tuned base masked language model (MLM) from the specified directory\n",
    "# This model is a BERT base model fine-tuned on a Portuguese dataset\n",
    "model_base = AutoModelForMaskedLM.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\"\n",
    ")\n",
    "\n",
    "# Load the tokenizer for the fine-tuned base MLM from the same directory\n",
    "# The tokenizer is used to preprocess the input text for the base model\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\"\n",
    ")\n",
    "\n",
    "# Load the fine-tuned large masked language model (MLM) from the specified directory\n",
    "# This model is a BERT large model fine-tuned on a Portuguese dataset\n",
    "model_large = AutoModelForMaskedLM.from_pretrained(\n",
    "    path_to_save_lm_large / \"bert-large-portuguese-cased-finetuned-mlm\"\n",
    ")\n",
    "\n",
    "# Load the tokenizer for the fine-tuned large MLM from the same directory\n",
    "# The tokenizer is used to preprocess the input text for the large model\n",
    "tokenizer_large = AutoTokenizer.from_pretrained(\n",
    "    path_to_save_lm_large / \"bert-large-portuguese-cased-finetuned-mlm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create a pipeline for the base masked language model (MLM)\n",
    "# The pipeline is used to fill in the masked tokens in the input text\n",
    "# 'fill-mask' specifies the task type for the pipeline\n",
    "# model_base is the fine-tuned base MLM model\n",
    "# tokenizer_base is the tokenizer for the base MLM model\n",
    "# top_k=5 specifies that the top 5 predictions for the masked token will be returned\n",
    "pipe_base = pipeline(\"fill-mask\", model=model_base, tokenizer=tokenizer_base, top_k=5)\n",
    "\n",
    "# Create a pipeline for the large masked language model (MLM)\n",
    "pipe_large = pipeline(\n",
    "    \"fill-mask\", model=model_large, tokenizer=tokenizer_large, top_k=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9297154545783997,\n",
       "  'token': 131,\n",
       "  'token_str': ':',\n",
       "  'sequence': 'O artigo 121 do Código Penal prevê o crime de :'},\n",
       " {'score': 0.01481659710407257,\n",
       "  'token': 21982,\n",
       "  'token_str': 'homicídio',\n",
       "  'sequence': 'O artigo 121 do Código Penal prevê o crime de homicídio'},\n",
       " {'score': 0.006103144492954016,\n",
       "  'token': 18144,\n",
       "  'token_str': 'roubo',\n",
       "  'sequence': 'O artigo 121 do Código Penal prevê o crime de roubo'},\n",
       " {'score': 0.002901835599914193,\n",
       "  'token': 1112,\n",
       "  'token_str': '“',\n",
       "  'sequence': 'O artigo 121 do Código Penal prevê o crime de “'},\n",
       " {'score': 0.0020643649622797966,\n",
       "  'token': 12244,\n",
       "  'token_str': 'assalto',\n",
       "  'sequence': 'O artigo 121 do Código Penal prevê o crime de assalto'}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_base(\"O artigo 121 do Código Penal prevê o crime de [MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9842374920845032,\n",
       "  'token': 131,\n",
       "  'token_str': ':',\n",
       "  'sequence': 'O artigo 121 do Código Penal prevê o crime de :'},\n",
       " {'score': 0.006213649641722441,\n",
       "  'token': 21982,\n",
       "  'token_str': 'homicídio',\n",
       "  'sequence': 'O artigo 121 do Código Penal prevê o crime de homicídio'},\n",
       " {'score': 0.0011775112943723798,\n",
       "  'token': 119,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'O artigo 121 do Código Penal prevê o crime de.'},\n",
       " {'score': 0.0009778104722499847,\n",
       "  'token': 1386,\n",
       "  'token_str': 'morte',\n",
       "  'sequence': 'O artigo 121 do Código Penal prevê o crime de morte'},\n",
       " {'score': 0.0006957394652999938,\n",
       "  'token': 9566,\n",
       "  'token_str': 'corrupção',\n",
       "  'sequence': 'O artigo 121 do Código Penal prevê o crime de corrupção'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_large(\"O artigo 121 do Código Penal prevê o crime de [MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.31187960505485535,\n",
       "  'token': 17225,\n",
       "  'token_str': 'julgado',\n",
       "  'sequence': 'O Código de Processo Civil prevê prazo em julgado para interposição de recurso pela Fazenda Pública'},\n",
       " {'score': 0.282763808965683,\n",
       "  'token': 21244,\n",
       "  'token_str': 'dobro',\n",
       "  'sequence': 'O Código de Processo Civil prevê prazo em dobro para interposição de recurso pela Fazenda Pública'},\n",
       " {'score': 0.17378923296928406,\n",
       "  'token': 5370,\n",
       "  'token_str': 'aberto',\n",
       "  'sequence': 'O Código de Processo Civil prevê prazo em aberto para interposição de recurso pela Fazenda Pública'},\n",
       " {'score': 0.09018026292324066,\n",
       "  'token': 3418,\n",
       "  'token_str': 'curso',\n",
       "  'sequence': 'O Código de Processo Civil prevê prazo em curso para interposição de recurso pela Fazenda Pública'},\n",
       " {'score': 0.03861454129219055,\n",
       "  'token': 4712,\n",
       "  'token_str': 'branco',\n",
       "  'sequence': 'O Código de Processo Civil prevê prazo em branco para interposição de recurso pela Fazenda Pública'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_base(\n",
    "    \"O Código de Processo Civil prevê prazo em [MASK] para interposição de recurso pela Fazenda Pública\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.6611300110816956,\n",
       "  'token': 2241,\n",
       "  'token_str': 'lei',\n",
       "  'sequence': 'O Código de Processo Civil prevê prazo em lei para interposição de recurso pela Fazenda Pública'},\n",
       " {'score': 0.27480894327163696,\n",
       "  'token': 21244,\n",
       "  'token_str': 'dobro',\n",
       "  'sequence': 'O Código de Processo Civil prevê prazo em dobro para interposição de recurso pela Fazenda Pública'},\n",
       " {'score': 0.016465405002236366,\n",
       "  'token': 2502,\n",
       "  'token_str': 'Lei',\n",
       "  'sequence': 'O Código de Processo Civil prevê prazo em Lei para interposição de recurso pela Fazenda Pública'},\n",
       " {'score': 0.011941857635974884,\n",
       "  'token': 5370,\n",
       "  'token_str': 'aberto',\n",
       "  'sequence': 'O Código de Processo Civil prevê prazo em aberto para interposição de recurso pela Fazenda Pública'},\n",
       " {'score': 0.00545860268175602,\n",
       "  'token': 20554,\n",
       "  'token_str': 'razoável',\n",
       "  'sequence': 'O Código de Processo Civil prevê prazo em razoável para interposição de recurso pela Fazenda Pública'}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_large(\n",
    "    \"O Código de Processo Civil prevê prazo em [MASK] para interposição de recurso pela Fazenda Pública\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train our Document Classifier Using Our Fine-Tuned Language Model\n",
    "\n",
    "### Understanding the Language Model Output Structure\n",
    "\n",
    "Before diving into the details of document classification, it's essential to grasp the structure of the output from the language model. The output is a vector with dimensions of `max_tokens` x `embedding_dimension`. Taking BERT-base as an example, the embedding dimension is 768. This means that for each token in the input text, there is a corresponding vector of size 768.\n",
    "\n",
    "In practical scenarios, utilizing the entire array of vectors as input for our classifier may not be feasible due to the vast amount of information involved. Instead, we focus on leveraging the vector corresponding to the `[CLS]` token.\n",
    "\n",
    "### The Significance of the `[CLS]` Token\n",
    "\n",
    "The `[CLS]` token is a special token that precedes the input text and represents the entirety of the input in the context of BERT models. This token's vector size is 768, which is significantly more manageable compared to the entire vector array. `[CLS]` stands for `CL`a`S`sification and is specifically designed for classification tasks.\n",
    "\n",
    "Here's an example to illustrate the usage of the `[CLS]` token:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "outputs = tokenizer('Eu gosto muito de farofa')\n",
    "tokenizer.decode(outputs['input_ids'])\n",
    "```\n",
    "\n",
    "Resulting output: `'[CLS] Eu gosto muito de farofa [SEP]'`\n",
    "\n",
    "In the above output, you'll notice that the `[CLS]` token is added to the start of the input text, while the `[SEP]` token is appended to the end. However, for classification purposes, we only need to focus on the `[CLS]` token and can ignore the `[SEP]` token. The role of the `[SEP]` token in BERT is to enable the separation of two sentences, but since our input text contains only one sentence, its usage is unnecessary here.\n",
    "\n",
    "### Implementing Classification Using the `[CLS]` Token\n",
    "\n",
    "Now that we know how to extract the vector for the `[CLS]` token, we can use it as input for our classifier. The classifier's output will be a vector of size `num_labels`, where `num_labels` refers to the number of labels present in our dataset. For example, if we have 4 labels, the classifier would output a vector of size 4.\n",
    "\n",
    "This output vector will be crucial in calculating the model's loss and updating its weights during the training process. By comparing the predicted label probabilities with the actual labels, we can measure the model's performance and make necessary adjustments to improve its accuracy.\n",
    "\n",
    "### Putting It All Together\n",
    "\n",
    "To summarize, the process of document classification using a fine-tuned language model involves the following steps:\n",
    "\n",
    "1. Tokenize the input text and add the `[CLS]` token at the beginning.\n",
    "2. Pass the tokenized input through the language model to obtain the output vector.\n",
    "3. Extract the vector corresponding to the `[CLS]` token.\n",
    "4. Use the `[CLS]` token vector as input for the classifier.\n",
    "5. Obtain the classifier's output vector, which represents the predicted label probabilities.\n",
    "6. Calculate the loss by comparing the predicted labels with the actual labels.\n",
    "7. Update the model's weights based on the calculated loss to improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoConfig,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((52026, 2), (13007, 2))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the training dataset from a Parquet file\n",
    "# Only the 'text' and 'label' columns are read from the file\n",
    "df_train = pd.read_parquet(\"data/legal/train.parquet\", columns=[\"text\", \"label\"])\n",
    "\n",
    "# Load the validation dataset from a Parquet file\n",
    "# Only the 'text' and 'label' columns are read from the file\n",
    "df_valid = pd.read_parquet(\"data/legal/valid.parquet\", columns=[\"text\", \"label\"])\n",
    "\n",
    "# Define the path to save the base masked language model (MLM)\n",
    "# This path points to the directory where the base MLM model will be saved\n",
    "path_to_save_lm_base = Path(\"./outputs/transformers_basics/bert_masked_lm_base\")\n",
    "\n",
    "# Define the path to save the large masked language model (MLM)\n",
    "# This path points to the directory where the large MLM model will be saved\n",
    "path_to_save_lm_large = Path(\"./outputs/transformers_basics/bert_masked_lm_large\")\n",
    "\n",
    "# Display the shapes of the training and validation datasets\n",
    "# This shows the number of rows and columns in each dataset\n",
    "df_train.shape, df_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'IMPROCEDENTE': 0,\n",
       "  'PROCEDENTE': 1,\n",
       "  'PARCIALMENTE PROCEDENTE': 2,\n",
       "  'EXTINTO SEM MÉRITO': 3},\n",
       " {0: 'IMPROCEDENTE',\n",
       "  1: 'PROCEDENTE',\n",
       "  2: 'PARCIALMENTE PROCEDENTE',\n",
       "  3: 'EXTINTO SEM MÉRITO'})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to map each unique label in the training dataset to a unique ID\n",
    "# df_train.label.unique() returns an array of unique labels in the training dataset\n",
    "# The dictionary comprehension iterates over the unique labels and assigns an ID to each label\n",
    "label2id = {df_train.label.unique()[i]: i for i in range(len(df_train.label.unique()))}\n",
    "\n",
    "# Create a dictionary to map each unique ID back to its corresponding label\n",
    "# This is the reverse mapping of the label2id dictionary\n",
    "# The dictionary comprehension iterates over the items in label2id and swaps the keys and values\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "# Display the label-to-ID and ID-to-label mappings\n",
    "label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1387</th>\n",
       "      <td>SENTENÇA Vistos etc. Dispensado o relatório, a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17972</th>\n",
       "      <td>SENTENÇA Relatório dispensado. No caso, não há...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34527</th>\n",
       "      <td>SENTENÇA Vistos etc. Trata-se de pedido de res...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58381</th>\n",
       "      <td>TERMO DE AUDIÊNCIA DE INSTRUÇÃO Ação Especial ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56474</th>\n",
       "      <td>SENTENÇA Trata-se de ação em que a parte autor...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "1387   SENTENÇA Vistos etc. Dispensado o relatório, a...      0\n",
       "17972  SENTENÇA Relatório dispensado. No caso, não há...      0\n",
       "34527  SENTENÇA Vistos etc. Trata-se de pedido de res...      1\n",
       "58381  TERMO DE AUDIÊNCIA DE INSTRUÇÃO Ação Especial ...      1\n",
       "56474  SENTENÇA Trata-se de ação em que a parte autor...      2"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map the labels in the training dataset to their corresponding IDs\n",
    "# This replaces the label names with their respective IDs using the label2id dictionary\n",
    "df_train[\"label\"] = df_train[\"label\"].map(label2id)\n",
    "\n",
    "# Map the labels in the validation dataset to their corresponding IDs\n",
    "# This replaces the label names with their respective IDs using the label2id dictionary\n",
    "df_valid[\"label\"] = df_valid[\"label\"].map(label2id)\n",
    "\n",
    "# Display the first few rows of the training dataset\n",
    "# This shows the updated training dataset with labels replaced by their corresponding IDs\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Convert the training DataFrame to a Hugging Face Dataset\n",
    "# This allows the use of Hugging Face's dataset utilities for training and evaluation\n",
    "dataset_labeled_train = datasets.Dataset.from_pandas(df_train)\n",
    "\n",
    "# Convert the validation DataFrame to a Hugging Face Dataset\n",
    "# This allows the use of Hugging Face's dataset utilities for validation and evaluation\n",
    "dataset_labeled_valid = datasets.Dataset.from_pandas(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from functools import partial\n",
    "\n",
    "# Load the tokenizer for the fine-tuned base masked language model (MLM)\n",
    "# This tokenizer is used to preprocess the input text for the base model\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\"\n",
    ")\n",
    "\n",
    "# Load the tokenizer for the fine-tuned large masked language model (MLM)\n",
    "# This tokenizer is used to preprocess the input text for the large model\n",
    "tokenizer_large = AutoTokenizer.from_pretrained(\n",
    "    path_to_save_lm_large / \"bert-large-portuguese-cased-finetuned-mlm\"\n",
    ")\n",
    "\n",
    "\n",
    "# Define a function to preprocess the input examples using a specified tokenizer\n",
    "# The function tokenizes the input text, truncates it to a maximum length of 512 tokens,\n",
    "# and pads the sequences to ensure they are of equal length\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "\n",
    "# Create a partial function for preprocessing using the base tokenizer\n",
    "preprocess_function_base = partial(preprocess_function, tokenizer=tokenizer_base)\n",
    "\n",
    "# Create a partial function for preprocessing using the large tokenizer\n",
    "preprocess_function_large = partial(preprocess_function, tokenizer=tokenizer_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501be242adb74cebb6508a6c3e38758f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee52d29ead3446cd9d144c91c44280b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13007 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19735489f3074cbfbb1b067f4db7ce1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a729c1260842a2bc6227abe5633977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13007 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the training dataset using the base tokenizer\n",
    "# The preprocess_function_base tokenizes the text, truncates it to 512 tokens, and pads the sequences\n",
    "# The batched=True argument processes the dataset in batches for efficiency\n",
    "dataset_labeled_train_tokenized_base = dataset_labeled_train.map(\n",
    "    preprocess_function_base, batched=True\n",
    ")\n",
    "\n",
    "# Tokenize the validation dataset using the base tokenizer\n",
    "dataset_labeled_valid_tokenized_base = dataset_labeled_valid.map(\n",
    "    preprocess_function_base, batched=True\n",
    ")\n",
    "\n",
    "# Tokenize the training dataset using the large tokenizer\n",
    "dataset_labeled_train_tokenized_large = dataset_labeled_train.map(\n",
    "    preprocess_function_large, batched=True\n",
    ")\n",
    "\n",
    "# Tokenize the validation dataset using the large tokenizer\n",
    "dataset_labeled_valid_tokenized_large = dataset_labeled_valid.map(\n",
    "    preprocess_function_large, batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Create a data collator for the base tokenizer\n",
    "# The data collator dynamically pads the input sequences to the maximum length in the batch\n",
    "# This ensures that all sequences in a batch have the same length, which is required for efficient processing\n",
    "data_collator_base = DataCollatorWithPadding(tokenizer=tokenizer_base)\n",
    "\n",
    "# Create a data collator for the large tokenizer\n",
    "data_collator_large = DataCollatorWithPadding(tokenizer=tokenizer_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the evaluate module from the Hugging Face library\n",
    "import evaluate\n",
    "\n",
    "# Load the accuracy metric from the evaluate module\n",
    "# This metric will be used to evaluate the performance of the model\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define a function to compute evaluation metrics\n",
    "# This function will be used to evaluate the performance of the model during training and validation\n",
    "def compute_metrics(eval_pred):\n",
    "    # Unpack the predictions and labels from the evaluation tuple\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Convert the model's output logits to predicted class labels\n",
    "    # np.argmax(predictions, axis=1) selects the index of the maximum logit for each prediction\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Compute the accuracy metric using the predicted and true labels\n",
    "    # accuracy.compute() calculates the accuracy of the predictions\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at outputs/transformers_basics/bert_masked_lm_base/bert-base-portuguese-cased-finetuned-mlm and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of unique labels in the training dataset\n",
    "# This will be used to configure the classification model\n",
    "n_labels = df_train.label.nunique()\n",
    "\n",
    "# Load the configuration for the base masked language model (MLM) and modify it for sequence classification\n",
    "# The configuration is loaded from the specified directory and the number of labels is set to n_labels\n",
    "config_base = AutoConfig.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\",\n",
    "    num_labels=n_labels,\n",
    ")\n",
    "\n",
    "# Load the base masked language model (MLM) and modify it for sequence classification\n",
    "# The model is loaded from the specified directory and the configuration is set to config_base\n",
    "classifier_base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\",\n",
    "    config=config_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4187509/2400232891.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_base = Trainer(\n",
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2710' max='2710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2710/2710 1:08:27, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.603200</td>\n",
       "      <td>0.615268</td>\n",
       "      <td>0.743369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.525200</td>\n",
       "      <td>0.587841</td>\n",
       "      <td>0.759437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.495700</td>\n",
       "      <td>0.557939</td>\n",
       "      <td>0.773199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.445800</td>\n",
       "      <td>0.571086</td>\n",
       "      <td>0.769816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.405400</td>\n",
       "      <td>0.575862</td>\n",
       "      <td>0.775505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2710, training_loss=0.5300090861056563, metrics={'train_runtime': 4108.5228, 'train_samples_per_second': 63.315, 'train_steps_per_second': 0.66, 'total_flos': 6.844430787637248e+16, 'train_loss': 0.5300090861056563, 'epoch': 5.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define the training arguments for the base classifier\n",
    "# These arguments configure various aspects of the training process\n",
    "training_args_base = TrainingArguments(\n",
    "    output_dir=path_to_save_lm_base\n",
    "    / \"base_classifier_legal\",  # Directory to save the model and other outputs\n",
    "    learning_rate=2e-5,  # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=48,  # Batch size for training (adjust based on GPU memory)\n",
    "    per_device_eval_batch_size=64,  # Batch size for evaluation (adjust based on GPU memory)\n",
    "    num_train_epochs=5,  # Number of training epochs\n",
    "    gradient_accumulation_steps=1,  # Number of steps to accumulate gradients before updating\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    bf16=True,  # Use 16-bit floating point precision for training (adjust based on GPU support)\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model after each epoch\n",
    "    logging_strategy=\"steps\",  # Log the training progress after each step\n",
    "    save_strategy=\"epoch\",  # Save the model after each epoch\n",
    "    eval_steps=1,  # Evaluate the model after every 1 epoch\n",
    "    save_steps=1,  # Save the model after every 1 epoch\n",
    "    logging_steps=10,  # Log the training progress after every 10 steps\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    seed=271828,  # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Create a Trainer instance for the base classifier\n",
    "# The Trainer handles the training and evaluation of the model\n",
    "trainer_base = Trainer(\n",
    "    model=classifier_base,  # The model to be trained\n",
    "    args=training_args_base,  # Training arguments\n",
    "    train_dataset=dataset_labeled_train_tokenized_base,  # Training dataset\n",
    "    eval_dataset=dataset_labeled_valid_tokenized_base,  # Evaluation dataset\n",
    "    tokenizer=tokenizer_base,  # Tokenizer for preprocessing the input text\n",
    "    data_collator=data_collator_base,  # Data collator for dynamic padding\n",
    "    compute_metrics=compute_metrics,  # Function to compute evaluation metrics\n",
    ")\n",
    "\n",
    "# Train the model using the Trainer\n",
    "trainer_base.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='97' max='102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 97/102 00:58 < 00:03, 1.63 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5579385161399841,\n",
       " 'eval_accuracy': 0.7731990466671792,\n",
       " 'eval_runtime': 62.6599,\n",
       " 'eval_samples_per_second': 207.581,\n",
       " 'eval_steps_per_second': 1.628,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_base.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Can you guess why the accuracy is so low?`\n",
    "\n",
    "\n",
    "## Understanding Low Accuracy: The Limitation of 512 Tokens\n",
    "\n",
    "When working with transformer models, it's essential to be aware of a key limitation: most models can only process a maximum of **512 tokens**. This restriction has a significant impact on the accuracy of predictions, especially when dealing with longer texts.\n",
    "\n",
    "### The Self-Attention Mechanism and Quadratic Complexity\n",
    "\n",
    "The 512-token limit is a result of the *quadratic complexity* of the **self-attention mechanism**, which is a fundamental component of transformer models. Self-attention allows the model to weigh the importance of each token in relation to others, enabling it to capture context and dependencies within the input text.\n",
    "\n",
    "However, the computational cost of self-attention grows quadratically with the number of tokens. As the input length increases, the memory and computational requirements become prohibitively expensive. To mitigate this issue, most transformer models impose a maximum token limit of 512.\n",
    "\n",
    "### The Impact of Truncation on Accuracy\n",
    "\n",
    "When an input text exceeds 512 tokens, the model automatically truncates it by removing tokens until it fits within the limit. This truncation process can have a detrimental effect on the model's accuracy.\n",
    "\n",
    "Important information, such as key context or relevant details, may be lost during truncation. The model is forced to make predictions based on an incomplete representation of the original text, leading to lower accuracy scores.\n",
    "\n",
    "### Strategies for Handling Longer Texts\n",
    "\n",
    "While the 512-token limit can be challenging, there are several approaches to mitigate its impact:\n",
    "\n",
    "1. **Sliding Window Approach**:\n",
    "    - Divide the long text into smaller, overlapping chunks (windows).\n",
    "    - Process each window individually and aggregate the results.\n",
    "    - This approach can help capture local context, but it may struggle with long-range dependencies.\n",
    "\n",
    "2. **Alternative Neural Network Architectures**:\n",
    "    - Consider using other architectures, such as Recurrent Neural Networks (RNNs) or Convolutional Neural Networks (CNNs).\n",
    "    - These architectures can handle longer sequences without the same token limit constraints.\n",
    "    - However, they may not capture long-range dependencies as effectively as transformers.\n",
    "\n",
    "3. **Transformer Variants for Longer Sequences**:\n",
    "    - Explore transformer-based models specifically designed for handling longer texts, such as Longformer and BigBird.\n",
    "    - These models introduce modifications to the self-attention mechanism to reduce computational complexity.\n",
    "    - Keep in mind that these models are relatively new and may have limitations or trade-offs compared to standard transformers.\n",
    "\n",
    "\n",
    "To make informed decisions about handling longer texts, it's crucial to understand the characteristics of your dataset. Analyze the average number of tokens per text and the distribution of text lengths.\n",
    "\n",
    "If a significant portion of your texts exceeds the 512-token limit, consider applying one of the strategies mentioned above. Experiment with different approaches and evaluate their impact on accuracy and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    52026.000000\n",
       "mean      2373.339407\n",
       "std       1822.717847\n",
       "min        151.000000\n",
       "25%       1133.000000\n",
       "50%       1799.000000\n",
       "75%       3031.000000\n",
       "max      11434.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer for the fine-tuned base masked language model (MLM)\n",
    "# This tokenizer is used to preprocess the input text for the base model\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\"\n",
    ")\n",
    "\n",
    "# Initialize an empty list to store the sizes of tokenized input sequences\n",
    "sizes = []\n",
    "\n",
    "# Iterate over each text in the training dataset\n",
    "for txt in df_train.text:\n",
    "    # Tokenize the text without truncation and get the length of the tokenized input sequence\n",
    "    # Append the length of the tokenized input sequence to the sizes list\n",
    "    sizes.append(len(tokenizer_base(txt, truncation=False)[\"input_ids\"]))\n",
    "\n",
    "# Convert the sizes list to a Pandas Series and display descriptive statistics\n",
    "# This provides an overview of the distribution of tokenized input sequence lengths\n",
    "pd.Series(sizes).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`As we can see above, the average number of tokens in our dataset is 2,373. This is significantly higher than the 512 token limit. Therefore, we need to employ a workaround to handle this limitation. We won't cover more complex approaches in this class, but we can use a simple and effective workaround - understanding our data! Let's see how we can do this.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SENTENÇA Tipo A RELATÓRIO Trata-se de ação declaratória de inexistência de débito e indenizatória por danos morais, com pedido de repetição de indébito, ajuizada por Lúcia Matias de Souza em face do Instituto Nacional do Seguro Social – INSS e do Banco Bradesco S/A, em razão da existência de contrato de empréstimo consignado celebrado perante a aludida instituição financeira que, segundo diz a autora, não foi por ela contratado. É o que importa relatar. Passo a decidir. FUNDAMENTAÇÃO Das preliminares arguidas Quanto à preliminar de ilegitimidade passiva alegada pelo INSS (anexo 11), entendo que a Autarquia ré detém legitimidade para figurar no pólo passivo da ação, tendo em vista que é responsável pelo gerenciamento e pagamento dos descontos realizados nos benefícios previdenciários em decorrência de empréstimo consignado. Assim, a partir do momento em que opera o desconto nos valores tem interesse e legitimidade para figurar no pólo passivo da presente demanda. Ademais, só o INSS tem poder para fazer cessar os descontos efetuados, sendo a cessação um dos objetos dessa demanda. No que diz respeito à suposta falta de interesse de agir, é preciso dizer que o caso sob exame não cuida de concessão de beneficio previdenciário, razão pela qual se faz desnecessário prévio requerimento administrativo. Desse modo, afastadas as preliminares suscitadas, passo ao exame do mérito. Mérito A questão posta em Juízo dispensa maiores esclarecimentos, eis que os fatos objetos de controvérsia encontram-se suficientemente provados. Os empréstimos consignados, disponibilizados aos aposentados e pensionistas do INSS, são feitos com descontos na folha de pagamento, conforme autorizado pela Lei 10.820/2003. O valor das parcelas é descontado diretamente do benefício previdenciário. Nas relações de consumo, que são estabelecidas entre instituições financeiras e beneficiários pactuantes dos contratos de empréstimos em consignação (Súmula n.o 297 do STJ), não se pode atribuir ao consumidor, considerado parte hipossuficiente na relação jurídica, a responsabilidade pelos encargos financeiros decorrentes de ocorrência de suspeita de fraude na operacionalização dos empréstimos consignados. Ademais, nessas relações, entre beneficiários e as instituições financeiras, consoante disposto no art. 6o, VIII, da Lei n.o 8.078/90, deve ser facilitada a defesa do consumidor, sobretudo considerando que as instituições financeiras detêm os documentos que comprovam a celebração dos contratos, restando dificultosa qualquer comprovação de fraude pelo beneficiário. Os doutrinadores afirmam, atualmente, que os fatos negativos são possíveis de serem provados e o devem ser pela parte que os alega, diferente do que acontecia outrora, quando vigorava a aplicação da máxima negativa non sunt probanda, ou seja, que os fatos negativos não precisavam ser provados, sendo impossível a produção de sua prova. Hoje, somente os fatos absolutamente negativos é que são insuscetíveis de prova, não pela sua negatividade, mas por sua indefinição. Tem-se como impossível se provar que nunca se esteve em determinado local. Então, nessas hipóteses, tem-se que a parte que alega o fato positivo é que tem o ônus probatório, aplicando-se a teoria da distribuição dinâmica do ônus da prova. Em relação aos fatos relativamente negativos, estes sim podem ser provados. Se, a título de exemplo, alguém afirma que não compareceu ao trabalho em um dia determinado, é possível provar indiretamente a sua ausência ao trabalho, comprovando que foi a um consultório médico. A chamada 'certidão negativa', expedida pelas autoridades fiscais, é um meio de prova de que 'não há débitos fiscais pendentes'. (DIDIER JR., Fredie. Curso de Direito Processual Civil. Vol. 2. Salvador: Editora Jus Podivm, 2007). Todavia, conforme leciona Luiz Guilherme Marinoni, “quando se inverte o ônus, é preciso supor que aquele que vai assumi-lo terá a possibilidade de cumpri-lo, pena de a inversão do ônus da prova significa a imposição de uma perda, e não apenas a transferência de um ônus. Nessa perspectiva, a inversão do ônus da prova somente deve ocorrer quando o réu tem a possibilidade de demonstrar a não existência do fato constitutivo” (Processo de Conhecimento. São Paulo: RT, 2007, p.269/270). No caso em análise, o contrato impugnado é o Contrato no 808431996, no valor de R$565,91 (quinhentos e sessenta e cinco reais e noventa e um centavos).O extrato do NB1061609828 (anexo 07) confirma os descontos no beneficio da parte autora. Pois bem. Tanto o INSS quanto o Banco réu não carrearam os contratos em tela nem qualquer documento que pudesse comprovar a celebração dos negócios jurídicos realizados pela parte autora. Ante a não comprovação da legitimidade dos contratos em tela, entendo que o Banco réu, como beneficiária do presente contrato, deve ser condenada a repetição de indébito. Quanto ao valor da indenização devida, tenho que a reparação pecuniária visa a proporcionar uma espécie de compensação que atenue a ofensa causada, atentando-se, que ao beneficiário não é dado tirar proveito do sinistro, posto que não se destina a indenização ao seu enriquecimento. Portanto, o valor deve ser apenas suficiente ao reparo, sob pena de estar o Judiciário autorizando o enriquecimento sem causa da vítima. O valor da indenização por danos morais deve ser suficiente para, a um só tempo, desestimular reiteração da conduta lesiva pelo réu e abrandar, na medida do possível, o constrangimento e a humilhação causados ao autor lesado. No caso em apreço, entendo que o pagamento de R$ 5.000,00 (cinco mil reais), é suficiente para atender à pretensão formulada. Com efeito, a indenização nos valores indicados demonstra-se adequada, na medida em que, deve-se ter em mente o seu caráter pedagógico, a fim de desestimular os réus a proceder, com desídia, em suas atividades, mormente quando envolvido interesses de pessoas que se encontrem em posição de maior vulnerabilidade. Portanto, tenho que resta configurada a responsabilidade do Banco Bradesco de indenizar, em dobro, o valor indevidamente descontado do benefício da parte promovente, nos termos do art. 42, parágrafo único do CDC, bem como indenizar os danos morais suportados pela parte promovente. Registre-se que, nas demandas referentes a empréstimos consignados celebrados fraudulentamente, a responsabilidade do INSS cinge-se tão somente a cessação dos descontos no benefício previdenciário do prejudicado, sendo a instituição financeira responsável pela reparação dos danos materiais e morais eventualmente suportados. DISPOSITIVO Isso posto, julgo PROCEDENTE o pedido para determinar que o INSS cesse os descontos das parcelas do Contratono 808431996. Condeno, também, a título de danos materiais, o Banco Bradesco a devolver os valores descontados com relação aos citados contratos de empréstimo, em dobro, nos termos do art. 42, parágrafo único, do CDC, devendo tais valores serem acrescidos de juros de mora de 1% ao mês desde o evento danoso (súmula 54 – STJ) e correção monetária com base no IPCA-E desde o efetivo prejuízo (súmula 43 – STJ). Condeno, ainda, o bancoréua pagar, a título de indenização por danos morais, a quantia de R$ 5.000,00 (cinco mil reais), valor este que deve ser atualizado exclusivamente pela taxa SELIC desde a publicação desta sentença. Declaro a inexistência do contrato no808431996. Declaro extinto o processo com resolução do mérito, nos termos do art. 487, I, do Código de Processo Civil. Custas e honorários advocatícios indevidos em primeiro grau de jurisdição (art. 55 da Lei no 9.099/95, c/c art. 1o da Lei no 10.259/01). Registre-se. Intimem-se as partes (Lei no 10.259/01, art. 8o). Campina Grande-PB, data supra. JUIZ FEDERAL\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.sample(10, random_state=271828)[\"text\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`Can you notice that the really relevant information for our classification task is not in the beginning of the text, but in the end?`\n",
    "\n",
    "> (....)\n",
    ">\n",
    "> DISPOSITIVO Isso posto, `julgo PROCEDENTE` o pedido para determinar que o INSS cesse os descontos das parcelas do Contratono 808431996. Condeno, também, a título de danos materiais, o Banco Bradesco a devolver os valores descontados com relação aos citados contratos de empréstimo, em dobro, nos termos do art. 42, parágrafo único, do CDC, devendo tais valores serem acrescidos de juros de mora de 1% ao mês desde o evento danoso (súmula 54 – STJ) e correção monetária com base no IPCA-E desde o efetivo prejuízo (súmula 43 – STJ). Condeno, ainda, o bancoréua pagar, a título de indenização por danos morais, a quantia de R$ 5.000,00 (cinco mil reais), valor este que deve ser atualizado exclusivamente pela taxa SELIC desde a publicação desta sentença. Declaro a inexistência do contrato no808431996. Declaro extinto o processo com resolução do mérito, nos termos do art. 487, I, do Código de Processo Civil. Custas e honorários advocatícios indevidos em primeiro grau de jurisdição (art. 55 da Lei no 9.099/95, c/c art. 1o da Lei no 10.259/01). Registre-se. Intimem-se as partes (Lei no 10.259/01, art. 8o). Campina Grande-PB, data supra. JUIZ FEDERAL\n",
    ">\n",
    "\n",
    "This is very common in this kind of documents. The judge starts with a thorough description of the case and then goes to the decision. So, we can use the last 512 tokens of the text to train our model. We just need to change the truncation_side parameter to 'left' in the tokenizer.\n",
    "\n",
    "Let's see how we can do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from functools import partial\n",
    "\n",
    "# Load the tokenizer for the fine-tuned base masked language model (MLM)\n",
    "# This tokenizer is used to preprocess the input text for the base model\n",
    "tokenizer_base = AutoTokenizer.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\"\n",
    ")\n",
    "\n",
    "# Load the tokenizer for the fine-tuned large masked language model (MLM)\n",
    "# This tokenizer is used to preprocess the input text for the large model\n",
    "tokenizer_large = AutoTokenizer.from_pretrained(\n",
    "    path_to_save_lm_large / \"bert-large-portuguese-cased-finetuned-mlm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_base.truncation_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Eu gosto muito [SEP]'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the input text using the base tokenizer\n",
    "# The padding=True argument ensures that the sequence is padded to the maximum length\n",
    "# The truncation=True argument ensures that the sequence is truncated to the maximum length if it exceeds it\n",
    "# The max_length=5 argument sets the maximum length of the tokenized sequence to 5 tokens\n",
    "out_len5 = tokenizer_base(\n",
    "    \"Eu gosto muito de farofa com banana\", padding=True, truncation=True, max_length=5\n",
    ")  # This is to simulate the truncation\n",
    "\n",
    "# Decode the tokenized input IDs back to a string\n",
    "# This converts the token IDs back to the corresponding text\n",
    "# The decoded text will be truncated to the first 5 tokens\n",
    "tokenizer_base.decode(out_len5[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the truncation side for the base tokenizer to 'left'\n",
    "# This means that if the input text needs to be truncated, tokens will be removed from the beginning (left side) of the sequence\n",
    "# This setting is useful when the most important information is at the end of the sequence\n",
    "tokenizer_base.truncation_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] com banana [SEP]'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the input text using the base tokenizer\n",
    "# The padding=True argument ensures that the sequence is padded to the maximum length\n",
    "# The truncation=True argument ensures that the sequence is truncated to the maximum length if it exceeds it\n",
    "# The max_length=5 argument sets the maximum length of the tokenized sequence to 5 tokens\n",
    "out_len5 = tokenizer_base(\n",
    "    \"Eu gosto muito de farofa com banana\", padding=True, truncation=True, max_length=5\n",
    ")\n",
    "\n",
    "# Decode the tokenized input IDs back to a string\n",
    "# This converts the token IDs back to the corresponding text\n",
    "# The decoded text will be truncated to the first 5 tokens\n",
    "tokenizer_base.decode(out_len5[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# Convert the training DataFrame to a Hugging Face Dataset\n",
    "# This allows the use of Hugging Face's dataset utilities for training and evaluation\n",
    "dataset_labeled_train = datasets.Dataset.from_pandas(df_train)\n",
    "\n",
    "# Convert the validation DataFrame to a Hugging Face Dataset\n",
    "# This allows the use of Hugging Face's dataset utilities for validation and evaluation\n",
    "dataset_labeled_valid = datasets.Dataset.from_pandas(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "# Define a function to preprocess the input examples using a specified tokenizer\n",
    "# The function tokenizes the input text, truncates it to a maximum length of 512 tokens,\n",
    "# and pads the sequences to ensure they are of equal length\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True, max_length=512)\n",
    "\n",
    "\n",
    "# Create a partial function for preprocessing using the base tokenizer\n",
    "# This partial function allows us to call preprocess_function with only the examples argument,\n",
    "# as the tokenizer argument is already set to tokenizer_base\n",
    "preprocess_function_base = partial(preprocess_function, tokenizer=tokenizer_base)\n",
    "\n",
    "# Create a partial function for preprocessing using the large tokenizer\n",
    "preprocess_function_large = partial(preprocess_function, tokenizer=tokenizer_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34342097880447a3b9fc66b600771624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/52026 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe7c1d453e0488b826af617ac70a6af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13007 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the training dataset using the base tokenizer\n",
    "# The preprocess_function_base tokenizes the text, truncates it to 512 tokens, and pads the sequences\n",
    "# The batched=True argument processes the dataset in batches for efficiency\n",
    "dataset_labeled_train_tokenized_base = dataset_labeled_train.map(\n",
    "    preprocess_function_base, batched=True\n",
    ")\n",
    "\n",
    "# Tokenize the validation dataset using the base tokenizer\n",
    "dataset_labeled_valid_tokenized_base = dataset_labeled_valid.map(\n",
    "    preprocess_function_base, batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Create a data collator for the base tokenizer\n",
    "# The data collator dynamically pads the input sequences to the maximum length in the batch\n",
    "# This ensures that all sequences in a batch have the same length, which is required for efficient processing\n",
    "data_collator_base = DataCollatorWithPadding(tokenizer=tokenizer_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the evaluate module from the Hugging Face library\n",
    "import evaluate\n",
    "\n",
    "# Load the accuracy metric from the evaluate module\n",
    "# This metric will be used to evaluate the performance of the model\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Define a function to compute evaluation metrics\n",
    "# This function will be used to evaluate the performance of the model during training and validation\n",
    "def compute_metrics(eval_pred):\n",
    "    # Unpack the predictions and labels from the evaluation tuple\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    # Convert the model's output logits to predicted class labels\n",
    "    # np.argmax(predictions, axis=1) selects the index of the maximum logit for each prediction\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    # Compute the accuracy metric using the predicted and true labels\n",
    "    # accuracy.compute() calculates the accuracy of the predictions\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at outputs/transformers_basics/bert_masked_lm_base/bert-base-portuguese-cased-finetuned-mlm and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of unique labels in the training dataset\n",
    "# This will be used to configure the classification model\n",
    "n_labels = df_train.label.nunique()\n",
    "\n",
    "# Load the configuration for the base masked language model (MLM) and modify it for sequence classification\n",
    "# The configuration is loaded from the specified directory and the number of labels is set to n_labels\n",
    "config_base = AutoConfig.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\",\n",
    "    num_labels=n_labels,\n",
    ")\n",
    "\n",
    "# Load the base masked language model (MLM) and modify it for sequence classification\n",
    "classifier_base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    path_to_save_lm_base / \"bert-base-portuguese-cased-finetuned-mlm\",\n",
    "    config=config_base,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4187509/2400232891.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_base = Trainer(\n",
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='2710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  50/2710 00:57 < 52:39, 0.84 it/s, Epoch 0.09/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2710, training_loss=0.11587148178987397, metrics={'train_runtime': 4188.4617, 'train_samples_per_second': 62.106, 'train_steps_per_second': 0.647, 'total_flos': 6.844430787637248e+16, 'train_loss': 0.11587148178987397, 'epoch': 5.0})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Define the training arguments for the base classifier\n",
    "# These arguments configure various aspects of the training process\n",
    "training_args_base = TrainingArguments(\n",
    "    output_dir=path_to_save_lm_base\n",
    "    / \"base_classifier_legal\",  # Directory to save the model and other outputs\n",
    "    learning_rate=2e-5,  # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=48,  # Batch size for training (adjust based on GPU memory)\n",
    "    per_device_eval_batch_size=64,  # Batch size for evaluation (adjust based on GPU memory)\n",
    "    num_train_epochs=5,  # Number of training epochs\n",
    "    gradient_accumulation_steps=1,  # Number of steps to accumulate gradients before updating\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    bf16=True,  # Use 16-bit floating point precision for training (adjust based on GPU support)\n",
    "    eval_strategy=\"epoch\",  # Evaluate the model after each epoch\n",
    "    logging_strategy=\"steps\",  # Log the training progress after each step\n",
    "    save_strategy=\"epoch\",  # Save the model after each epoch\n",
    "    eval_steps=1,  # Evaluate the model after every 1 epoch\n",
    "    save_steps=1,  # Save the model after every 1 epoch\n",
    "    logging_steps=10,  # Log the training progress after every 10 steps\n",
    "    load_best_model_at_end=True,  # Load the best model at the end of training\n",
    "    seed=271828,  # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Create a Trainer instance for the base classifier\n",
    "# The Trainer handles the training and evaluation of the model\n",
    "trainer_base = Trainer(\n",
    "    model=classifier_base,  # The model to be trained\n",
    "    args=training_args_base,  # Training arguments\n",
    "    train_dataset=dataset_labeled_train_tokenized_base,  # Training dataset\n",
    "    eval_dataset=dataset_labeled_valid_tokenized_base,  # Evaluation dataset\n",
    "    tokenizer=tokenizer_base,  # Tokenizer for preprocessing the input text\n",
    "    data_collator=data_collator_base,  # Data collator for dynamic padding\n",
    "    compute_metrics=compute_metrics,  # Function to compute evaluation metrics\n",
    ")\n",
    "\n",
    "# Train the model using the Trainer\n",
    "trainer_base.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacob/.cache/pypoetry/virtualenvs/deep-learning-gen-ai-vEhrekpc-py3.12/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='94' max='102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 94/102 00:59 < 00:05, 1.57 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.1189282238483429,\n",
       " 'eval_accuracy': 0.9610209886983931,\n",
       " 'eval_runtime': 64.6572,\n",
       " 'eval_samples_per_second': 201.169,\n",
       " 'eval_steps_per_second': 1.578,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_base.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've achieved a significant improvement in our model's accuracy, which soared from 77.9% to an impressive 96.1%. This upswing is indeed fantastic news!\n",
    "\n",
    "Let's gain a better understanding of this improvement by examining it in terms of the error rate. The error rate is simply calculated as (1 - accuracy). With this formula, our initial error rate was 22.5%, and our improved error rate dropped dramatically to 3.9%.\n",
    "\n",
    "To put this into perspective, we've effectively reduced the error rate by nearly six-fold! In other words, our model is now making far fewer mistakes than before, indicating an exponential enhancement in its overall performance.\n",
    "\n",
    "By using the last 512 tokens in the text data, we were able to direct the focus of our model towards the most relevant information. This approach is a simple yet effective workaround to overcome the 512 token limitation in transformers.\n",
    "\n",
    "This method may seem simple, but it's proven to be an effectively strategic approach to overcome such limitations and handle large amounts of data proficiently. `Remember, sometimes simplicity is the key to master complex challenges!`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. What are the key advantages of transformers compared to traditional sequential models like RNNs and LSTMs?\n",
    "\n",
    "2. How does the self-attention mechanism in transformers function, and why is it important?\n",
    "\n",
    "3. What role does positional encoding play in transformer architectures, and how is it typically implemented?\n",
    "\n",
    "4. Why do transformers have quadratic complexity with respect to sequence length, and what challenges does this present when processing long texts?\n",
    "\n",
    "5. In what ways have transformers been applied beyond natural language processing, and can you provide examples?\n",
    "\n",
    "6. Can you describe the differences between BERT, GPT, and T5 transformer architectures?\n",
    "\n",
    "7. How can transformers be utilized as feature extractors in NLP tasks?\n",
    "\n",
    "8. What is the impact of the 512-token limit in transformers, and how can it affect the performance of models on longer texts?\n",
    "\n",
    "9. What workaround was used in the notebook to handle texts longer than 512 tokens in the document classification task?\n",
    "\n",
    "10. What steps were taken in the notebook to fine-tune a transformer model on domain-specific text and use it for a classification task?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Answers are commented inside this cell`\n",
    "<!-- \n",
    "\n",
    "1. Transformers offer key advantages over traditional sequential models like RNNs and LSTMs in that they can handle long-range dependencies and parallelize computations. Unlike RNNs and LSTMs, which process tokens sequentially and can struggle with vanishing gradients, transformers process entire sequences simultaneously using self-attention, allowing for efficient computation and better handling of dependencies across distant tokens.\n",
    "\n",
    "2. The self-attention mechanism allows transformers to weigh the relevance of different tokens in the input sequence when generating the output for a specific token. It functions by computing Query, Key, and Value vectors for each token. The attention scores are calculated as the dot product of the Query vector of a token with the Key vectors of all tokens, resulting in weights that determine how much attention to pay to each token. These weights are then applied to the Value vectors to produce a new representation, capturing contextual relationships across the sequence.\n",
    "\n",
    "3. Positional encoding addresses the fact that transformers process input tokens simultaneously without innate sequence order. It injects information about the position of each token in the sequence into the model. Typically, positional encoding is implemented by adding positional vectors to the input embeddings, often using sine and cosine functions of different frequencies to generate unique encodings for each position. This enables the transformer to capture the order of tokens and recognize sequence patterns.\n",
    "\n",
    "4. Transformers have quadratic complexity with respect to sequence length because the self-attention mechanism requires calculating attention scores between every pair of tokens in the sequence. For a sequence of length _n_, this results in _n²_ computations. This quadratic scaling leads to high computational costs and memory usage when processing long texts, posing challenges in terms of efficiency and feasibility for longer sequences.\n",
    "\n",
    "5. Transformers have been applied beyond natural language processing in various domains by adapting the self-attention mechanism to other types of sequential data. Examples include:\n",
    "   - **Computer Vision**: Vision Transformers (ViT) treat images as sequences of patches (tokens) and capture spatial relationships using self-attention.\n",
    "   - **Music Generation**: Models like MuseNet use transformers to generate complex musical compositions by modeling sequences of musical notes and attributes.\n",
    "   - **Speech Recognition**: Speech-Transformers process sequences of acoustic feature vectors to transcribe speech to text.\n",
    "   - **Video Processing**: Video Transformers apply self-attention across spatiotemporal tokens to analyze and understand video content.\n",
    "\n",
    "6. **BERT** is a bidirectional encoder transformer that uses masked language modeling and next sentence prediction during pre-training, focusing on understanding the context from both directions. **GPT** is a unidirectional (left-to-right) decoder-only transformer that uses language modeling to predict the next word, excelling at text generation. **T5** is a text-to-text transfer transformer that frames all NLP tasks as text generation problems, using an encoder-decoder architecture and trained with a span-corruption objective.\n",
    "\n",
    "7. Transformers can be used as feature extractors by using the contextualized representations generated by the model's hidden layers. Specifically, the vector corresponding to the `[CLS]` token, which represents the entire input sequence, can be extracted and used as a feature vector for downstream tasks like classification or regression. This approach leverages the rich syntactic and semantic information captured by the transformer without necessarily fine-tuning the whole model.\n",
    "\n",
    "8. The 512-token limit in transformers arises because processing longer sequences would require excessive computational resources due to the quadratic complexity of self-attention. This limit means that when input texts exceed 512 tokens, they are truncated, potentially removing important information, especially if critical content is beyond this limit. This truncation can negatively affect the model's performance and accuracy on tasks involving long documents.\n",
    "\n",
    "9. In the notebook, to handle texts longer than 512 tokens, the tokenizer's truncation side was adjusted to `'left'`, so that the model keeps the last 512 tokens of the text instead of the first 512. This approach ensures that in documents like legal texts, where important information like the judgment is often at the end, critical content is retained for the model to process, thereby improving accuracy.\n",
    "\n",
    "10. The notebook followed these steps:\n",
    "    - **Step 1**: Started with a pretrained transformer model.\n",
    "    - **Step 2**: Optionally fine-tuned the model on domain-specific legal text to adapt it to the language and style of the domain.\n",
    "    - **Step 3**: Tokenized and prepared the dataset, adjusting the tokenizer to truncate the beginning of texts (keep the end).\n",
    "    - **Step 4**: Trained the model on the specific classification task (predicting court decision labels) using the adjusted data.\n",
    "    - The process involved loading datasets, preprocessing, configuring the model for classification, and training with appropriate settings to handle the 512-token limit. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning-gen-ai-vEhrekpc-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
